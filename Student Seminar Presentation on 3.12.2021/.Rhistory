A_depth2 <- as.data.frame(A_depth2)
summary(A_depth2)
str(A_depth2)
A_depth2$Order <- as.factor(A_depth2$Order)
A_depth2$Temperature <- as.factor(A_depth2$Temperature)
A_depth2$Moisture <- as.factor(A_depth2$Moisture)
A_depth2$LULC <- as.factor(A_depth2$LULC)
A_depth2$Parent.material <- as.factor(A_depth2$Parent.material)
str(A_depth2)
## Split into calibration and validation and check the distribution
set.seed(1)
nc <- nrow(A_depth2)
index_cali <- sample(1:nc, size=nc*0.7, replace = F)
A_depth_cali <- A_depth2[index_cali,]
A_depth_vali <- A_depth2[-index_cali,]
summary(A_depth_cali$Order)
summary(A_depth_vali$Order)
summary(A_depth_cali$Temperature)
summary(A_depth_vali$Temperature)
summary(A_depth_cali$Moisture)
summary(A_depth_vali$Moisture)
summary(A_depth_cali$Parent.material)
summary(A_depth_vali$Parent.material)
summary(A_depth_cali$LULC)
summary(A_depth_vali$LULC)
A_depth2
# Select only the key variables
#' This function is where you'll select the key variables that you'd like to look at
#'
#' @param dataset - the input dataset
#' @param resp_var_id - the response variable
#' @param num_expl_var_ids - the numeric variables you'd like to consider
#' @param fact_expl_var_ids - the factor variables you'd like to consider
#'
#' @return - a subsetted dataframe
#' @export
#'
#' @examples - A_depth2_small <- var_select(A_depth2, resp_var_id = "A_depth", fact_expl_var_ids =c("Order", "Temperature", "Moisture", "Parent.material", "LULC"))
var_select <- function(dataset, resp_var_id, num_expl_var_ids = NULL, fact_expl_var_ids) {
# Select only certain columns
col_ids <- c(resp_var_id, num_expl_var_ids, fact_expl_var_ids)
# Select the dataset
df_out <- dataset[,col_ids]
# Return the dataframe
return(df_out)
}
A_depth2_small <- var_select(A_depth2, resp_var_id = "A_depth", fact_expl_var_ids =c("Order", "Temperature", "Moisture", "Parent.material", "LULC"))
#' The regularized models require factor variables to be coded as "dummies"; variables that are only 1 or 0
#'
#' @param dataset - the input dataset
#' @param fact_expl_var_ids - the factor variables you'd like to consider
#'
#' @return
#' @export - a dataset that has every factor variable coded as a column
#'
#' @examples - df_wide(dataset = A_depth2_small, fact_expl_var_ids =c("Order", "Temperature", "Moisture", "Parent.material", "LULC"))
df_wide <- function(dataset, fact_expl_var_ids) {
# Run the fast dummies code
dataset <- fastDummies::dummy_cols(dataset)
# Cut out the original categorical variables
df_out <- dataset[,!names(dataset) %in% fact_expl_var_ids]
# Return the dataset
return(df_out)
}
A_depth2_wide <- df_wide(dataset = A_depth2_small, fact_expl_var_ids =c("Order", "Temperature", "Moisture", "Parent.material", "LULC"))
A_depth2_wide
#' A function that splits the data into three parts
#'
#' @param dataset - the input dataset
#' @param train_frac - the training percentage. Defaults to 60%
#' @param val_frac - the validation percentage. Default is 20%
#' @param test_frac - the test percentage. Default is 20%
#' @param rseed - random seed
#' @param print_check - checks if the percentages match and if the split was done properly
#'
#' @return returns a list with the three datasets and the rows selected
#' @export
#'
#' @examples - train_val_test(dataset = A_depth2, print_check = 1)
train_val_test <- function(dataset, train_frac = 0.6, val_frac = 0.2, test_frac = 0.2, rseed = 1597, print_check = 0) {
# Check if the percentages add up to 100%
if (train_frac + val_frac + test_frac != 1){
print("Error: The training-validation-test percentages do not add up to 100%.")
} else {
# Set random seed for reproducibility
set.seed(seed = rseed)
# Sample training rows
all_rows = seq(1,nrow(dataset))
n_train = floor(train_frac*nrow(dataset))
# print(n_train)
train_rows = sample(x = all_rows, size = n_train)
# Find out how many rows remain
remain_rows = setdiff(all_rows, train_rows)
# print(remain_rows)
# See what percentage of the remaining rows go to val
val_vs_test_frac = val_frac/(val_frac + test_frac)
# Compute validation rows
n_val = floor(val_vs_test_frac*length(remain_rows))
# print(n_val)
val_rows = sample(x = remain_rows, size = n_val)
# Compute test rows
test_rows = setdiff(remain_rows, val_rows)
# Compute the training, validation, and testing datasets
train_set = dataset[train_rows,]
val_set = dataset[val_rows,]
test_set = dataset[test_rows,]
# Check the percentages
if (print_check == 1){
# Checks if the percentages match
print(c(nrow(train_set),nrow(val_set),nrow(test_set))/nrow(dataset))
# Check if the randomization was correct
print(intersect(intersect(train_rows, val_rows), test_rows))
}
# Return a list
return(list(train_set = train_set, val_set = val_set, test_set = test_set,
train_rows = train_rows, val_rows = val_rows, test_rows = test_rows))
}
}
# Generate three datasets in the long format for the linear model
data_sources = train_val_test(A_depth2_small, rseed = 5748)
A_depth_train <- data_sources$train_set
A_depth_val <- data_sources$val_set
A_depth_test <- data_sources$test_set
# Generate three wide datasets for the regularized models
A_depth_train_wide <- df_wide(A_depth_train, fact_expl_var_ids =c("Order", "Temperature", "Moisture", "Parent.material", "LULC"))
A_depth_val_wide <- df_wide(A_depth_val, fact_expl_var_ids =c("Order", "Temperature", "Moisture", "Parent.material", "LULC"))
A_depth_test_wide <- df_wide(A_depth_test, fact_expl_var_ids =c("Order", "Temperature", "Moisture", "Parent.material", "LULC"))
#' This function does three main things:
#' 1. It runs all possible linear regression models on the training set
#' 2. It runs ridge models on the training set for various values of the tuning parameter
#' 3. It runs lasso models on the training set for various values of the tuning parameter
#'
#' @param train_set - the input training dataset for the linear model
#' @param train_set_wide - the input training dataset for the regularized models
#' @param resp_var_id - the response variable
#' @param num_expl_var_ids - the numeric explanatory variables
#' @param fact_expl_var_ids - the factor explanatory variables
#' @param lin_mod_ind - should linear models be run? Defaults to yes.
#' @param ridge_ind - should ridge models be run? Defaults to yes.
#' @param lasso_ind - should lasso models be run? Defaults to yes.
#' @param ridge_max - maximum value of the ridge tuning parameter. Default is 20.
#' @param lasso_max - maximum value of the lasso tuning parameter. Default is 20.
#' @param ridge_step - change in the ridge tuning parameter between runs. Default is 0.1.
#' @param lasso_step - change in the lasso tuning parameter between runs. Default is 0.1.
#'
#' @return - a list of linear, ridge, and lasso models
#' @export
#'
#' @examples - train_models <- all_subsets_regression(train_set =  A_depth_train, train_set_wide =  A_depth_train_wide, resp_var_id = c("A_depth"), fact_expl_var_ids = c("Order", "Temperature", "Moisture", "Parent.material", "LULC"), lin_mod_ind = 1, lasso_ind = 1, ridge_ind = 1)
all_subsets_regression <- function(train_set, train_set_wide, resp_var_id, num_expl_var_ids = NULL, fact_expl_var_ids, lin_mod_ind = 1, ridge_ind = 1, lasso_ind = 1, ridge_max = 20, lasso_max = 20, ridge_step = 0.1, lasso_step = 0.1) {
# Combine both variable IDs into one vector
expl_var_ids <- c(num_expl_var_ids, fact_expl_var_ids)
# Extract response and explanatory variables
resp_var <- train_set[resp_var_id]
expl_var <- train_set[expl_var_ids]
# Create a new dataset
# dataset_small <- cbind(resp_var, num_expl_var, fact_expl_var)
# Generate a storage list for all the linear models
lm_list = list()
# Generate a storage list for all the lasso models
lasso_list = list()
# Generate a storage list for all the lasso models
ridge_list = list()
# print(expl_var)
# Run the linear models
if (lin_mod_ind == 1){
# Initialize a count variable
count = 1
# Run all possible regressions
for (i in 1:length(expl_var)){
# Generate a list of possible combinations
all_combos <- combn(length(expl_var),i)
# print(all_combos)
for (j in 1:ncol(all_combos)){
# Cut down the explanatory variables
expl_var_cut <- expl_var[,all_combos[,j]]
# Get the relevant dataset
data_in = cbind(resp_var, expl_var_cut)
# print(data_in)
expl_var_names = colnames(expl_var[all_combos[,j]])
colnames(data_in) <- c(resp_var_id, expl_var_names)
# Generate a formula
f <- as.formula(
paste(resp_var_id, paste("."),
sep = " ~ "))
# print(f)
# Feed into the model
lm_list[[count]] <- lm(f, data = data_in)
# lm_list[[count]] <- lm(A_depth ~ ., data = data_in)
count = count + 1
}
}
}
if (lasso_ind == 1){
# Initialize a count variable
count = 1
# Generate a sequence of tuning parameters
lasso_seq = seq(0, lasso_max, by = lasso_step)
# Feed this into the elastic net code
for (i in 1:length(lasso_seq)){
# Alpha == 1 is a lasso penalty
lasso_list[[count]] <- glmnet(x = train_set_wide[,c(2:ncol(train_set_wide))], y = train_set_wide[,1], lambda = lasso_seq[i],family = "gaussian", alpha = 1)
# Update the count variable
count = count + 1
}
}
if (ridge_ind == 1){
# Initialize a count variable
count = 1
# Generate a sequence of tuning parameters
ridge_seq = seq(0, ridge_max, by = ridge_step)
# Feed this into the elastic net code
for (i in 1:length(ridge_seq)){
# Alpha == 0 is a ridge penalty
ridge_list[[count]] <- glmnet(x = train_set_wide[,c(2:ncol(train_set_wide))], y = train_set_wide[,1], lambda = ridge_seq[i],family = "gaussian", alpha = 0)
# Update the count variable
count = count + 1
}
}
# Export the lists
return(list(lm_list = lm_list, ridge_list = ridge_list, lasso_list = lasso_list))
# return(list(lm_list = lm_list, ridge_list = ridge_list, lasso_list = lasso_list))
# return(train_set_small)
}
# function(train_set, train_set_long, resp_var_id, num_expl_var_ids, fact_expl_var_ids, lin_mod_ind = 1, lasso_ind = 1, ridge_ind = 1, lasso_max = 20, ridge_max = 20, lasso_step = 0.1, ridge_step = 0.1) {
train_models <- all_subsets_regression(train_set =  A_depth_train, train_set_wide =  A_depth_train_wide, resp_var_id = c("A_depth"), fact_expl_var_ids = c("Order", "Temperature", "Moisture", "Parent.material", "LULC"), lin_mod_ind = 1, lasso_ind = 1, ridge_ind = 1)
library(dplyr)
library(glmnet)
library(MASS)
#' This function does three main things:
#' 1. It runs all possible linear regression models on the training set
#' 2. It runs ridge models on the training set for various values of the tuning parameter
#' 3. It runs lasso models on the training set for various values of the tuning parameter
#'
#' @param train_set - the input training dataset for the linear model
#' @param train_set_wide - the input training dataset for the regularized models
#' @param resp_var_id - the response variable
#' @param num_expl_var_ids - the numeric explanatory variables
#' @param fact_expl_var_ids - the factor explanatory variables
#' @param lin_mod_ind - should linear models be run? Defaults to yes.
#' @param ridge_ind - should ridge models be run? Defaults to yes.
#' @param lasso_ind - should lasso models be run? Defaults to yes.
#' @param ridge_max - maximum value of the ridge tuning parameter. Default is 20.
#' @param lasso_max - maximum value of the lasso tuning parameter. Default is 20.
#' @param ridge_step - change in the ridge tuning parameter between runs. Default is 0.1.
#' @param lasso_step - change in the lasso tuning parameter between runs. Default is 0.1.
#'
#' @return - a list of linear, ridge, and lasso models
#' @export
#'
#' @examples - train_models <- all_subsets_regression(train_set =  A_depth_train, train_set_wide =  A_depth_train_wide, resp_var_id = c("A_depth"), fact_expl_var_ids = c("Order", "Temperature", "Moisture", "Parent.material", "LULC"), lin_mod_ind = 1, lasso_ind = 1, ridge_ind = 1)
all_subsets_regression <- function(train_set, train_set_wide, resp_var_id, num_expl_var_ids = NULL, fact_expl_var_ids, lin_mod_ind = 1, ridge_ind = 1, lasso_ind = 1, ridge_max = 20, lasso_max = 20, ridge_step = 0.1, lasso_step = 0.1) {
# Combine both variable IDs into one vector
expl_var_ids <- c(num_expl_var_ids, fact_expl_var_ids)
# Extract response and explanatory variables
resp_var <- train_set[resp_var_id]
expl_var <- train_set[expl_var_ids]
# Create a new dataset
# dataset_small <- cbind(resp_var, num_expl_var, fact_expl_var)
# Generate a storage list for all the linear models
lm_list = list()
# Generate a storage list for all the lasso models
lasso_list = list()
# Generate a storage list for all the lasso models
ridge_list = list()
# print(expl_var)
# Run the linear models
if (lin_mod_ind == 1){
# Initialize a count variable
count = 1
# Run all possible regressions
for (i in 1:length(expl_var)){
# Generate a list of possible combinations
all_combos <- combn(length(expl_var),i)
# print(all_combos)
for (j in 1:ncol(all_combos)){
# Cut down the explanatory variables
expl_var_cut <- expl_var[,all_combos[,j]]
# Get the relevant dataset
data_in = cbind(resp_var, expl_var_cut)
# print(data_in)
expl_var_names = colnames(expl_var[all_combos[,j]])
colnames(data_in) <- c(resp_var_id, expl_var_names)
# Generate a formula
f <- as.formula(
paste(resp_var_id, paste("."),
sep = " ~ "))
# print(f)
# Feed into the model
lm_list[[count]] <- lm(f, data = data_in)
# lm_list[[count]] <- lm(A_depth ~ ., data = data_in)
count = count + 1
}
}
}
if (lasso_ind == 1){
# Initialize a count variable
count = 1
# Generate a sequence of tuning parameters
lasso_seq = seq(0, lasso_max, by = lasso_step)
# Feed this into the elastic net code
for (i in 1:length(lasso_seq)){
# Alpha == 1 is a lasso penalty
lasso_list[[count]] <- glmnet(x = train_set_wide[,c(2:ncol(train_set_wide))], y = train_set_wide[,1], lambda = lasso_seq[i],family = "gaussian", alpha = 1)
# Update the count variable
count = count + 1
}
}
if (ridge_ind == 1){
# Initialize a count variable
count = 1
# Generate a sequence of tuning parameters
ridge_seq = seq(0, ridge_max, by = ridge_step)
# Feed this into the elastic net code
for (i in 1:length(ridge_seq)){
# Alpha == 0 is a ridge penalty
ridge_list[[count]] <- glmnet(x = train_set_wide[,c(2:ncol(train_set_wide))], y = train_set_wide[,1], lambda = ridge_seq[i],family = "gaussian", alpha = 0)
# Update the count variable
count = count + 1
}
}
# Export the lists
return(list(lm_list = lm_list, ridge_list = ridge_list, lasso_list = lasso_list))
# return(list(lm_list = lm_list, ridge_list = ridge_list, lasso_list = lasso_list))
# return(train_set_small)
}
# function(train_set, train_set_long, resp_var_id, num_expl_var_ids, fact_expl_var_ids, lin_mod_ind = 1, lasso_ind = 1, ridge_ind = 1, lasso_max = 20, ridge_max = 20, lasso_step = 0.1, ridge_step = 0.1) {
train_models <- all_subsets_regression(train_set =  A_depth_train, train_set_wide =  A_depth_train_wide, resp_var_id = c("A_depth"), fact_expl_var_ids = c("Order", "Temperature", "Moisture", "Parent.material", "LULC"), lin_mod_ind = 1, lasso_ind = 1, ridge_ind = 1)
#' A Function to Compute the Best Linear Model on the Validation Set
#'
#' @param lm_list - a list of candidate linear models from all_subsets_regression
#' @param val_set - the validation set for the linear models.
#' @param resp_var - the response variable
#'
#' @return - A list with the optimal model and the MSE for all the models
#' @export
#'
#' @examples - best_lm(train_models$lm_list, val_set = A_depth_val, resp_var = c("A_depth"))
best_lm <- function(lm_list, val_set, resp_var) {
# Create an MSE storage vector
MSE_sto_vec <- rep(0, length(lm_list))
# Loop through for all items in the list
for (i in 1:length(lm_list)){
# Compute predictions
lm_pred <- predict.lm(object = lm_list[[i]], newdata = val_set)
# Extract the response variable
# val_set[resp_var]
# Compute MSE
MSE_sto_vec[i] <- sum(((val_set[resp_var] - lm_pred)^2))
# MSE_sto_vec[i] <- sum(((val_set$A_depth - lm_pred)^2))
}
# Find the minimum value
opt_mod <- which.min(MSE_sto_vec)
# Return the optimal value and the MSE vector
return(list(opt_mod = opt_mod, MSE_sto_vec = MSE_sto_vec))
}
val_list_lm <-  best_lm(train_models$lm_list, val_set = A_depth_val, resp_var = c("A_depth"))
#### A Function to Compute the Best Ridge Model on the Validation Set
#' A Function to Compute the Best Ridge Model on the Validation Set
#'
#' @param ridge_list - a list of candidate ridge models from all_subsets_regression
#' @param val_set_wide - the wide version of the validation set
#'
#' @return - A list with the optimal model and the MSE for all the models
#' @export
#'
#' @examples - best_ridge(train_models$ridge_list, val_set_wide = A_depth_val_wide)
best_ridge <- function(ridge_list, val_set_wide) {
# Combine both variable IDs into one vector
# expl_var_ids <- c(num_expl_var_ids, fact_expl_var_ids)
# Create an MSE storage vector
MSE_sto_vec <- rep(0, length(ridge_list))
# Store the response variable
val_set_resp <- val_set_wide[,1]
# Cut down the validation set by eliminating the first column
val_set_expl <- val_set_wide[,-1]
# Convert to a matrix and append a column
val_set_expl <- as.matrix(cbind(const = 1, val_set_expl))
# Loop through for all items in the list
for (i in 1:length(ridge_list)){
# Extract model coefficients
ridge_coeffs <- as.matrix(coef(ridge_list[[i]]))
# print(ridge_coeffs)
# Compute predictions
ridge_pred <- val_set_expl %*% ridge_coeffs
# ridge_pred <- glmnet::predict(object = ridge_list[[i]], newdata = val_set_expl)
# Extract the response variable
# val_set[resp_var]
# Compute MSE
MSE_sto_vec[i] <- sum(((val_set_resp - ridge_pred)^2))
# MSE_sto_vec[i] <- sum(((val_set$A_depth - lm_pred)^2))
}
# Find the minimum value
opt_mod <- which.min(MSE_sto_vec)
# Return the optimal value and the MSE vector
return(list(opt_mod = opt_mod, MSE_sto_vec = MSE_sto_vec))
}
val_list_ridge <- best_ridge(train_models$ridge_list, val_set_wide = A_depth_val_wide)
#' A Function to Compute the Best LASSO Model on the Validation Set
#'
#' @param lasso_list - a list of candidate ridge models from all_subsets_regression
#' @param val_set_wide - the wide version of the validation set
#'
#' @return - A list with the optimal model and the MSE for all the models
#' @export
#'
#' @examples - best_lasso(train_models$lasso_list, val_set_wide = A_depth_val_wide)
best_lasso <- function(lasso_list, val_set_wide) {
# Create an MSE storage vector
MSE_sto_vec <- rep(0, length(lasso_list))
# Store the response variable
val_set_resp <- val_set_wide[,1]
# Cut down the validation set by eliminating the first column
val_set_expl <- val_set_wide[,-1]
# Convert to a matrix and append a column
val_set_expl <- as.matrix(cbind(const = 1, val_set_expl))
# Loop through for all items in the list
for (i in 1:length(lasso_list)){
# Extract model coefficients
lasso_coeffs <- as.matrix(coef(lasso_list[[i]]))
# Compute predictions
lasso_pred <- val_set_expl %*% lasso_coeffs
# Compute MSE
MSE_sto_vec[i] <- sum(((val_set_resp - lasso_pred)^2))
# MSE_sto_vec[i] <- sum(((val_set$A_depth - lm_pred)^2))
}
# Find the minimum value
opt_mod <- which.min(MSE_sto_vec)
# Return the optimal value and the MSE vector
return(list(opt_mod = opt_mod, MSE_sto_vec = MSE_sto_vec))
}
val_list_lasso <- best_lasso(train_models$lasso_list, val_set_wide = A_depth_val_wide)
val_list_lasso$opt_mod
#### Computing the optimal model on the test set
#' A function to compute the best model on the test set
#'
#' @param test_set - the test set computed earlier
#' @param test_set_wide - the wide version of the test set computed earlier
#' @param resp_var - the respnose variable
#' @param lm_list - a list of all candidate linear models, though only the best one from the val set is used
#' @param ridge_list - a list of all candidate ridge models, though only the best one from the val set is used
#' @param lasso_list  - a list of all candidate lasso models, though only the best one from the val set is used
#' @param lm_opt - the number of the best linear model from the val set
#' @param ridge_opt - the number of the best ridge model from the val set
#' @param lasso_opt - the number of the best lasso model from the val set
#' @param n_class - the number of models used. Do NOT change this without a good reason.
#'
#' @return - the best model. 1 = linear, 2 = ridge, 3 = lasso.
#' @export
#'
#' @examples - best_model_overall(test_set = A_depth_test, test_set_wide = A_depth_test_wide, resp_var = "A_depth", lm_list = train_models$lm_list, ridge_list = train_models$ridge_list,  lasso_list = train_models$lasso_list,  lm_opt= val_list_lm$opt_mod, ridge_opt = val_list_ridge$opt_mod,  lasso_opt = val_list_lasso$opt_mod)
best_model_overall <- function(test_set, test_set_wide, resp_var, lm_list, ridge_list, lasso_list, lm_opt, ridge_opt, lasso_opt, n_class = 3) {
# Extract the best three models from each
lm_best_mod <- lm_list[[lm_opt]]
ridge_best_mod <- ridge_list[[ridge_opt]]
lasso_best_mod <- lasso_list[[lasso_opt]]
# Create an MSE storage vector
MSE_sto_vec <- rep(0, n_class)
## LM
# Compute the predictions of the best lm model
lm_pred <- predict.lm(object = lm_best_mod, newdata = test_set)
# Compute MSE for LM
MSE_sto_vec[1] <- sum(((test_set[resp_var] - lm_pred)^2))
## Ridge and Lasso
# Store the response variable
test_set_resp_wide <- test_set_wide[,1]
# Cut down the validation set by eliminating the first column
test_set_expl_wide <- test_set_wide[,-1]
# Convert to a matrix and append a column
test_set_expl_wide <- as.matrix(cbind(const = 1, test_set_expl_wide))
## RIDGE ##
# Extract ridge coefficients
ridge_coeffs <- as.matrix(coef(ridge_best_mod))
# Compute the prediction
ridge_pred <-  test_set_expl_wide %*% ridge_coeffs
# Compute MSE for Ridge
MSE_sto_vec[2] <- sum(((test_set_resp_wide - ridge_pred)^2))
## Lasso ##
# Extract ridge coefficients
lasso_coeffs <- as.matrix(coef(lasso_best_mod))
# Compute the prediction
lasso_pred <-  test_set_expl_wide %*% lasso_coeffs
# Compute MSE for Ridge
MSE_sto_vec[3] <- sum(((test_set_resp_wide - lasso_pred)^2))
# Find the minimum value
opt_mod <- which.min(MSE_sto_vec)
# Return the optimal value and the MSE vector
return(list(opt_mod = opt_mod, MSE_sto_vec = MSE_sto_vec))
}
best_model_overall(test_set = A_depth_test, test_set_wide = A_depth_test_wide, resp_var = "A_depth",
lm_list = train_models$lm_list, ridge_list = train_models$ridge_list,
lasso_list = train_models$lasso_list,
lm_opt= val_list_lm$opt_mod, ridge_opt = val_list_ridge$opt_mod,
lasso_opt = val_list_lasso$opt_mod)
val_list_ridge$opt_mod
train_models$ridge_list
val_list_ridge$opt_mod
train_models$ridge_list[[10]]
train_models$ridge_list[[10]] %>% summary()
winner <- train_models$ridge_list[[10]]
winner$beta
levels(A_depth_train$Order)
levels(A_depth2$Order)
train_models$lm_list[[31]]
# setwd("~/Pringle Lab/1PROJECTS/Nitrogen Budget/R docs/2022")
#setwd("~/Pringle Lab/1PROJECTS/Nitrogen Budget/R docs/2022")
require(ggplot2)
require(dplyr)
require(tibble)
library(readr)
require(gridExtra)
BiomDica <- read.csv("221026_Biomass_GAS_CN_DICA.csv", header = T)
