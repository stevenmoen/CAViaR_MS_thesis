---
title: "M.S. Thesis - First Draft"
author: "Steven Moen"
output: html_notebook
---

# Background and Introduction

Value-at-risk modeling, or VaR, is a commonly used tool to measure riskiness in a financial institution. CAViaR was a new take on risk modeling originally proposed by Robert Engle and Simone Manganelli in 2004 which builds upon the literature of modeling regression quantiles in settings which are essential 

and agree with the basic premise of the CAViaR paper that many of the nonparametric historical simulation methods don’t have attractive statistical properties and are chosen more for experimental qualities (an example of this in the 1998 paper by Boudoukh, Richardson, and Whitelaw cited by Engle which uses a semiparametric "hybrid" approach)
I think trying to think too narrowly about VaR isn't a very interesting statistical problem - in a sense, VaR is just a useful summarization of quantile risk put into dollar figures for easy consumption
In my opinion, the most interesting question in the CAViaR paper isn’t VaR per se, it’s testing their theorems under relaxations of their assumptions to better understand their approach to extreme value theory. In particular, the following analyses catch my eye:
How important is the assumption C2 on page 374 of the CAVIAR paper (which underlies Theorem 1 about the consistency of the estimator beta hat)? It states that "conditional on all of the past information...the error terms form a stationary process". It may be an interesting test to see how much nonstationarity affects the consistency of the estimator beta hat. I could do this via simulation and see if there's a lot of value here, which may lead us to a more fundamental result.
In my opinion, theorems 4 and 5 (p. 371) are the most important in the paper because they state that the DQ_IS and DQ_OOS quantities are pivotals asymptotically. But it begs the question of how much does that matter, especially in the context of VaR, which in all likelihood is working with limited data? Therefore, it seems to me that the DQ8 and DQ9 assumptions are the shakiest assumptions required for theorem 5. It might be interesting to see how far off from a chi-squared distribution real-world data actually is. This may allow us to develop "rules of thumb" about the accuracy of these asymptotic distributions.

```{r}

# Read in relevant libraries
library(microbenchmark)
library(data.table)
library(quantmod)
library(ggplot2)
library(tseries)
library(zoo)
library(magrittr)
library(dplyr)
library(kableExtra)
library(formattable)
library(quantreg)
library(MTS)

# Set up working directory
setwd("~/Documents/GitHub/CaviaR")
source('caviar_SM.R')
```

```{r}
#' This is a function which pulls data for use
#'
#' @param symbol - symbol to pull
#' @param compl_case - defaults to true...only includes complete cases in the data
#' @param adj_close - use adjusted closing prices. Default is yes.
#' @param log_return - use log return? Default is yes.
#'
#' @return - a data frame which can be fed into later functions
#' @export
#'
#' @examples - data_pull("SPY")
data_pull = function(symbol, compl_case = 1, adj_close = 1, log_return = 1, start_date = "1900-01-01", end_date = Sys.Date()){
  # Pull in data from quantmod
  response_pull = getSymbols(symbol, auto.assign = FALSE, from = start_date, to = end_date)
  # Get adjusted closing price
  if (adj_close == TRUE){
    df = Ad(response_pull)
  } else {
    df = Cl(response_pull)
  }
  # Return complete cases only 
  if (compl_case == TRUE){
    df = df[complete.cases(df), ]
  } else{
    df = df
  }
  # Calculate log return of data
  if (log_return == TRUE){
    lr = log(df[,1]/shift(df[,1], 1, type = "lag"))
    # Combine data
    df_out = cbind(df, lr)
    # Rename the data 
    colnames(df_out) <- c(sym=symbol, paste0(symbol, "_log_return"))
  } else{
    df_out = df
  }
  # Return data
  return(df_out)
}

ibm = data_pull("IBM", start_date = "1986-04-06", end_date = "1999-04-08")
# ibm

```

```{r}
#' Pull the data and run the caviar function on it
#'
#' @param input_data - data to use in the function
#' @param range_data - range of the data to use
#'
#' @return - a list of values from the caviar function
#' @export
#'
#' @examples - caviar_pull(spy)
caviar_pull = function(input_data, range_data = (2:dim(input_data)[1])){
  # Run the caviar data
  caviar <- caviarOptim(input_data[range_data,2])
  return(caviar)
}

```


```{r}
#' Function for producing rolling predictions
#' Model 1 = Symmetric Absolute Value, 2 = Asymmetric slope, 3 = Indirect GARCH, 4 = Adaptive
#'
#' @param input_data - input data from the previous function
#' @param range_data - range of the data to consider
#' @param nfcst - number of forecasts to make
#' @param model - model to use (integers 1 through 4). Defaults to 1. 
#' @param level - level of significance to use.
#' @param G - argument for the k parameter in the 4th model (adaptive). Default is 5
#'
#' @return - an xts object which contains rolling CAViaR predictions
#' @export
#'
#' @examples - rolling_predictions(spy, nfcst = 22)
rolling_predictions = function(input_data, range_data = (2:dim(input_data)[1]), nfcst = 250, model =1, level = 0.01, G = 5, col = 2){
  # Run the varpredict function
  varpredict <- rollapplyr(input_data[range_data,col], length(range_data) - nfcst, caviarOptim, model, level, predict = 1, k = G) %>% lag
  # Eliminate NAs
  # pred_no_na = na.omit(varpredict)
  # Return the data
  # return(pred_no_na)
  return(varpredict)
}

# mth_series[2:100, 1]

```

```{r}
#' Function to Calculate Loss
#'
#' @param symbol - symbol to work with from quantmod. Must be in quotations to work
#' @param start_dt - start date of the data to build the forecast on 
#' @param end_dt - end date of the data to build the forecast on  
#' @param nfcst - number of data points to use in the forecast
#' @param model - model to use. Defaults to 1
#' @param level - level of significance. Defaults to 1%
#' @param G - argument for the k parameter in the 4th model (adaptive). Default is 5
#'
#' @return - loss using absolute value
#' @export - a plot of the data
#'
#' @examples
loss_calc = function(symbol, start_dt, end_dt, nfcst, model = 1, level = 0.01, G = 5){
  # Pull in the data
  raw_data = data_pull(symbol, start_date = start_dt, end_date = end_dt)
  # Forecast based on the data
  fcst = na.omit(rolling_predictions(raw_data, nfcst = nfcst, model = model, level = level, G = G))*(-1)
  # Extract actuals
  act = tail(raw_data, n = nfcst)[,2]
  # Join the two together and rename
  join = merge(fcst,act,all=TRUE)
  colnames(join) <- c("Fcst_VaR", "Act_Return")
  # print(join)
  # Calculate the losses
  loss = abs(sum(ifelse(act > fcst, level, (-1)*(1-level))))
  # Plot the data
  plot = plot.xts(join, col = c("red", "black"), lty = c(2,1), main = "Log Return from the SPY vs. Fcst. VaR",grid.col = NA, legend.loc = "bottomleft")
  return(list(loss, plot, act, fcst))
}

```



```{r}
#' Stock simulation for use in CAViaR simulation study
#'
#' @param n1 - number of data points in first series
#' @param n2 - number of data points in second series
#' @param s1_sd - standard deviation of first series
#' @param s2_sd - standard deviation of second series 
#' @param s1_mean - mean of first series 
#' @param s2_mean - mean of second series 
#' @param rseed - random seed for reproducibility
#'
#' @return - output time series
#' @export
#'
#' @examples hth_series = stock_simul(100, 25, 10, 10)
stock_simul = function(n1, n2,s1_gr, s2_gr, s1_sd, s2_sd, rseed = 1234){
  # Set random seed
  set.seed(rseed)
  # Initialize vector and put first value in the series
  s1 <- rep(1, n1)
  # Simulate first series
  for (i in 2:n1){
    s1[i] <- s1[i-1]*(1+s1_gr) + rnorm(1, mean = 0, sd = s1_sd)
  }
  # Initialize vector
  s2 <- rep(1, n2)
  # Put first value in
  s2[1] <- s1[n1]*(1+s2_gr) + rnorm(1, mean = 0, sd = s2_sd)
  # Simulate second series
  for (i in 2:n2){
    s2[i] <- s2[i-1]*(1+s2_gr) + rnorm(1, mean = 0, sd = s2_sd)
  }
  # Combine the two series
  long <- as.matrix(c(s1, s2))
  # Export the data
  return(long)
}

# test <- lth_series[500,1]*(1)

```



```{r}
# Low to high volatility series
lth_series = stock_simul(500, 25, 0.001, -0.005, 0.001, 0.01)
mth_series = stock_simul(500, 25, 0.001, -0.005, 0.005, 0.01)
hth_series = stock_simul(500, 25,  0.001, -0.005, 0.01, 0.01)

# lth_series
```


```{r}
# Plot the data
plot(lth_series, type = "l", ylim = c(0.8, 1.8), xlab = "Days", ylab = "Growth (Indexed to 1)", main = "Growth Comparison of 3 Simulated Stocks")
lines(mth_series, type = "l", col = "red")
lines(hth_series, type = "l", col = "blue")
legend("bottomright", legend=c("Low Vol. Stock", "Mid Vol. Stock", "High Vol. Stock"),col=c("black", "red", "blue"), lty = c(1,1,1))

# Transform the data to be differenced log data
dl_lth <- diff(log(lth_series))
dl_mth <- diff(log(mth_series))
dl_hth <- diff(log(hth_series))

# Plot the differenced data
plot(dl_lth, type = "l", xlab = "Days", ylab = "Differenced Log Growth", ylim = c(-0.04, 0.04), main = "Log  Growth Rate Comparisons of 3 Simulated Stocks")
lines(dl_mth, type = "l", col = "red")
lines(dl_hth, type = "l", col = "blue")
legend("bottomright", legend=c("Low Vol. Stock", "Mid Vol. Stock", "High Vol. Stock"),col=c("black", "red", "blue"), lty = c(1,1,1))

# dl_lth
```


```{r}
# Let's test the series
m1_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 1, G = 10, col = 1)
m2_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 2, G = 10, col = 1)
m3_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 3, G = 10, col = 1)
m4_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 4, G = 10, col = 1)

# Let's test the series
m1_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 1, G = 10, col = 1)
m2_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 2, G = 10, col = 1)
m3_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 3, G = 10, col = 1)
m4_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 4, G = 10, col = 1)

# Let's test the series
m1_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 1, G = 10, col = 1)
m2_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 2, G = 10, col = 1)
m3_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 3, G = 10, col = 1)
m4_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 4, G = 10, col = 1)
```

```{r}

```

```{r}
# First plot - LTH VaR
plot(dl_lth[500:524], type = "l", ylim = c(-0.04, 0.04), xlab = "Forecast Day", ylab = "Log Return", main = "VaR Forecast in High-Volatility Environment for a Low-Vol. Stock")
legend("topright", legend=c("Simulated Return", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "red", "green", "blue", "purple"), lty = c(1,2,2,2,2))
lines(m1_lth[2:26]*(-1), col = "red", lty = 2)
lines(m2_lth[2:26]*(-1), col = "green", lty = 2)
lines(m3_lth[2:26]*(-1), col = "blue", lty = 2)
lines(m4_lth[2:26]*(-1), col = "purple", lty = 2)

# Second plot - MTH VaR
plot(dl_mth[500:524], type = "l", ylim = c(-0.04, 0.04), xlab = "Forecast Day", ylab = "Log Return", main = "VaR Forecast in High-Volatility Environment for a Mid-Vol. Stock")
legend("topright", legend=c("Simulated Return", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "red", "green", "blue", "purple"), lty = c(1,2,2,2,2))
lines(m1_mth[2:26]*(-1), col = "red", lty = 2)
lines(m2_mth[2:26]*(-1), col = "green", lty = 2)
lines(m3_mth[2:26]*(-1), col = "blue", lty = 2)
lines(m4_mth[2:26]*(-1), col = "purple", lty = 2)

# Second plot - HTH VaR
plot(dl_hth[500:524], type = "l", ylim = c(-0.04, 0.04), xlab = "Forecast Day", ylab = "Log Return", main = "VaR Forecast in High-Volatility Environment for a Low-Vol. Stock")
legend("topright", legend=c("Simulated Return", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "red", "green", "blue", "purple"), lty = c(1,2,2,2,2))
lines(m1_hth[2:26]*(-1), col = "red", lty = 2)
lines(m2_hth[2:26]*(-1), col = "green", lty = 2)
lines(m3_hth[2:26]*(-1), col = "blue", lty = 2)
lines(m4_hth[2:26]*(-1), col = "purple", lty = 2)

```

That doesn't look promising. What if we just look at the last 10?

```{r}
# Let's test the series
m1_hth_sm = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 10, model = 1, G = 10, col = 1)
```

```{r}
plot(dl_hth[515:524], type = "l", ylim = c(-0.04, 0.04))
lines(m1_hth_sm[1:10], col = "red")
```

Not good. Let's look at an in-sample test.

```{r}
# Let's test the series in sample
m1_hth_sm_is_05 = rolling_predictions(dl_hth, range_data = (1:400), nfcst = 10, model = 1, level = 0.05, G = 10, col = 1)
m1_hth_sm_is_01 = rolling_predictions(dl_hth, range_data = (1:400), nfcst = 10, model = 1, level = 0.01, G = 10, col = 1)


```

```{r}
plot(dl_hth[390:399], type = "l", ylim = c(-0.04, 0.04))
lines(m1_hth_sm_is_05[1:10], col = "red")
lines(m1_hth_sm_is_01[1:10], col = "green")
```

```{r}
cav
```



```{r}
#' Stock simulation for use in CAViaR simulation study
#'
#' @param n1 - number of data points in first series
#' @param n2 - number of data points in second series
#' @param s1_sd - standard deviation of first series
#' @param s2_sd - standard deviation of second series 
#' @param s1_mean - mean of first series 
#' @param s2_mean - mean of second series 
#' @param rseed - random seed for reproducibility
#'
#' @return - output time series
#' @export
#'
#' @examples hth_series = stock_simul(100, 25, 10, 10)
stock_simul = function(n1, n2, s1_sd, s2_sd, s1_mean = 0, s2_mean = 0, rseed = 1234){
  # Set random seed
  set.seed(rseed)
  # Simulate first series
  for (i in 1:(n1-1)){
    
  }
  # s1 <- rnorm(n1, mean = s1_mean, sd = s1_sd)
  # Simulate second series
  s2 <- rnorm(n2, mean = s2_mean, sd = s2_sd)
  # Combine the two series
  long <- as.matrix(c(s1, s2))
  # Export the data
  return(long)
}


```


# Combining a Diffusion Index Model with the Quantile Regression Model

Response is IBM, pick 5 stocks for predictors.

```{r}
# Pull IBM
ibm = data_pull("IBM", start_date = "1986-04-06", end_date = "1999-04-08")

# Pull 10 stocks at random that existed in the DJIA over the timeframe
# Exxon
xom = data_pull("XOM", start_date = "1986-04-06", end_date = "1999-04-08")

# PG
pg = data_pull("PG", start_date = "1986-04-06", end_date = "1999-04-08")

# General Electric
ge = data_pull("GE", start_date = "1986-04-06", end_date = "1999-04-08")

# 3M
mmm = data_pull("MMM", start_date = "1986-04-06", end_date = "1999-04-08")

# Disney
dis = data_pull("DIS", start_date = "1986-04-06", end_date = "1999-04-08")

```

There is probably some work to be done with choosing these stocks. I only picked ones with data.

```{r}
# Plot the data
# plot.xts(ibm[,2])

```

Let's just see if there is any merit to just regressing one set of stocks on another.

```{r}
# Predictors
?SWfore
SWfore
```

The code that is used in a diffusion index model is as follows. The key is figuring out where to make the extension. I'll go through and comment everything and see what's what.

```{r}
#' Below is the modified diffusion index code.
#'
#' @param y - response variable
#' @param x - predictor variables
#' @param orig - forecast origin
#' @param m - number of diffusion indexes used
#' @param tau - VaR level to use; must be between 0 and 1
#'
#' @return - returns a list of variables for use in the diffusion index
#' @export
#'
#' @examples
mod_di = function (y, x, orig, m, tau) 
{
  # Converts the response variables into a matrix
  if (!is.matrix(x)) 
      x = as.matrix(x)
  # nT is number of t time-steps
  nT = dim(x)[1]
  # k is the number of diffusion indices used
  k = dim(x)[2]
  # Sanity checks to ensure that the origin isn't past the number of time points
  if (orig > nT) 
      orig = nT
  # Makes sure that there aren't more predictors than there variables in the dataset
  if (m > k) 
      m = k
  # Makes sure there are at least some variables
  if (m < 1) 
      m = 1
  # Subdivides the dataframe
  x1 = x[1:orig, ]
  # Calculates means of each row
  me = apply(x1, 2, mean)
  # Calculates standard deviations of each column
  se = sqrt(apply(x1, 2, var))
  # Creates a matrix x1, which normalizes all the columns. 
  # This may be an issue since it assumes that the distribution is sufficiently described by the first two moments
  x1 = x
  for (i in 1:k) {
      x1[, i] = (x1[, i] - me[i])/se[i]
  }
  V1 = cov(x1[1:orig, ])
  # Performs an eigen decomposition
  m1 = eigen(V1)
  # Selects eigenvalues
  sdev = m1$values
  # Selects eigenvectors
  M = m1$vectors
  # Makes a smaller matrix
  M1 = M[, 1:m]
  # This is the diffusion index model - [orig x p]*[p x m] = [orig x m]
  Dindex = x1 %*% M1
  # Cut down both the response and predictors to be a reasonable size
  y1 = y[1:orig]
  DF = Dindex[1:orig, ]
  # Apply the linear model - HERE is the key.
  # mm = lm(y1 ~ DF) - old function
  mm = rq(y1 ~ DF, tau = tau)
  # Puts coefficients in a matrix
  coef = matrix(mm$coefficients, (m + 1), 1)
  # Initializes yhat variables and MSE
  yhat = NULL
  MSE = NULL
  if (orig < nT) {
    # Creates a nfcst by (m+1) matrix
    newx = cbind(rep(1, (nT - orig)), Dindex[(orig + 1):nT, 
        ])
    # [nfcstx(m+1)]*[(m+1)x1] = [nfcstx1]
    yhat = newx %*% coef
    # Calculates errors
    err = y[(orig + 1):nT] - yhat
    MSE = mean(err^2)
    cat("MSE of out-of-sample forecasts: ", MSE, "\n")
  }
  SWfore <- list(coef = coef, yhat = yhat, MSE = MSE, loadings = M1, 
      DFindex = Dindex)
}



# ?cbind
```

Now that we have the function, let's see if we can get it to work. First, we'll need to compile all of our data

```{r, cache = TRUE}
#' This is a function which creates a data frame for the response and explanatory variables that we'll feed into the diffusion index
#'
#' @param symbol_list - a list of symbols recognizable by the 
#' @param resp_var - the response variable we'd like to forecast; default is SPY
#' @param compl_case - defaults to true...only includes complete cases in the data
#'
#' @return - a data frame which can be fed into the SWfore function
#' @export
#'
#' @examples - diff_index_df(c("XLF", "XLE", "PSCT", "XLV", "VPU", "XLP", "IGF", "XWEB", "PPTY"))
diff_index_df = function(symbol_list, resp_var = "SPY", compl_case = 1, adj_close = 1, resp_adj_close = 1){
  # Pull in response variable
  response_pull = getSymbols(resp_var, auto.assign = FALSE)
  # Get adjusted closing price
  if (adj_close == TRUE){
    diff_df = Ad(response_pull)
  } else {
    diff_df = Cl(response_pull)
  }
  # Loop through the symbols and join in data
  for (i in 1:length(symbol_list)){
    # Pull closing price
    expl_pull = getSymbols(symbol_list[i], auto.assign = FALSE)
    # Extract closing price - 4th element
    if (adj_close == TRUE){
      expl_cl = Ad(expl_pull)
    } else {
      expl_cl = Cl(expl_pull)
    }
    # Join the data
    diff_df = merge(diff_df, expl_cl, join = "left", fill = NA)
  }
  # Return complete cases only 
  if (compl_case == TRUE){
    diff_df_out = diff_df[complete.cases(diff_df), ]
  } else{
    diff_df_out = diff_df
  }
  # Difference data
  return(diff_df_out)
}
```

```{r, cache = TRUE}
#' Converts a diff_df into a data frame with approximate percentage changes diff(log(diff_df))
#'
#' @param diff_df - output of the diff_index_df function with complete cases
#'
#' @return - retuns the differenced data
#' @export
#'
#' @examples - pc_diff_index(test_compl) 

pc_diff_index = function(diff_df){
  # Difference the log of the data
  pc_diff_index = diff(log(diff_df))
  # Remove the first row
  pc_diff_index_out = pc_diff_index[-1,]
  return(pc_diff_index_out)
}

```



```{r}
df = diff_index_df(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG")
pc_df = pc_diff_index(df)
```

```{r}
# pc_df
```

Now, let's test it. Moment of truth.

```{r}
# See what the 5% and 1% VaR levels look like
var_5pc = mod_di(pc_df[,1], pc_df[,-1], 100, 2, 0.05)
var_1pc = mod_di(pc_df[,1], pc_df[,-1], 100, 2, 0.01)

# Create a plot
ts.plot(pc_df[101:400,1])
lines(var_5pc$yhat[1:300], col = "blue", lty = 2)
lines(var_1pc$yhat[1:300], col = "red", lty = 2)

# Calculate accuracy
var_1pc$yhat
```


# JUNK CODE

```{r}
# Testing to see how apply function works
test_mat = matrix(rep(seq(1,5),5), nrow=5, ncol = 5)
# ?as.matrix
# Test apply function. Row means should be 1 through 5, col means should all be 3
row_me = apply(test_mat, 1, mean)
col_me = apply(test_mat, 2, mean)
row_se = sqrt(apply(test_mat, 1, var))
col_se = sqrt(apply(test_mat, 2, var))

# print(c(row_me, col_me, row_se, col_se))

# Exactly as I expected.

# test_mat
```

```{r}
# Test the matrix multiplication
```


