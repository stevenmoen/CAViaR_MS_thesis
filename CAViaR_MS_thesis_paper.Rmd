---
title: "M.S. Thesis - First Draft"
author: "Steven Moen"
output: html_notebook
---

# Background and Introduction

Value-at-risk modeling, or VaR is a commonly used tool to measure riskiness in a financial institution. CAViaR was a new take on risk modeling originally proposed by Robert Engle and Simone Manganelli in 2004 which builds upon the literature of modeling regression quantiles in settings which are essential 

and agree with the basic premise of the CAViaR paper that many of the nonparametric historical simulation methods don’t have attractive statistical properties and are chosen more for experimental qualities (an example of this in the 1998 paper by Boudoukh, Richardson, and Whitelaw cited by Engle which uses a semiparametric "hybrid" approach)
I think trying to think too narrowly about VaR isn't a very interesting statistical problem - in a sense, VaR is just a useful summarization of quantile risk put into dollar figures for easy consumption
In my opinion, the most interesting question in the CAViaR paper isn’t VaR per se, it’s testing their theorems under relaxations of their assumptions to better understand their approach to extreme value theory. In particular, the following analyses catch my eye:
How important is the assumption C2 on page 374 of the CAVIAR paper (which underlies Theorem 1 about the consistency of the estimator beta hat)? It states that "conditional on all of the past information...the error terms form a stationary process". It may be an interesting test to see how much nonstationarity affects the consistency of the estimator beta hat. I could do this via simulation and see if there's a lot of value here, which may lead us to a more fundamental result.
In my opinion, theorems 4 and 5 (p. 371) are the most important in the paper because they state that the DQ_IS and DQ_OOS quantities are pivotals asymptotically. But it begs the question of how much does that matter, especially in the context of VaR, which in all likelihood is working with limited data? Therefore, it seems to me that the DQ8 and DQ9 assumptions are the shakiest assumptions required for theorem 5. It might be interesting to see how far off from a chi-squared distribution real-world data actually is. This may allow us to develop "rules of thumb" about the accuracy of these asymptotic distributions.

```{r}

# Read in relevant libraries
library(microbenchmark)
library(data.table)
library(quantmod)
library(ggplot2)
library(tseries)
library(zoo)
library(magrittr)
library(dplyr)
library(kableExtra)
library(formattable)
library(quantreg)
library(MTS)

# Set up working directory
setwd("~/Documents/GitHub/CaviaR")
source('caviar_SM.R')
```

```{r}
#' This is a function which pulls data for use
#'
#' @param symbol - symbol to pull
#' @param compl_case - defaults to true...only includes complete cases in the data
#' @param adj_close - use adjusted closing prices. Default is yes.
#' @param log_return - use log return? Default is yes.
#'
#' @return - a data frame which can be fed into later functions
#' @export
#'
#' @examples - data_pull("SPY")
data_pull = function(symbol, compl_case = 1, adj_close = 1, log_return = 1, start_date = "1900-01-01", end_date = Sys.Date()){
  # Pull in data from quantmod
  response_pull = getSymbols(symbol, auto.assign = FALSE, from = start_date, to = end_date)
  # Get adjusted closing price
  if (adj_close == TRUE){
    df = Ad(response_pull)
  } else {
    df = Cl(response_pull)
  }
  # Return complete cases only 
  if (compl_case == TRUE){
    df = df[complete.cases(df), ]
  } else{
    df = df
  }
  # Calculate log return of data
  if (log_return == TRUE){
    lr = log(df[,1]/shift(df[,1], 1, type = "lag"))
    # Combine data
    df_out = cbind(df, lr)
    # Rename the data 
    colnames(df_out) <- c(sym=bol, paste0(symbol, "_log_return"))
  } else{
    df_out = df
  }
  # Return data
  return(df_out)
}

ibm = data_pull("IBM", start_date = "1986-04-06", end_date = "1999-04-08")
# ibm

```

```{r}
#' Pull the data and run the caviar function on it
#'
#' @param input_data - data to use in the function
#' @param range_data - range of the data to use
#'
#' @return - a list of values from the caviar function
#' @export
#'
#' @examples - caviar_pull(spy)
caviar_pull = function(input_data, range_data = (2:dim(input_data)[1])){
  # Run the caviar data
  caviar <- caviarOptim(input_data[range_data,2])
  return(caviar)
}

```


```{r}
#' Function for producing rolling predictions
#' Model 1 = Symmetric Absolute Value, 2 = Asymmetric slope, 3 = Indirect GARCH, 4 = Adaptive
#'
#' @param input_data - input data from the previous function
#' @param range_data - range of the data to consider
#' @param nfcst - number of forecasts to make
#' @param model - model to use (integers 1 through 4). Defaults to 1. 
#' @param level - level of significance to use.
#' @param G - argument for the k parameter in the 4th model (adaptive). Default is 5
#'
#' @return - an xts object which contains rolling CAViaR predictions
#' @export
#'
#' @examples - rolling_predictions(spy, nfcst = 22)
rolling_predictions = function(input_data, range_data = (2:dim(input_data)[1]), nfcst = 250, model =1, level = 0.01, G = 5, col = 2){
  # Run the varpredict function
  varpredict <- rollapplyr(input_data[range_data,col], length(range_data) - nfcst, caviarOptim, model, level, predict = 1, k = G) %>% lag
  # Eliminate NAs
  # pred_no_na = na.omit(varpredict)
  # Return the data
  # return(pred_no_na)
  return(varpredict)
}

# mth_series[2:100, 1]

```

```{r}
#' Function to Calculate Loss
#'
#' @param symbol - symbol to work with from quantmod. Must be in quotations to work
#' @param start_dt - start date of the data to build the forecast on 
#' @param end_dt - end date of the data to build the forecast on  
#' @param nfcst - number of data points to use in the forecast
#' @param model - model to use. Defaults to 1
#' @param level - level of significance. Defaults to 1%
#' @param G - argument for the k parameter in the 4th model (adaptive). Default is 5
#'
#' @return - loss using absolute value
#' @export - a plot of the data
#'
#' @examples
loss_calc = function(symbol, start_dt, end_dt, nfcst, model = 1, level = 0.01, G = 5){
  # Pull in the data
  raw_data = data_pull(symbol, start_date = start_dt, end_date = end_dt)
  # Forecast based on the data
  fcst = na.omit(rolling_predictions(raw_data, nfcst = nfcst, model = model, level = level, G = G))*(-1)
  # Extract actuals
  act = tail(raw_data, n = nfcst)[,2]
  # Join the two together and rename
  join = merge(fcst,act,all=TRUE)
  colnames(join) <- c("Fcst_VaR", "Act_Return")
  # print(join)
  # Calculate the losses
  loss = abs(sum(ifelse(act > fcst, level, (-1)*(1-level))))
  # Plot the data
  plot = plot.xts(join, col = c("red", "black"), lty = c(2,1), main = "Log Return from the SPY vs. Fcst. VaR",grid.col = NA, legend.loc = "bottomleft")
  return(list(loss, plot, act, fcst))
}

```



```{r}
#' Stock simulation for use in CAViaR simulation study
#'
#' @param n1 - number of data points in first series
#' @param n2 - number of data points in second series
#' @param s1_sd - standard deviation of first series
#' @param s2_sd - standard deviation of second series 
#' @param s1_mean - mean of first series 
#' @param s2_mean - mean of second series 
#' @param rseed - random seed for reproducibility
#'
#' @return - output time series
#' @export
#'
#' @examples hth_series = stock_simul(100, 25, 10, 10)
stock_simul = function(n1, n2,s1_gr, s2_gr, s1_sd, s2_sd, rseed = 1234){
  # Set random seed
  set.seed(rseed)
  # Initialize vector and put first value in the series
  s1 <- rep(1, n1)
  # Simulate first series
  for (i in 2:n1){
    s1[i] <- s1[i-1]*(1+s1_gr) + rnorm(1, mean = 0, sd = s1_sd)
  }
  # Initialize vector
  s2 <- rep(1, n2)
  # Put first value in
  s2[1] <- s1[n1]*(1+s2_gr) + rnorm(1, mean = 0, sd = s2_sd)
  # Simulate second series
  for (i in 2:n2){
    s2[i] <- s2[i-1]*(1+s2_gr) + rnorm(1, mean = 0, sd = s2_sd)
  }
  # Combine the two series
  long <- as.matrix(c(s1, s2))
  # Export the data
  return(long)
}

# test <- lth_series[500,1]*(1)

```



```{r}
# Low to high volatility series
lth_series = stock_simul(500, 25, 0.001, -0.005, 0.001, 0.01)
mth_series = stock_simul(500, 25, 0.001, -0.005, 0.005, 0.01)
hth_series = stock_simul(500, 25,  0.001, -0.005, 0.01, 0.01)

# lth_series
```


```{r}
# Plot the data
plot(lth_series, type = "l", ylim = c(0.8, 1.8), xlab = "Days", ylab = "Growth (Indexed to 1)", main = "Growth Comparison of 3 Simulated Stocks")
lines(mth_series, type = "l", col = "red")
lines(hth_series, type = "l", col = "blue")
legend("bottomright", legend=c("Low Vol. Stock", "Mid Vol. Stock", "High Vol. Stock"),col=c("black", "red", "blue"), lty = c(1,1,1))

# Transform the data to be differenced log data
dl_lth <- diff(log(lth_series))
dl_mth <- diff(log(mth_series))
dl_hth <- diff(log(hth_series))

# Plot the differenced data
plot(dl_lth, type = "l", xlab = "Days", ylab = "Differenced Log Growth", ylim = c(-0.04, 0.04), main = "Log  Growth Rate Comparisons of 3 Simulated Stocks")
lines(dl_mth, type = "l", col = "red")
lines(dl_hth, type = "l", col = "blue")
legend("bottomright", legend=c("Low Vol. Stock", "Mid Vol. Stock", "High Vol. Stock"),col=c("black", "red", "blue"), lty = c(1,1,1))

# dl_lth
```


```{r}
# Let's test the series
m1_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 1, G = 10, col = 1)
m2_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 2, G = 10, col = 1)
m3_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 3, G = 10, col = 1)
m4_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 4, G = 10, col = 1)

# Let's test the series
m1_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 1, G = 10, col = 1)
m2_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 2, G = 10, col = 1)
m3_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 3, G = 10, col = 1)
m4_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 4, G = 10, col = 1)

# Let's test the series
m1_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 1, G = 10, col = 1)
m2_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 2, G = 10, col = 1)
m3_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 3, G = 10, col = 1)
m4_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 4, G = 10, col = 1)
```

```{r}

```

```{r}
# First plot - LTH VaR
plot(dl_lth[500:524], type = "l", ylim = c(-0.04, 0.04), xlab = "Forecast Day", ylab = "Log Return", main = "VaR Forecast in High-Volatility Environment for a Low-Vol. Stock")
legend("topright", legend=c("Simulated Return", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "red", "green", "blue", "purple"), lty = c(1,2,2,2,2))
lines(m1_lth[2:26]*(-1), col = "red", lty = 2)
lines(m2_lth[2:26]*(-1), col = "green", lty = 2)
lines(m3_lth[2:26]*(-1), col = "blue", lty = 2)
lines(m4_lth[2:26]*(-1), col = "purple", lty = 2)

# Second plot - MTH VaR
plot(dl_mth[500:524], type = "l", ylim = c(-0.04, 0.04), xlab = "Forecast Day", ylab = "Log Return", main = "VaR Forecast in High-Volatility Environment for a Mid-Vol. Stock")
legend("topright", legend=c("Simulated Return", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "red", "green", "blue", "purple"), lty = c(1,2,2,2,2))
lines(m1_mth[2:26]*(-1), col = "red", lty = 2)
lines(m2_mth[2:26]*(-1), col = "green", lty = 2)
lines(m3_mth[2:26]*(-1), col = "blue", lty = 2)
lines(m4_mth[2:26]*(-1), col = "purple", lty = 2)

# Second plot - HTH VaR
plot(dl_hth[500:524], type = "l", ylim = c(-0.04, 0.04), xlab = "Forecast Day", ylab = "Log Return", main = "VaR Forecast in High-Volatility Environment for a Low-Vol. Stock")
legend("topright", legend=c("Simulated Return", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "red", "green", "blue", "purple"), lty = c(1,2,2,2,2))
lines(m1_hth[2:26]*(-1), col = "red", lty = 2)
lines(m2_hth[2:26]*(-1), col = "green", lty = 2)
lines(m3_hth[2:26]*(-1), col = "blue", lty = 2)
lines(m4_hth[2:26]*(-1), col = "purple", lty = 2)

```

That doesn't look promising. What if we just look at the last 10?

```{r}
# Let's test the series
m1_hth_sm = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 10, model = 1, G = 10, col = 1)
```

```{r}
plot(dl_hth[515:524], type = "l", ylim = c(-0.04, 0.04))
lines(m1_hth_sm[1:10], col = "red")
```

Not good. Let's look at an in-sample test.

```{r}
# Let's test the series in sample
m1_hth_sm_is_05 = rolling_predictions(dl_hth, range_data = (1:400), nfcst = 10, model = 1, level = 0.05, G = 10, col = 1)
m1_hth_sm_is_01 = rolling_predictions(dl_hth, range_data = (1:400), nfcst = 10, model = 1, level = 0.01, G = 10, col = 1)


```

```{r}
plot(dl_hth[390:399], type = "l", ylim = c(-0.04, 0.04))
lines(m1_hth_sm_is_05[1:10], col = "red")
lines(m1_hth_sm_is_01[1:10], col = "green")
```

```{r}
cav
```



```{r}
#' Stock simulation for use in CAViaR simulation study
#'
#' @param n1 - number of data points in first series
#' @param n2 - number of data points in second series
#' @param s1_sd - standard deviation of first series
#' @param s2_sd - standard deviation of second series 
#' @param s1_mean - mean of first series 
#' @param s2_mean - mean of second series 
#' @param rseed - random seed for reproducibility
#'
#' @return - output time series
#' @export
#'
#' @examples hth_series = stock_simul(100, 25, 10, 10)
stock_simul = function(n1, n2, s1_sd, s2_sd, s1_mean = 0, s2_mean = 0, rseed = 1234){
  # Set random seed
  set.seed(rseed)
  # Simulate first series
  for (i in 1:(n1-1)){
    
  }
  # s1 <- rnorm(n1, mean = s1_mean, sd = s1_sd)
  # Simulate second series
  s2 <- rnorm(n2, mean = s2_mean, sd = s2_sd)
  # Combine the two series
  long <- as.matrix(c(s1, s2))
  # Export the data
  return(long)
}


```


# Combining a Diffusion Index Model with the Quantile Regression Model

Response is IBM, pick 5 stocks for predictors.

```{r}
# Pull IBM
ibm = data_pull("IBM", start_date = "1986-04-06", end_date = "1999-04-08")

# Pull 10 stocks at random that existed in the DJIA over the timeframe
# Exxon
xom = data_pull("XOM", start_date = "1986-04-06", end_date = "1999-04-08")

# PG
pg = data_pull("PG", start_date = "1986-04-06", end_date = "1999-04-08")

# General Electric
ge = data_pull("GE", start_date = "1986-04-06", end_date = "1999-04-08")

# 3M
mmm = data_pull("MMM", start_date = "1986-04-06", end_date = "1999-04-08")

# Disney
dis = data_pull("DIS", start_date = "1986-04-06", end_date = "1999-04-08")

```

There is probably some work to be done with choosing these stocks. I only picked ones with data.

```{r}
# Plot the data
# plot.xts(ibm[,2])

```

Let's just see if there is any merit to just regressing one set of stocks on another.

```{r}
# Predictors
?SWfore
SWfore
```

