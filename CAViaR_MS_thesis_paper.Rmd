---
title: "M.S. Thesis - First Draft"
author: "Steven Moen"
date: "Friday, January 24th, 2020"
output: html_notebook
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE, cache = TRUE)
```

# Background and Introduction

Value-at-risk modeling, or VaR, is a commonly used tool to measure riskiness in a financial institution. CAViaR was a new take on risk modeling originally proposed by Robert Engle and Simone Manganelli in 2004 which builds upon the literature of modeling regression quantiles in settings which are essential. 

and agree with the basic premise of the CAViaR paper that many of the nonparametric historical simulation methods don’t have attractive statistical properties and are chosen more for experimental qualities (an example of this in the 1998 paper by Boudoukh, Richardson, and Whitelaw cited by Engle which uses a semiparametric "hybrid" approach)
I think trying to think too narrowly about VaR isn't a very interesting statistical problem - in a sense, VaR is just a useful summarization of quantile risk put into dollar figures for easy consumption
In my opinion, the most interesting question in the CAViaR paper isn’t VaR per se, it’s testing their theorems under relaxations of their assumptions to better understand their approach to extreme value theory. In particular, the following analyses catch my eye:
How important is the assumption C2 on page 374 of the CAVIAR paper (which underlies Theorem 1 about the consistency of the estimator beta hat)? It states that "conditional on all of the past information...the error terms form a stationary process". It may be an interesting test to see how much nonstationarity affects the consistency of the estimator beta hat. I could do this via simulation and see if there's a lot of value here, which may lead us to a more fundamental result.
In my opinion, theorems 4 and 5 (p. 371) are the most important in the paper because they state that the DQ_IS and DQ_OOS quantities are pivotals asymptotically. But it begs the question of how much does that matter, especially in the context of VaR, which in all likelihood is working with limited data? Therefore, it seems to me that the DQ8 and DQ9 assumptions are the shakiest assumptions required for theorem 5. It might be interesting to see how far off from a chi-squared distribution real-world data actually is. This may allow us to develop "rules of thumb" about the accuracy of these asymptotic distributions.

# Methods Used

Fill in.

# Data Used

# Results


```{r}

# Read in relevant libraries
library(microbenchmark)
library(data.table)
library(quantmod)
library(ggplot2)
library(tseries)
library(zoo)
library(magrittr)
library(dplyr)
library(kableExtra)
library(formattable)
library(quantreg)
library(MTS)

# Set up working directory
setwd("~/Documents/GitHub/CaviaR")
source('caviar_SM.R')
```

```{r}
# This code below is for use in the CAViaR sections.
```


```{r}
#' This is a function which pulls data for use in the CAViaR model
#'
#' @param symbol - symbol to pull
#' @param compl_case - defaults to true...only includes complete cases in the data
#' @param adj_close - use adjusted closing prices. Default is yes.
#' @param log_return - use log return? Default is yes.
#'
#' @return - a data frame which can be fed into later functions
#' @export
#'
#' @examples - data_pull("SPY")
data_pull = function(symbol, compl_case = 1, adj_close = 1, log_return = 1, start_date = "1900-01-01", end_date = Sys.Date()){
  # Pull in data from quantmod
  response_pull = getSymbols(symbol, auto.assign = FALSE, from = start_date, to = end_date)
  # Get adjusted closing price
  if (adj_close == TRUE){
    df = Ad(response_pull)
  } else {
    df = Cl(response_pull)
  }
  # Return complete cases only 
  if (compl_case == TRUE){
    df = df[complete.cases(df), ]
  } else{
    df = df
  }
  # Calculate log return of data
  if (log_return == TRUE){
    lr = log(df[,1]/shift(df[,1], 1, type = "lag"))
    # Combine data
    df_out = cbind(df, lr)
    # Rename the data 
    colnames(df_out) <- c(sym=symbol, paste0(symbol, "_log_return"))
  } else{
    df_out = df
  }
  # Return data
  return(df_out)
}

ibm = data_pull("IBM", start_date = "1986-04-06", end_date = "1999-04-08")
# ibm

```

```{r}
#' Pull the data and run the CAViaR function on it
#'
#' @param input_data - data to use in the function
#' @param range_data - range of the data to use
#'
#' @return - a list of values from the caviar function
#' @export
#'
#' @examples - caviar_pull(spy)
caviar_pull = function(input_data, range_data = (2:dim(input_data)[1])){
  # Run the caviar data
  caviar <- caviarOptim(input_data[range_data,2])
  return(caviar)
}

```


```{r}
#' Function for producing rolling predictions
#' Model 1 = Symmetric Absolute Value, 2 = Asymmetric slope, 3 = Indirect GARCH, 4 = Adaptive
#'
#' @param input_data - input data from the previous function
#' @param range_data - range of the data to consider
#' @param nfcst - number of forecasts to make
#' @param model - model to use (integers 1 through 4). Defaults to 1. 
#' @param level - level of significance to use.
#' @param G - argument for the k parameter in the 4th model (adaptive). Default is 5
#'
#' @return - an xts object which contains rolling CAViaR predictions
#' @export
#'
#' @examples - rolling_predictions(spy, nfcst = 22)
rolling_predictions = function(input_data, range_data = (2:dim(input_data)[1]), nfcst = 250, model =1, level = 0.01, G = 5, col = 2){
  # Run the varpredict function
  varpredict <- rollapplyr(input_data[range_data,col], length(range_data) - nfcst, caviarOptim, model, level, predict = 1, k = G) %>% lag
  # Eliminate NAs
  # pred_no_na = na.omit(varpredict)
  # Return the data
  # return(pred_no_na)
  return(varpredict)
}

# mth_series[2:100, 1]

```

```{r}
#' Function to Calculate Loss from the above predictions
#'
#' @param symbol - symbol to work with from quantmod. Must be in quotations to work
#' @param start_dt - start date of the data to build the forecast on 
#' @param end_dt - end date of the data to build the forecast on  
#' @param nfcst - number of data points to use in the forecast
#' @param model - model to use. Defaults to 1
#' @param level - level of significance. Defaults to 1%
#' @param G - argument for the k parameter in the 4th model (adaptive). Default is 5
#'
#' @return - loss using absolute value
#' @export - a plot of the data
#'
#' @examples
loss_calc = function(symbol, start_dt, end_dt, nfcst, model = 1, level = 0.01, G = 5){
  # Pull in the data
  raw_data = data_pull(symbol, start_date = start_dt, end_date = end_dt)
  # Forecast based on the data
  fcst = na.omit(rolling_predictions(raw_data, nfcst = nfcst, model = model, level = level, G = G))*(-1)
  # Extract actuals
  act = tail(raw_data, n = nfcst)[,2]
  # Join the two together and rename
  join = merge(fcst,act,all=TRUE)
  colnames(join) <- c("Fcst_VaR", "Act_Return")
  # print(join)
  # Calculate the losses
  loss = abs(sum(ifelse(act > fcst, level, (-1)*(1-level))))
  # Plot the data
  plot = plot.xts(join, col = c("red", "black"), lty = c(2,1), main = "Log Return from the SPY vs. Fcst. VaR",grid.col = NA, legend.loc = "bottomleft")
  return(list(loss, plot, act, fcst))
}

```



```{r}
#' Stock simulation for use in CAViaR simulation study
#'
#' @param n1 - number of data points in first series
#' @param n2 - number of data points in second series
#' @param s1_sd - standard deviation of first series
#' @param s2_sd - standard deviation of second series 
#' @param s1_mean - mean of first series 
#' @param s2_mean - mean of second series 
#' @param rseed - random seed for reproducibility
#'
#' @return - output time series
#' @export
#'
#' @examples hth_series = stock_simul(100, 25, 10, 10)
stock_simul = function(n1, n2,s1_gr, s2_gr, s1_sd, s2_sd, rseed = 1234){
  # Set random seed
  set.seed(rseed)
  # Initialize vector and put first value in the series
  s1 <- rep(1, n1)
  # Simulate first series
  for (i in 2:n1){
    s1[i] <- s1[i-1]*(1+s1_gr) + rnorm(1, mean = 0, sd = s1_sd)
  }
  # Initialize vector
  s2 <- rep(1, n2)
  # Put first value in
  s2[1] <- s1[n1]*(1+s2_gr) + rnorm(1, mean = 0, sd = s2_sd)
  # Simulate second series
  for (i in 2:n2){
    s2[i] <- s2[i-1]*(1+s2_gr) + rnorm(1, mean = 0, sd = s2_sd)
  }
  # Combine the two series
  long <- as.matrix(c(s1, s2))
  # Export the data
  return(long)
}

# test <- lth_series[500,1]*(1)

```

```{r}
# Above concludes the CAViaR Code

# Below is for the simulation study
```

```{r}
# Low to high volatility series
lth_series = stock_simul(500, 25, 0.001, -0.005, 0.001, 0.01)
mth_series = stock_simul(500, 25, 0.001, -0.005, 0.005, 0.01)
hth_series = stock_simul(500, 25,  0.001, -0.005, 0.01, 0.01)
```


```{r}
# Plot the data
plot(lth_series, type = "l", ylim = c(0.8, 1.8), xlab = "Days", ylab = "Growth (Indexed to 1)", main = "Growth Comparison of 3 Simulated Stocks")
lines(mth_series, type = "l", col = "red")
lines(hth_series, type = "l", col = "blue")
legend("bottomright", legend=c("Low Vol. Stock", "Mid Vol. Stock", "High Vol. Stock"),col=c("black", "red", "blue"), lty = c(1,1,1))

# Transform the data to be differenced log data
dl_lth <- diff(log(lth_series))
dl_mth <- diff(log(mth_series))
dl_hth <- diff(log(hth_series))



# Plot the differenced data
plot(dl_lth, type = "l", xlab = "Days", ylab = "Differenced Log Growth", ylim = c(-0.04, 0.04), main = "Log  Growth Rate Comparisons of 3 Simulated Stocks")
lines(dl_mth, type = "l", col = "red")
lines(dl_hth, type = "l", col = "blue")
legend("bottomright", legend=c("Low Vol. Stock", "Mid Vol. Stock", "High Vol. Stock"),col=c("black", "red", "blue"), lty = c(1,1,1))
```


```{r}
# Let's test the series
m1_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 1, G = 10, col = 1)
m2_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 2, G = 10, col = 1)
m3_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 3, G = 10, col = 1)
m4_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 4, G = 10, col = 1)

# Let's test the series
m1_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 1, G = 10, col = 1)
m2_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 2, G = 10, col = 1)
m3_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 3, G = 10, col = 1)
m4_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 4, G = 10, col = 1)

# Let's test the series
m1_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 1, G = 10, col = 1)
m2_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 2, G = 10, col = 1)
m3_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 3, G = 10, col = 1)
m4_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 4, G = 10, col = 1)
```

```{r}

```

```{r}
# First plot - LTH VaR
plot(dl_lth[500:524], type = "l", ylim = c(-0.04, 0.04), xlab = "Forecast Day", ylab = "Log Return", main = "VaR Forecast in High-Volatility Environment for a Low-Vol. Stock")
legend("topright", legend=c("Simulated Return", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "red", "green", "blue", "purple"), lty = c(1,2,2,2,2))
lines(m1_lth[2:26]*(-1), col = "red", lty = 2)
lines(m2_lth[2:26]*(-1), col = "green", lty = 2)
lines(m3_lth[2:26]*(-1), col = "blue", lty = 2)
lines(m4_lth[2:26]*(-1), col = "purple", lty = 2)

# Second plot - MTH VaR
plot(dl_mth[500:524], type = "l", ylim = c(-0.04, 0.04), xlab = "Forecast Day", ylab = "Log Return", main = "VaR Forecast in High-Volatility Environment for a Mid-Vol. Stock")
legend("topright", legend=c("Simulated Return", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "red", "green", "blue", "purple"), lty = c(1,2,2,2,2))
lines(m1_mth[2:26]*(-1), col = "red", lty = 2)
lines(m2_mth[2:26]*(-1), col = "green", lty = 2)
lines(m3_mth[2:26]*(-1), col = "blue", lty = 2)
lines(m4_mth[2:26]*(-1), col = "purple", lty = 2)

# Second plot - HTH VaR
plot(dl_hth[500:524], type = "l", ylim = c(-0.04, 0.04), xlab = "Forecast Day", ylab = "Log Return", main = "VaR Forecast in High-Volatility Environment for a Low-Vol. Stock")
legend("topright", legend=c("Simulated Return", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "red", "green", "blue", "purple"), lty = c(1,2,2,2,2))
lines(m1_hth[2:26]*(-1), col = "red", lty = 2)
lines(m2_hth[2:26]*(-1), col = "green", lty = 2)
lines(m3_hth[2:26]*(-1), col = "blue", lty = 2)
lines(m4_hth[2:26]*(-1), col = "purple", lty = 2)

```

That doesn't look promising. What if we just look at the last 10?

```{r}
# Let's test the series
m1_hth_sm = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 10, model = 1, G = 10, col = 1)
```

```{r}
plot(dl_hth[515:524], type = "l", ylim = c(-0.04, 0.04))
lines(m1_hth_sm[1:10], col = "red")
```

Not good. Let's look at an in-sample test.

```{r}
# Let's test the series in sample
m1_hth_sm_is_05 = rolling_predictions(dl_hth, range_data = (1:400), nfcst = 10, model = 1, level = 0.05, G = 10, col = 1)
m1_hth_sm_is_01 = rolling_predictions(dl_hth, range_data = (1:400), nfcst = 10, model = 1, level = 0.01, G = 10, col = 1)


```

```{r}
plot(dl_hth[390:399], type = "l", ylim = c(-0.04, 0.04))
lines(m1_hth_sm_is_05[1:10], col = "red")
lines(m1_hth_sm_is_01[1:10], col = "green")
```

```{r}
# We may need to expand this to other areas. Ask Wei Biao for simulation advice.
```



# Combining a Diffusion Index Model with the Quantile Regression Model

```{r}
# Below is code that pulls real data for analysis
```


Response is IBM, pick 5 stocks for predictors.

```{r}
# Pull IBM
ibm = data_pull("IBM", start_date = "1986-04-06", end_date = "1999-04-08")

# Pull 10 stocks at random that existed in the DJIA over the timeframe
# Exxon
xom = data_pull("XOM", start_date = "1986-04-06", end_date = "1999-04-08")

# PG
pg = data_pull("PG", start_date = "1986-04-06", end_date = "1999-04-08")

# General Electric
ge = data_pull("GE", start_date = "1986-04-06", end_date = "1999-04-08")

# 3M
mmm = data_pull("MMM", start_date = "1986-04-06", end_date = "1999-04-08")

# Disney
dis = data_pull("DIS", start_date = "1986-04-06", end_date = "1999-04-08")

```

There is probably some work to be done with choosing these stocks. I only picked ones with data.

```{r}
# Plot the data
# plot.xts(ibm[,2])

```

Let's just see if there is any merit to just regressing one set of stocks on another.

```{r}
# Predictors
?SWfore
SWfore
```

The code that is used in a diffusion index model is as follows. The key is figuring out where to make the extension. I'll go through and comment everything and see what's what.

```{r}
#' Below is the modified diffusion index code.
#'
#' @param y - response variable
#' @param x - predictor variables
#' @param orig - forecast origin
#' @param m - number of diffusion indexes used
#' @param tau - VaR level to use; must be between 0 and 1
#'
#' @return - returns a list of variables for use in the diffusion index
#' @export
#'
#' @examples
mod_di = function (y, x, orig, m, tau) 
{
  # Converts the response variables into a matrix
  if (!is.matrix(x)) 
      x = as.matrix(x)
  # nT is number of t time-steps
  nT = dim(x)[1]
  # k is the number of diffusion indices used
  k = dim(x)[2]
  # Sanity checks to ensure that the origin isn't past the number of time points
  if (orig > nT) 
      orig = nT
  # Makes sure that there aren't more predictors than there variables in the dataset
  if (m > k) 
      m = k
  # Makes sure there are at least some variables
  if (m < 1) 
      m = 1
  # Subdivides the dataframe
  x1 = x[1:orig, ]
  # Calculates means of each row
  me = apply(x1, 2, mean)
  # Calculates standard deviations of each column
  se = sqrt(apply(x1, 2, var))
  # Creates a matrix x1, which normalizes all the columns. 
  # This may be an issue since it assumes that the distribution is sufficiently described by the first two moments
  x1 = x
  for (i in 1:k) {
      x1[, i] = (x1[, i] - me[i])/se[i]
  }
  V1 = cov(x1[1:orig, ])
  # Performs an eigen decomposition
  m1 = eigen(V1)
  # Selects eigenvalues
  sdev = m1$values
  # Selects eigenvectors
  M = m1$vectors
  # Makes a smaller matrix
  M1 = M[, 1:m]
  # This is the diffusion index model - [orig x p]*[p x m] = [orig x m]
  Dindex = x1 %*% M1
  # Cut down both the response and predictors to be a reasonable size
  y1 = y[1:orig]
  DF = Dindex[1:orig, ]
  # Apply the linear model - HERE is the key.
  # mm = lm(y1 ~ DF) - old function
  mm = rq(y1 ~ DF, tau = tau)
  # Puts coefficients in a matrix
  coef = matrix(mm$coefficients, (m + 1), 1)
  # Initializes yhat variables and MSE
  yhat = NULL
  MSE = NULL
  if (orig < nT) {
    # Creates a nfcst by (m+1) matrix
    newx = cbind(rep(1, (nT - orig)), Dindex[(orig + 1):nT, 
        ])
    # [nfcstx(m+1)]*[(m+1)x1] = [nfcstx1]
    yhat = newx %*% coef
    # Calculates errors
    err = y[(orig + 1):nT] - yhat
    MSE = mean(err^2)
    cat("MSE of out-of-sample forecasts: ", MSE, "\n")
  }
  SWfore <- list(coef = coef, yhat = yhat, MSE = MSE, loadings = M1, 
      DFindex = Dindex)
}



# ?cbind
```

Now that we have the function, let's see if we can get it to work. First, we'll need to compile all of our data

```{r, cache = TRUE}
#' This is a function which creates a data frame for the response and explanatory variables that we'll feed into the diffusion index
#'
#' @param symbol_list - a list of symbols recognizable by the 
#' @param resp_var - the response variable we'd like to forecast; default is SPY
#' @param compl_case - defaults to true...only includes complete cases in the data
#'
#' @return - a data frame which can be fed into the SWfore function
#' @export
#'
#' @examples - diff_index_df(c("XLF", "XLE", "PSCT", "XLV", "VPU", "XLP", "IGF", "XWEB", "PPTY"))
diff_index_df = function(symbol_list, resp_var = "SPY", compl_case = 1, adj_close = 1, resp_adj_close = 1, start_date = "1900-01-01", end_date = Sys.Date()){
  # Pull in response variable
  response_pull = getSymbols(resp_var, auto.assign = FALSE, from = start_date, to = end_date)
  # Get adjusted closing price
  if (adj_close == TRUE){
    diff_df = Ad(response_pull)
  } else {
    diff_df = Cl(response_pull)
  }
  # Loop through the symbols and join in data
  for (i in 1:length(symbol_list)){
    # Pull closing price
    expl_pull = getSymbols(symbol_list[i], auto.assign = FALSE, from = start_date, to = end_date)
    # Extract closing price - 4th element
    if (adj_close == TRUE){
      expl_cl = Ad(expl_pull)
    } else {
      expl_cl = Cl(expl_pull)
    }
    # Join the data
    diff_df = merge(diff_df, expl_cl, join = "left", fill = NA)
  }
  # Return complete cases only 
  if (compl_case == TRUE){
    diff_df_out = diff_df[complete.cases(diff_df), ]
  } else{
    diff_df_out = diff_df
  }
  # Difference data
  return(diff_df_out)
}

?getSymbols
```


Now, let's test it. Moment of truth.

```{r}
# See what the 5% and 1% VaR levels look like
var_5pc = mod_di(pc_df[,1], pc_df[,-1], 100, 2, 0.05)
var_1pc = mod_di(pc_df[,1], pc_df[,-1], 100, 2, 0.01)

# Create a plot
ts.plot(pc_df[101:400,1])
lines(var_5pc$yhat[1:300], col = "blue", lty = 2)
lines(var_1pc$yhat[1:300], col = "red", lty = 2)

# Calculate accuracy
var_1pc$yhat
```

```{r, cache = TRUE}
#' Converts a diff_df into a data frame with approximate percentage changes diff(log(diff_df))
#'
#' @param diff_df - output of the diff_index_df function with complete cases
#'
#' @return - retuns the differenced data
#' @export
#'
#' @examples - pc_diff_index(test_compl) 

pc_diff_index = function(diff_df){
  # Difference the log of the data
  pc_diff_index = diff(log(diff_df))
  # Remove the first row
  pc_diff_index_out = pc_diff_index[-1,]
  return(pc_diff_index_out)
}

```



```{r}
df = diff_index_df(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31")
pc_df = pc_diff_index(df)

# Check the dimensions
dim(pc_df)
# head(pc_df)
# tail(pc_df)
```

```{r}
# Decide on the optimal number of vectors.

# (y, x, orig, m, tau) 

#' Function that calculates loss over a given period of time for the diffusion index model
#'
#' @param y - response variables
#' @param x - explanatory variable
#' @param orig - forecast origin
#' @param end - forecasting ending
#' @param m - number of diffusion indices to use
#' @param tau - VaR level
#'
#' @return - returns a list of the loss sum and the loss vector
#' @export
#'
#' @examples - loss_calc(pc_df[,1], pc_df[,-1], 757, 1027, 1, 0.01)
loss_calc = function(y, x, orig, end, m, tau){
  # Extract y_hat values
  di = mod_di(y,x,orig,m, tau)
  yhat = di$yhat[1:(end-orig)]
  # Calculate the loss
  # Initialize loss vector
  lvec = rep(0,(end-orig))
  # Take the difference
  for (i in 1:(end-orig)){
    # Calculate an indicator variable
    ind = ifelse(y[orig+i] < yhat[i], 1,0)
    # Use indicator in function below
    lvec[i] = (tau - ind)*(y[orig+i] - yhat[i])
  }
  # Add up the losses - change to look at sum of losses. Won't change decision criterion
  sumloss = sum(lvec)
  # sumloss = sum(lvec)/length(lvec)
  return(list(sumloss,lvec))
}

test = loss_calc(pc_df[,1], pc_df[,-1], 757, 1027, 1, 0.01)
# test


# abc = mod_di(pc_df[,1], pc_df[,-1], 757,1,0.01)
# abc$yhat

```

```{r}
#' Function that selects the optimal number of predictors
#'
#' @param y - response vector
#' @param x - predictor variables
#' @param orig - forecast origin
#' @param end - ending of validation set
#' @param tau - VaR in question
#' @param low_m - low value of m to consider
#' @param high_m - high value of m to consider
#'
#' @return - returns the optimal value of m
#' @export
#'
#' @examples - opt_m(pc_df[,1], pc_df[,-1], 757, 1027, 0.01, low_m =1, high_m  = 5)
opt_m = function(y, x, orig, end, tau, low_m = 1, high_m){
  # Initialize a loss vector
  loss_vec = rep(0,high_m-low_m + 1)
  # Initialize an m vector
  m_vec = seq(low_m, high_m, by = 1)
  # Loop through and populate the loss vector
  for (i in 1:length(loss_vec)){
    loss_vec[i] = loss_calc(y=y,x=x,orig=orig,end=end, m = m_vec[i], tau = tau)[[1]]
  }
  # Find the minimizer
  opt_m = which.min(loss_vec)
  # Return the loss_vector and the minimzer
  return(list(opt_m, loss_vec))
}

opt_pred = opt_m(y = pc_df[,1], x = pc_df[,-1], orig = 757, end = 1007, tau = 0.01, low_m =1, high_m  = 5)
opt_pred[[1]]

# test1 = loss_calc(pc_df[,1], pc_df[,-1], 757, 1027, 1, 0.01)[[1]]
# test2 = loss_calc(pc_df[,1], pc_df[,-1], 757, 1027, 2, 0.01)[[1]]
# test3 = loss_calc(pc_df[,1], pc_df[,-1], 757, 1027, 3, 0.01)[[1]]
# test4 = loss_calc(pc_df[,1], pc_df[,-1], 757, 1027, 4, 0.01)[[1]]
# test5 = loss_calc(pc_df[,1], pc_df[,-1], 757, 1027, 5, 0.01)[[1]]

# print(c(test1, test2, test3, test4, test5))

```

Now we have the optimal number of indices to use, which in this case is 3.

```{r}
# Run the model
mv_fcst = mod_di(pc_df[,1], pc_df[,-1], 1007, 3, 0.01)

```

```{r}
# Warning!! These can take a while to run (~1 hour). Proceed with caution.

# Run the univariate CAViaR models - these take a while. Be careful.
# uvcav_1 = rolling_predictions(pc_df[,1], range_data = (1:length(pc_df[,1])), nfcst = 250, model = 1, G = 10, col = 1, level = 0.01)
# uvcav_2 = rolling_predictions(pc_df[,1], range_data = (1:length(pc_df[,1])), nfcst = 250, model = 2, G = 10, col = 1, level = 0.01)
# uvcav_3 = rolling_predictions(pc_df[,1], range_data = (1:length(pc_df[,1])), nfcst = 250, model = 3, G = 10, col = 1, level = 0.01)
# uvcav_4 = rolling_predictions(pc_df[,1], range_data = (1:length(pc_df[,1])), nfcst = 250, model = 4, G = 10, col = 1, level = 0.01)
```

```{r}
# index(pc_df[1008,])

# Create a plot
ts.plot(pc_df[1008:1257,1], ylim = c(-0.1,0.1), xlab = "Trading Days", ylab = "Percent Change in PG", main = "Predicting PG Returns Over Last 250 Trading Days in 2008", sub = paste("First Trading Day is ", index(pc_df[1008,]), ", Last Trading Day is ", index(pc_df[1257,]), sep=""))
legend("topleft", legend=c("Actual Return", "Multivariate CAViaR", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "blue", "red", "green", "orange", "purple"), lty = c(1,2,2,2,2,2))
lines(mv_fcst$yhat[1:250], col = "blue", lty = 2)
lines(as.ts(uvcav_1[1008:1257])*(-1), col = "red", lty = 2)
lines(as.ts(uvcav_2[1008:1257])*(-1), col = "green", lty = 2)
lines(as.ts(uvcav_3[1008:1257])*(-1), col = "orange", lty = 2)
lines(as.ts(uvcav_4[1008:1257])*(-1), col = "purple", lty = 2)
# pc_df[1008:1257,1]

# legend("topright", legend=c("Simulated Return", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "red", "green", "blue", "purple"), lty = c(1,2,2,2,2))
# lines(m1_mth[2:26]*(-1), col = "red", lty = 2)
# lines(m2_mth[2:26]*(-1), col = "green", lty = 2)
# lines(m3_mth[2:26]*(-1), col = "blue", lty = 2)
# lines(m4_mth[2:26]*(-1), col = "purple", lty = 2)

# length(uvcav_1)
```

```{r}
#' A function to calculate losses based on the test sample
#'
#' @param true_vec - the true vector of returns
#' @param pred_vec - the predicted vector from the model runs
#' @param tau - VaR level. Must match what the model used
#'
#' @return - total losses and the entire loss vector
#' @export
#'
#' @examples
loss_test = function(true_vec, pred_vec, tau){
  # Initialize a loss vector
  lvec = rep(0, length(true_vec))
  # Initialize a break vector to see when VaR is broken
  bvec = rep(0, length(true_vec))
  for (i in 1:length(true_vec)){
    # Calculate an indicator variable
    bvec[i] = ifelse(true_vec[i] < pred_vec[i], 1,0)
    # Use indicator in function below
    lvec[i] = (tau - bvec[i])*(true_vec[i] - pred_vec[i])
  }
  # Add up the losses
  # sumloss = sum(lvec)/length(lvec)
  sumloss = sum(lvec)
  # Add up the VaR breakage
  varbreak = sum(bvec)/length(bvec)
  return(list(sumloss,lvec, varbreak, bvec))
}
```


```{r}
# Calculate the losses
loss_mv = loss_test(pc_df[1008:1257,1], mv_fcst$yhat[1:250], tau = 0.01)[[1]]
loss_uv1 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_1[1008:1257])*(-1), tau = 0.01)[[1]]
loss_uv2 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_2[1008:1257])*(-1), tau = 0.01)[[1]]
loss_uv3 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_3[1008:1257])*(-1), tau = 0.01)[[1]]
loss_uv4 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_4[1008:1257])*(-1), tau = 0.01)[[1]]

# Combine into a vector
loss_vec = c(loss_mv, loss_uv1, loss_uv2, loss_uv3, loss_uv4)

# Calculate the VaR breaks
break_mv = loss_test(pc_df[1008:1257,1], mv_fcst$yhat[1:250], tau = 0.01)[[3]]
break_uv1 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_1[1008:1257])*(-1), tau = 0.01)[[3]]
break_uv2 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_2[1008:1257])*(-1), tau = 0.01)[[3]]
break_uv3 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_3[1008:1257])*(-1), tau = 0.01)[[3]]
break_uv4 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_4[1008:1257])*(-1), tau = 0.01)[[3]]

# Combine into a vector
break_vec = c(break_mv, break_uv1, break_uv2, break_uv3, break_uv4)

# Combine into a data frame
df = as.data.frame(rbind(loss_vec, break_vec))
# Add row/column names
colnames(df) <- c("Multivariate CAViaR", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive")
rownames(df) <- c("Losses", "VaR Breaks (%)")
# Convert to a table
  df %>% kable(caption = "Comparison of VaR Methods", digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling()


```


```{r}
# Calculate an indicator variable
ind = ifelse(y[orig+i] < yhat[i], 1,0)
# Use indicator in function below
lvec[i] = (tau - ind)*(y[orig+i] - yhat[i])


uvcav_1

# lines(var_1pc$yhat[1:300], col = "red", lty = 2)

# mod_di = function (y, x, orig, m, tau) 

ts.plot(mv_fcst$yhat)
lines(pc_df[1028:1257,1], col = "red")
```


```{r}
var_1pc_k1 = mod_di(pc_df[,1], pc_df[,-1], 757, 1, 0.01)
var_1pc_k3 = mod_di(pc_df[,1], pc_df[,-1], 757, 1, 0.01)
var_1pc_k5 = mod_di(pc_df[,1], pc_df[,-1], 757, 1, 0.01)

# var_1pc_k3


```


# Stock simulation

Real data is messy, but simulation could give us some idea of whether combining all the stocks into a pool could give a lift.

```{r}
#' Stock simulation for use in CAViaR simulation study
#'
#' @param n1 - number of data points in first series
#' @param n2 - number of data points in second series
#' @param s1_sd - standard deviation of first series
#' @param s2_sd - standard deviation of second series 
#' @param s1_mean - mean of first series 
#' @param s2_mean - mean of second series 
#' @param rseed - random seed for reproducibility
#'
#' @return - output time series
#' @export
#'
#' @examples hth_series = stock_simul(100, 25, 10, 10)
stock_simul = function(n1, n2,s1_gr, s2_gr, s1_sd, s2_sd, rseed = 1234){
  # Set random seed
  set.seed(rseed)
  # Initialize vector and put first value in the series
  s1 <- rep(1, n1)
  # Simulate first series
  for (i in 2:n1){
    s1[i] <- s1[i-1]*(1+s1_gr) + rnorm(1, mean = 0, sd = s1_sd)
  }
  # Initialize vector
  s2 <- rep(1, n2)
  # Put first value in
  s2[1] <- s1[n1]*(1+s2_gr) + rnorm(1, mean = 0, sd = s2_sd)
  # Simulate second series
  for (i in 2:n2){
    s2[i] <- s2[i-1]*(1+s2_gr) + rnorm(1, mean = 0, sd = s2_sd)
  }
  # Combine the two series
  long <- as.matrix(c(s1, s2))
  # Export the data
  return(long)
}

# test <- lth_series[500,1]*(1)

```






```{r}
#' Stock data generator for use in CAViaR simulation study
#' For maximum flexibility, this will only generate one series with one variance and growth rate.
#' Note that for simplicity, this is modeling the percentage change
#' 
#' @param t - number of time data points
#' @param mu - the mean return in a given day
#' @param sigma - the standard deviation
#' @param rseed - random seed for reproducibility. Default is NULL
#'
#' @return - output percentage changes of a stock
#' @export
#'
#' @examples dat_gen(1000, 0.001, 0.1)
dat_gen = function(t, mu, sigma, rseed = NULL){
  # Have an option for with and without random seed
  if (is.null(rseed) == TRUE){
    dat = rnorm(t, mu, sigma)
  }
  else {
    set.seed(rseed)
    dat = rnorm(t, mu, sigma)
  }
  return(dat)
}

# mean(dat_gen(1000, 0.001, 0.1))

```

```{r}
# Generalize the above vector to look at many different periods


```

# JUNK CODE

```{r}
# Testing to see how apply function works
test_mat = matrix(rep(seq(1,5),5), nrow=5, ncol = 5)
# ?as.matrix
# Test apply function. Row means should be 1 through 5, col means should all be 3
row_me = apply(test_mat, 1, mean)
col_me = apply(test_mat, 2, mean)
row_se = sqrt(apply(test_mat, 1, var))
col_se = sqrt(apply(test_mat, 2, var))

# print(c(row_me, col_me, row_se, col_se))

# Exactly as I expected.

# test_mat
```

```{r}
# Test the matrix multiplication
```


