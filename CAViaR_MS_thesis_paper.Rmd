---
title: "M.S. Thesis - First Draft"
author: "Steven Moen"
date: "Friday, January 24th, 2020"
output: html_notebook
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE, cache = TRUE)
```

# Background and Introduction

Value-at-risk modeling, or VaR, is a commonly used tool to measure riskiness in a financial institution. CAViaR was a new take on risk modeling originally proposed by Robert Engle and Simone Manganelli in 2004 which builds upon the literature of modeling regression quantiles in settings which are essential. 

and agree with the basic premise of the CAViaR paper that many of the nonparametric historical simulation methods don’t have attractive statistical properties and are chosen more for experimental qualities (an example of this in the 1998 paper by Boudoukh, Richardson, and Whitelaw cited by Engle which uses a semiparametric "hybrid" approach)
I think trying to think too narrowly about VaR isn't a very interesting statistical problem - in a sense, VaR is just a useful summarization of quantile risk put into dollar figures for easy consumption
In my opinion, the most interesting question in the CAViaR paper isn’t VaR per se, it’s testing their theorems under relaxations of their assumptions to better understand their approach to extreme value theory. In particular, the following analyses catch my eye:
How important is the assumption C2 on page 374 of the CAVIAR paper (which underlies Theorem 1 about the consistency of the estimator beta hat)? It states that "conditional on all of the past information...the error terms form a stationary process". It may be an interesting test to see how much nonstationarity affects the consistency of the estimator beta hat. I could do this via simulation and see if there's a lot of value here, which may lead us to a more fundamental result.
In my opinion, theorems 4 and 5 (p. 371) are the most important in the paper because they state that the DQ_IS and DQ_OOS quantities are pivotals asymptotically. But it begs the question of how much does that matter, especially in the context of VaR, which in all likelihood is working with limited data? Therefore, it seems to me that the DQ8 and DQ9 assumptions are the shakiest assumptions required for theorem 5. It might be interesting to see how far off from a chi-squared distribution real-world data actually is. This may allow us to develop "rules of thumb" about the accuracy of these asymptotic distributions.

# Methods Used

Fill in.

# Data Used

# Results

# CODE

## Libraries

```{r}
# Read in relevant libraries
library(microbenchmark)
library(data.table)
library(quantmod)
library(ggplot2)
library(tseries)
library(zoo)
library(magrittr)
library(dplyr)
library(kableExtra)
library(formattable)
library(quantreg)
library(MTS)
library(plot3D)

# Set up working directory
# setwd("~/Documents/GitHub/CaviaR")

# source('caviar_SM.R')
source('~/Documents/GitHub/CaviaR/caviar_SM.R')
```

```{r}
# This code below is for use in the CAViaR sections.
```

```{r}
# Here is code that I'll wrap some parts in to avoid superfluous output
quiet <- function(x) { 
  sink(tempfile()) 
  on.exit(sink()) 
  invisible(force(x)) 
} 
```

## Univariate CAViaR Section

```{r}
#' This is a function which pulls data for use in the CAViaR model
#'
#' @param symbol - symbol to pull
#' @param compl_case - defaults to true...only includes complete cases in the data
#' @param adj_close - use adjusted closing prices. Default is yes.
#' @param log_return - use log return? Default is yes.
#'
#' @return - a data frame which can be fed into later functions
#' @export
#'
#' @examples - data_pull("SPY")
data_pull = function(symbol, compl_case = 1, adj_close = 1, log_return = 1, start_date = "1900-01-01", end_date = Sys.Date()){
  # Pull in data from quantmod
  response_pull = getSymbols(symbol, auto.assign = FALSE, from = start_date, to = end_date)
  # Get adjusted closing price
  if (adj_close == TRUE){
    df = Ad(response_pull)
  } else {
    df = Cl(response_pull)
  }
  # Return complete cases only 
  if (compl_case == TRUE){
    df = df[complete.cases(df), ]
  } else{
    df = df
  }
  # Calculate log return of data
  if (log_return == TRUE){
    lr = log(df[,1]/shift(df[,1], 1, type = "lag"))
    # Combine data
    df_out = cbind(df, lr)
    # Rename the data 
    colnames(df_out) <- c(sym=symbol, paste0(symbol, "_log_return"))
  } else{
    df_out = df
  }
  # Return data
  return(df_out)
}

ibm = data_pull("IBM", start_date = "1986-04-06", end_date = "1999-04-08")

```

```{r}
#' Pull the data and run the CAViaR function on it
#'
#' @param input_data - data to use in the function
#' @param range_data - range of the data to use
#'
#' @return - a list of values from the caviar function
#' @export
#'
#' @examples - caviar_pull(spy)
caviar_pull = function(input_data, range_data = (2:dim(input_data)[1])){
  # Run the caviar data
  caviar <- caviarOptim(input_data[range_data,2])
  return(caviar)
}

```


```{r}
#' Function for producing rolling predictions
#' Model 1 = Symmetric Absolute Value, 2 = Asymmetric slope, 3 = Indirect GARCH, 4 = Adaptive
#'
#' @param input_data - input data from the previous function
#' @param range_data - range of the data to consider
#' @param nfcst - number of forecasts to make
#' @param model - model to use (integers 1 through 4). Defaults to 1. 
#' @param level - level of significance to use.
#' @param G - argument for the k parameter in the 4th model (adaptive). Default is 5
#'
#' @return - an xts object which contains rolling CAViaR predictions
#' @export
#'
#' @examples - rolling_predictions(spy, nfcst = 22)
rolling_predictions = function(input_data, range_data = (2:dim(input_data)[1]), nfcst = 250, model =1, level = 0.01, G = 5, col = 2){
  # Run the varpredict function
  varpredict <- rollapplyr(input_data[range_data,col], length(range_data) - nfcst, caviarOptim, model, level, predict = 1, k = G) %>% lag
  # Eliminate NAs
  # pred_no_na = na.omit(varpredict)
  # Return the data
  # return(pred_no_na)
  return(varpredict)
}

```

```{r}
#' Function to Calculate Loss from the above predictions
#'
#' @param symbol - symbol to work with from quantmod. Must be in quotations to work
#' @param start_dt - start date of the data to build the forecast on 
#' @param end_dt - end date of the data to build the forecast on  
#' @param nfcst - number of data points to use in the forecast
#' @param model - model to use. Defaults to 1
#' @param level - level of significance. Defaults to 1%
#' @param G - argument for the k parameter in the 4th model (adaptive). Default is 5
#'
#' @return - loss using absolute value
#' @export - a plot of the data
#'
#' @examples
loss_calc_uv = function(symbol, start_dt, end_dt, nfcst, model = 1, level = 0.01, G = 5){
  # Pull in the data
  raw_data = data_pull(symbol, start_date = start_dt, end_date = end_dt)
  # Forecast based on the data
  fcst = na.omit(rolling_predictions(raw_data, nfcst = nfcst, model = model, level = level, G = G))*(-1)
  # Extract actuals
  act = tail(raw_data, n = nfcst)[,2]
  # Join the two together and rename
  join = merge(fcst,act,all=TRUE)
  colnames(join) <- c("Fcst_VaR", "Act_Return")
  # print(join)
  # Calculate the losses
  loss = abs(sum(ifelse(act > fcst, level, (-1)*(1-level))))
  # Plot the data
  plot = plot.xts(join, col = c("red", "black"), lty = c(2,1), main = "Log Return from the SPY vs. Fcst. VaR",grid.col = NA, legend.loc = "bottomleft")
  return(list(loss, plot, act, fcst))
}

```



```{r}
#' Stock simulation for use in CAViaR simulation study
#'
#' @param n1 - number of data points in first series
#' @param n2 - number of data points in second series
#' @param s1_sd - standard deviation of first series
#' @param s2_sd - standard deviation of second series 
#' @param s1_mean - mean of first series 
#' @param s2_mean - mean of second series 
#' @param rseed - random seed for reproducibility
#'
#' @return - output time series
#' @export
#'
#' @examples hth_series = stock_simul(100, 25, 10, 10)
stock_simul = function(n1, n2,s1_gr, s2_gr, s1_sd, s2_sd, rseed = 1234){
  # Set random seed
  set.seed(rseed)
  # Initialize vector and put first value in the series
  s1 <- rep(1, n1)
  # Simulate first series
  for (i in 2:n1){
    s1[i] <- s1[i-1]*(1+s1_gr) + rnorm(1, mean = 0, sd = s1_sd)
  }
  # Initialize vector
  s2 <- rep(1, n2)
  # Put first value in
  s2[1] <- s1[n1]*(1+s2_gr) + rnorm(1, mean = 0, sd = s2_sd)
  # Simulate second series
  for (i in 2:n2){
    s2[i] <- s2[i-1]*(1+s2_gr) + rnorm(1, mean = 0, sd = s2_sd)
  }
  # Combine the two series
  long <- as.matrix(c(s1, s2))
  # Export the data
  return(long)
}

# test <- lth_series[500,1]*(1)

```

```{r}
# Above concludes the CAViaR Code

# Below is for the simulation study - it's at the bottom of the junk code pile
```

## MV Caviar - Pulling the data

```{r, cache = TRUE}
#' This is a function which creates a data frame for the response and explanatory variables that we'll feed into the diffusion index
#'
#' @param symbol_list - a list of symbols recognizable by the 
#' @param resp_var - the response variable we'd like to forecast; default is SPY
#' @param compl_case - defaults to true...only includes complete cases in the data
#' @param adj_close - use adjusted closing prices for the explanatory variables? default is 1 for YES
#' @param resp_adj_close - use adjusted closing prices for the explanatory variables? default is 1 for YES
#' @param start_date - starting data to use
#' @param end_date - ending date of the data
#' @param lag_pred - do we lag the predictions? It is STRONGLY recommended that this is 0
#'
#' @return - a data frame which can be fed into the SWfore function
#' @export
#'
#' @examples - diff_index_df(c("XLF", "XLE", "PSCT", "XLV", "VPU", "XLP", "IGF", "XWEB", "PPTY"))
diff_index_df = function(symbol_list, resp_var = "SPY", compl_case = 1, adj_close = 1, resp_adj_close = 1, start_date = "1900-01-01", end_date = Sys.Date(), lag_pred = 1){
  # Pull in response variable
  response_pull = getSymbols(resp_var, auto.assign = FALSE, from = start_date, to = end_date)
  # Get adjusted closing price
  if (resp_adj_close == TRUE){
    diff_df = Ad(response_pull)
  } else {
    diff_df = Cl(response_pull)
  }
  # Loop through the symbols and join in data
  for (i in 1:length(symbol_list)){
    # Pull closing price
    expl_pull = getSymbols(symbol_list[i], auto.assign = FALSE, from = start_date, to = end_date)
    # Extract closing price - 4th element
    if (adj_close == TRUE){
      expl_cl = Ad(expl_pull)
    } else {
      expl_cl = Cl(expl_pull)
    }
    # New code for 4.16.2020 - lag the explanatory variables
    if (lag_pred == TRUE){
      # Lag the explanatory variables by 1
      lag_exp = lag(expl_cl, 1)
      # Append the first lag to the data frame
      diff_df = merge(diff_df, lag_exp, join = "left", fill = NA)
    } else{
      # Return the data frame without lags
      diff_df = merge(diff_df, expl_cl, join = "left", fill = NA)
    }
  }
  if (lag_pred == TRUE){
    # Chop off the first row
    diff_df = diff_df[-1,]
  }
  else {
    print("PLEASE NOTE - the explanatory variables in this DF are NOT lagged. Be careful to avoid look-ahead bias!")
  }
  # Return complete cases only 
  if (compl_case == TRUE){
    diff_df_out = diff_df[complete.cases(diff_df), ]
  } else{
    diff_df_out = diff_df
  }
  
  return(diff_df_out)
}

```

```{r, cache = TRUE}
#' Converts a diff_df into a data frame with approximate percentage changes diff(log(diff_df))
#'
#' @param diff_df - output of the diff_index_df function with complete cases
#'
#' @return - retuns the differenced data
#' @export
#'
#' @examples - pc_diff_index(test_compl) 

pc_diff_index = function(diff_df){
  # Difference the log of the data
  pc_diff_index = diff(log(diff_df))
  # Remove the first row
  pc_diff_index_out = pc_diff_index[-1,]
  return(pc_diff_index_out)
}

```

```{r}
# Troubleshooting the data 
df = diff_index_df(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", lag_pred = 1)
pc_df = pc_diff_index(df)
# tail(pc_df)

```

```{r}
# Problem solving adequate coverage of bond funds on 4.23.2020
# Troubleshooting the data 
bond_df = diff_index_df(c("TLT", "IEF", "SHY", "LQD"), resp_var = "SPY", start_date = "2004-01-01", end_date = "2008-12-31", lag_pred = 1)

glob_df = diff_index_df(c("JXI", "KXI", "IXJ", "IXP", "IXN", "RXI", "EXI", "IXG", "MXI", "IXC"), resp_var = "SPY", start_date = "2004-01-01", end_date = "2008-12-31", lag_pred = 1)
# bond_df
# sum(is.na(glob_df))
# pc_df = pc_diff_index(df)
# head(pc_df)
```


## MV Caviar - Modified DI code

The code that is used in a diffusion index model is as follows. The key is figuring out where to make the extension. I'll go through and comment everything and see what's what.

```{r}
#' Below is the modified diffusion index code.
#'
#' @param y - response variable
#' @param x - predictor variables
#' @param orig - forecast origin
#' @param m - number of diffusion indexes used
#' @param tau - VaR level to use; must be between 0 and 1
#' @param end - specifies an alternate ending value
#' @param print_mdl - print the model summary and the MSE
#'
#' @return - returns a list of variables for use in the diffusion index
#' @export
#'
#' @examples
mod_di = function (y, x, orig, m, tau, end = NULL, print_mdl = 0) 
{
  # Converts the response variables into a matrix
  if (!is.matrix(x)) 
      x = as.matrix(x)
  # nT is number of t time-steps
  nT = dim(x)[1]
  # Add a line to establish the number of data points used in the test.
  if (is.null(end) != TRUE){
    nT = end
  }
  # k is the number of diffusion indices used
  k = dim(x)[2]
  # Sanity checks to ensure that the origin isn't past the number of time points
  if (orig > nT) 
      orig = nT
  # Makes sure that there aren't more predictors than there variables in the dataset
  if (m > k) 
      m = k
  # Makes sure there are at least some variables
  if (m < 1) 
      m = 1
  # Subdivides the dataframe
  x1 = x[1:orig, ]
  # Calculates means of each row
  me = apply(x1, 2, mean)
  # Calculates standard deviations of each column
  se = sqrt(apply(x1, 2, var))
  # Creates a matrix x1, which normalizes all the columns. 
  # This may be an issue since it assumes that the distribution is sufficiently described by the first two moments
  x1 = x
  for (i in 1:k) {
      x1[, i] = (x1[, i] - me[i])/se[i]
  }
  V1 = cov(x1[1:orig, ])
  # Performs an eigen decomposition
  m1 = eigen(V1)
  # Selects eigenvalues
  sdev = m1$values
  # Selects eigenvectors
  M = m1$vectors
  # Makes a smaller matrix
  M1 = M[, 1:m]
  # This is the diffusion index model - [orig x p]*[p x m] = [orig x m]
  Dindex = x1 %*% M1
  # Cut down both the response and predictors to be a reasonable size
  y1 = y[1:orig]
  DF = Dindex[1:orig, ]
  # Apply the linear model - HERE is the key.
  # mm = lm(y1 ~ DF) - old function
  mm = rq(y1 ~ DF, tau = tau)
  # Print the data
  if (print_mdl == 1){
    print(summary(mm))
  }
  # Puts coefficients in a matrix
  coef = matrix(mm$coefficients, (m + 1), 1)
  # Initializes yhat variables and MSE
  yhat = NULL
  MSE = NULL
  if (orig < nT) {
    # Creates a nfcst by (m+1) matrix
    newx = cbind(rep(1, (nT - orig)), Dindex[(orig + 1):nT, 
        ])
    # [nfcstx(m+1)]*[(m+1)x1] = [nfcstx1]
    yhat = newx %*% coef
    # Calculates errors
    err = y[(orig + 1):nT] - yhat
    MSE = mean(err^2)
    if (print_mdl == 1){
      cat("MSE of out-of-sample forecasts: ", MSE, "\n")
    }
  }
  SWfore <- list(coef = coef, yhat = yhat, MSE = MSE, loadings = M1, 
      DFindex = Dindex)
}

```



Now that we have the function, let's see if we can get it to work. First, we'll need to compile all of our data

Let's add a code which incorporates the previous values of the return.

```{r}
#' Below is the modified diffusion index code to include lagged variables.
#'
#' @param y - response variable
#' @param x - predictor variables
#' @param orig - forecast origin
#' @param m - number of diffusion indexes used
#' @param tau - VaR level to use; must be between 0 and 1
#' @param ar_tf - AR transformation type. (1 - no transformation,
#' 2 - absolute value, 3 - asymmetric slope)
#' @param p - number of AR lags to include. Default is one.
#' @param print_mdl - option to print the model summary to make sure everytning is ok. 0 is default.
#' @param model - model type (1 - SAV, 2 - AS, 3 - GARCH, 4 - ADAPTIVE) 
#'
#' @return - returns a list of variables for use in the diffusion index
#' @export
#'
#' @examples
mod_di_wl = function (y, x, orig, m, tau, ar_tf = 1, p = 1, print_mdl = 0, model = 1, end = NULL) 
{
  # Converts the response variables into a matrix
  if (!is.matrix(x)) 
      x = as.matrix(x)
  # nT is number of t time-steps
  nT = dim(x)[1]
  # Add a line to establish the number of data points used in the test.
  if (is.null(end) != TRUE){
    nT = end
  }
  # k is the number of diffusion indices used
  k = dim(x)[2]
  # Sanity checks to ensure that the origin isn't past the number of time points
  if (orig > nT) 
      orig = nT
  # Makes sure that there aren't more predictors than there variables in the dataset
  if (m > k) 
      m = k
  # Makes sure there are at least some variables
  if (m < 1) 
      m = 1
  # Subdivides the dataframe
  x1 = x[1:orig, ]
  # Calculates means of each row
  me = apply(x1, 2, mean)
  # Calculates standard deviations of each column
  se = sqrt(apply(x1, 2, var))
  # Creates a matrix x1, which normalizes all the columns. 
  # This may be an issue since it assumes that the distribution is sufficiently described by the first two moments
  x1 = x
  for (i in 1:k) {
      x1[, i] = (x1[, i] - me[i])/se[i]
  }
  V1 = cov(x1[1:orig, ])
  # Performs an eigen decomposition
  m1 = eigen(V1)
  # Selects eigenvalues
  sdev = m1$values
  # Selects eigenvectors
  M = m1$vectors
  # Makes a smaller matrix
  M1 = M[, 1:m]
  # This is the diffusion index model - [orig x p]*[p x m] = [orig x m]
  Dindex = x1 %*% M1
  # Cut down both the response and predictors to be a reasonable size
  y1 = y[1:orig]
  DF = Dindex[1:orig, ]
  # Copy the data frame
  DF_wl = Dindex
  # Lag the y-variable
  for (i in 1:p){
    # Create a lagged variable
    lag_var = lag(y, i)
    # Append the first lag to the data frame
    DF_wl = cbind(DF_wl,lag_var)
  }
  # Identify the right columns
  l_ar = ncol(DF_wl)
  f_ar = l_ar - p + 1
  # Keep the last columns kept to the side
  all_lag = DF_wl[,(f_ar:l_ar)]
  # Cut off the first row to avoid NA's
  DF_trim = DF_wl[1:orig,]
  # Rename the columns
  # Here's the new function with an untransformed AR(p) lag
  if (ar_tf == 1){
    # Incorporate everything in to an input data frame
    df_in = cbind(y1[-(1:p)], DF_trim[-(1:p),])
    # Rename the columns
    # Initialize a character vector
    nvec = c(rep(0, 1+m+p))
    # Populate the vector - first value is the response
    nvec[1] <- names(y)
    # Next are the diffusion indices
    for (i in 1:m){
      nvec[i+1] = paste0("Diff_Index_", i)
    }
    # Next are the lagged variables
    for (i in 1:p){
      nvec[i+1+m] = paste0("Lag_", i)
    }
    # Assign the names
    names(df_in) <- nvec
    # Run the model
    mm = rq(df_in[,1] ~ df_in[,-1], tau = tau)
  }
  # Here's the new function with an SAV AR(p) lag
  if (ar_tf == 2){
    # Incorporate everything in to an input data frame
    df_in = cbind(y1[-(1:p)], DF_trim[-(1:p),-(f_ar:l_ar)], abs(DF_trim[-(1:p),(f_ar:l_ar)]))
    # Rename the columns
    # Initialize a character vector
    nvec = c(rep(0, 1+m+p))
    # Populate the vector - first value is the response
    nvec[1] <- names(y)
    # Next are the diffusion indices
    for (i in 1:m){
      nvec[i+1] = paste0("Diff_Index_", i)
    }
    # Next are the lagged variables
    for (i in 1:p){
      nvec[i+1+m] = paste0("Lag_", i)
    }
    # Assign the names. Note that this is a matrix
    names(df_in) <- nvec
    # Run the model
    mm = rq(df_in[,1] ~ df_in[,-1], tau = tau)
  }
  # Here's the new function with an asymmetric slope for the AR(1) lag
  # Indicator; 0 if percent change is negative, 1 if it's positive
  # indi = ifelse(DF_trim[,ar] < 0, 0, 1)
  if (ar_tf == 3){
    # Create a matrix of indicators
    indi_mat = matrix(0, nrow(DF_wl), p)
    # Generalize the above code
    for (i in 1:p){
      # Populate the indicator
      indi_mat[,i] = ifelse(DF_wl[,f_ar + i - 1] < 0, 0, 1)
    }
  }
  # Fitting the regression
  if (ar_tf == 3){
    # Incorporate everything in to an input data frame
    df_in = cbind(y1[-(1:p)], DF_trim[-(1:p),-(f_ar:l_ar)], DF_trim[-(1:p),(f_ar:l_ar)], indi_mat[((p+1):orig),])
    # Rename the columns
    # Initialize a character vector
    nvec = c(rep(0, 1+m+2*p))
    # Populate the vector - first value is the response
    nvec[1] <- names(y)
    # Next are the diffusion indices
    for (i in 1:m){
      nvec[i+1] = paste0("Diff_Index_", i)
    }
    # Next are the lagged variables
    for (i in 1:p){
      nvec[i+1+m] = paste0("Lag_", i)
    }
    # Last are the positive indicator variables
    for (i in 1:p){
      nvec[i+1+m+p] = paste0("Pos_Val_for_Lag_", i)
    }
    # Assign the names. Note that this is a matrix
    names(df_in) <- nvec
    # Run the model
    mm = rq(df_in[,1] ~ df_in[,-1], tau = tau)
    # mm = rq(y1[-(1:p)] ~ DF_trim[-(1:p),-(f_ar:l_ar)] + DF_trim[-(1:p),(f_ar:l_ar)] + indi_mat[((p+1):orig),], tau = tau)
    # Add a different line to account for the indicator variable
    # intercept + m + 2*nlag to account for the number of indicator variables
    coef = matrix(mm$coefficients, (1 + m + 2*p), 1)
  }
  if (print_mdl == 1){
    print(summary(mm))
  }
  # Puts coefficients in a matrix - added the AR terms
  # coef = matrix(mm$coefficients, (m + 1), 1)
  if (ar_tf != 3){
    coef = matrix(mm$coefficients, (1 + m + p), 1)
  }
  # Initializes yhat variables and MSE
  yhat = NULL
  loss = NULL
  if (orig < nT) {
    # Creates a nfcst by (m+2) matrix
    # Add on the lagged variables
    newx = cbind(rep(1, (nT - orig)), Dindex[(orig + 1):nT, ], all_lag[(orig+1):nT,])
    # Incorporate lagged variables
    if (ar_tf == 3){
      newx = cbind(rep(1, (nT - orig)), Dindex[(orig + 1):nT, ], all_lag[(orig+1):nT,], indi_mat[(orig+1):nT,])
    }
    # [nfcstx(m+1)]*[(m+1)x1] = [nfcstx1]
    yhat = newx %*% coef
    # Calculates errors
    loss = abs(sum(ifelse(y[(orig + 1):nT] > yhat, tau, (-1)*(1-tau))))
    # Modifying this part to only print this if specified
    if (print_mdl == 1){
      cat("Losses of out-of-sample forecasts: ", loss, "\n")
    }
  }
  SWfore <- list(coef = coef, yhat = yhat, loss = loss, loadings = M1, 
      DFindex = Dindex, name_vector = nvec)
}

```


Let's test to see if the new functions work.

```{r}
# var_1pc = mod_di(pc_df[,1], pc_df[,-1], 100, 2, 0.01, print_mdl = 0)
# var_1pc_ar1 = mod_di_wl(pc_df[,1], pc_df[,-1], 100, m = 3, 0.01, ar_tf = 1, print_mdl = 1, p =1)
# var_1pc_ar2 = mod_di_wl(pc_df[,1], pc_df[,-1], 100, m = 2, 0.01, ar_tf = 2, print_mdl = 1, p =4)
# var_1pc_ar3 = mod_di_wl(pc_df[,1], pc_df[,-1], 100, m = 2, 0.01, ar_tf = 3, print_mdl = 1, p =2)
# var_1pc_ar3$name_vector

# ?rq
```

Seems reasonable.

Let's create a quick plot.

```{r}
# Create a plot
# ts.plot(pc_df[101:400,1])
# lines(var_1pc$yhat[1:300], col = "blue", lty = 2)
# lines(var_1pc_ar1$yhat[1:300], col = "red", lty = 2)
# lines(var_1pc_ar2$yhat[1:300], col = "green", lty = 2)
# lines(var_1pc_ar3$yhat[1:300], col = "orange", lty = 2)
```


Testing the old functions

```{r}
# See what the 5% and 1% VaR levels look like
# var_5pc = mod_di(pc_df[,1], pc_df[,-1], 100, 2, 0.05)
# var_1pc = mod_di(pc_df[,1], pc_df[,-1], 100, 2, 0.01)
# 
# # Create a plot
# ts.plot(pc_df[101:400,1])
# lines(var_5pc$yhat[1:300], col = "blue", lty = 2)
# lines(var_1pc$yhat[1:300], col = "red", lty = 2)
# 
# # Calculate accuracy
# var_1pc$yhat
```


## MV Caviar - Calcuating losses

```{r}
# Decide on the optimal number of vectors.

# (y, x, orig, m, tau) 

#' Function that calculates loss over a given period of time for the diffusion index model
#'
#' @param y - response variables
#' @param x - explanatory variable
#' @param orig - forecast origin
#' @param end - forecasting ending. Note: as the function is currently written on 2/24, this option doesn't do anything.
#' @param m - number of diffusion indices to use
#' @param tau - VaR level
#' @param mod_di - use the modified DI?
#'
#' @return - returns a list of the loss sum and the loss vector
#' @export
#'
#' @examples - loss_calc(pc_df[,1], pc_df[,-1], 757, 1027, 1, 0.01)
loss_calc = function(y, x, orig, m, tau, mod_di = 0, ar_tf = 1, p = 1, print_mdl = 0, model = 1, end = NULL){
  # Extract y_hat values
  if (mod_di == 0){
    di = mod_di(y=y,x=x,orig=orig,m=m, tau=tau, end = end, print_mdl = print_mdl)
  }
  else {
    di = mod_di_wl(y=y,x=x,orig=orig,m=m, tau=tau, ar_tf = ar_tf, p = p, print_mdl = print_mdl, model = model, end = end)
  }
  # mod_di_wl = function (y, x, orig, m, tau, ar_tf = 1, p = 1, print_mdl = 0, model = 1)
  yhat = di$yhat[1:(end-orig)]
  # Calculate the loss
  # Initialize loss vector
  lvec = rep(0,(end-orig))
  # Take the difference
  for (i in 1:(end-orig)){
    # Calculate an indicator variable
    ind = ifelse(y[orig+i] < yhat[i], 1,0)
    # Use indicator in function below
    lvec[i] = (tau - ind)*(y[orig+i] - yhat[i])
  }
  # Add up the losses - change to look at sum of losses. Won't change decision criterion
  sumloss = sum(lvec)
  # sumloss = sum(lvec)/length(lvec)
  return(list(sumloss,lvec))
}

# test = loss_calc(pc_df[,1], pc_df[,-1], orig = 757, end = 1007, m = 1, p= 1, 0.01, mod_di = 1, ar_tf = 1)
# test

```



## MV Caviar - Choosing the optimal number of predictors

```{r}
#' Function that selects the optimal number of predictors
#'
#' @param y - response vector
#' @param x - predictor variables
#' @param orig - forecast origin
#' @param end - ending of validation set
#' @param tau - VaR in question
#' @param low_m - low value of m to consider
#' @param high_m - high value of m to consider
#'
#' @return - returns the optimal value of m
#' @export
#'
#' @examples - opt_m(pc_df[,1], pc_df[,-1], 757, 1027, 0.01, low_m =1, high_m  = 5)
opt_m = function(y, x, orig, end = NULL, tau, low_m = 1, high_m, mod_di = 0, ar_tf = 1, p = 1, print_mdl = 0, model = 1, rowname = NULL){
  # Initialize a loss vector
  loss_vec = rep(0,high_m-low_m + 1)
  # Initialize an m vector
  m_vec = seq(low_m, high_m, by = 1)
  # Loop through and populate the loss vector
  for (i in 1:length(loss_vec)){
    loss_vec[i] = quiet(loss_calc(y=y,x=x,orig=orig,end=end, m = m_vec[i], tau = tau, mod_di = mod_di, ar_tf = ar_tf, p = p, print_mdl = print_mdl, model = model))[[1]]
  }
  # Find the minimizer
  opt_m = which.min(loss_vec)
  opt_p = NA
  # Combine into a data frame
  df = as.data.frame(cbind(opt_m, opt_p))
  names(df) <- c("Optimal m", "Optimal p")
  # Assign a rowname
  if (is.null(rowname) == TRUE){
    # Write the row names
    rownames(df) <- c("MV CAViaR")
  }
  else {
    rownames(df) <- rowname
  }
  # Return the loss_vector and the minimzer
  return(list(opt_m, loss_vec, df))
}
```

```{r}
#' Function that selects the optimal number of lags
#'
#' @param y - response vector
#' @param x - predictor variables
#' @param orig - forecast origin
#' @param end - ending of validation set
#' @param tau - VaR in question
#' @param low_m - low value of m to consider
#' @param high_m - high value of m to consider
#'
#' @return - returns the optimal value of m
#' @export
#'
#' @examples - opt_mp(y = pc_df[,1], x = pc_df[,-1], orig = 757, end = 1007, tau = 0.01, low_m =1, high_m  = 5, low_p = 1, high_p = 10, ar_tf = 2, mod_di = 1)
opt_mp = function(y, x, orig, end = NULL, tau, low_m = 1, high_m, low_p = 1, high_p, mod_di = 0, ar_tf = 1, print_mdl = 0, model = 1, print_mp = 0, rowname = NULL){
  # Initialize a loss matrix
  loss_mat = matrix(0, high_p-low_p + 1,high_m-low_m + 1)
  # Initialize a p vector
  p_vec = seq(low_p, high_p, by = 1)
  # Loop through and populate the loss vector
  for (i in 1:nrow(loss_mat)){
    loss_mat[i,] = opt_m(y = y, x = x, orig = orig, end = end, tau = tau, low_m = low_m, high_m  = high_m, p = i, mod_di = mod_di, ar_tf = ar_tf, print_mdl = print_mdl, model = model)[[2]]
  }
  # Find the minimizer
  opt_p = which(loss_mat == min(loss_mat), arr.ind = TRUE)[1,1]
  opt_m = which(loss_mat == min(loss_mat), arr.ind = TRUE)[1,2]
  # Print the optimal p and optimal m
  df = as.data.frame(cbind(opt_m, opt_p))
  names(df) <- c("Optimal m", "Optimal p")
  # Assign a rowname
  if (is.null(rowname) == TRUE){
    if (ar_tf == 1){
      # Write the row names
      rownames(df) <- c("MV CAViaR + AR")
    } else if (ar_tf == 2){
      # Write the row names
      rownames(df) <- c("MV CAViaR + SAV")
    } else if (ar_tf == 3){
      # Write the row names
      rownames(df) <- c("MV CAViaR + AS")
    } else {
      rownames(df) <- c("Unknown Model")
    }
  }
  else {
    rownames(df) <- rowname
  }
  # Print the df if the option is turned on
  if (print_mp == 1){
    print(df)
  }
  # Return the loss_vector and the minimzer
  return(list(opt_m, opt_p, loss_mat, df))
}
```

```{r}
opt_mp_out = opt_mp(y = pc_df[,1], x = pc_df[,-1], orig = 757, end = 1007, tau = 0.01, low_m =1, high_m  = 5, low_p = 1, high_p = 10, ar_tf = 2, mod_di = 1)
# ?persp3D
# persp3D(z = (blah[[3]]*(-1)), theta = 50,
        # ticktype="detailed", xlab="Number of Lags", ylab="Number of Diffusion Indices", zlab="",axes=FALSE)
# axes3d(c('x--','z'))
# Use custom labels
# axis3d(edge= 'x+-', at =seq(1,nrow(blah),by=1)) 
                    # labels = rownames(fd)[seq(500,2000,by=500)] )
# persp3d(x, y, z, theta=50, phi=25, expand=0.75, col=color[zcol],
        # ticktype="detailed", xlab="", ylab="time", zlab="",axes=TRUE)
```

```{r}

# The plot above looks awful
opt_pm[[2]]
m <- 1:5
p <- 1:10
# persp(p,m, z = -opt_pm[[3]], theta = 10)

# Still looks horrendous. We'll come back later, but this probably isn't worth the time fixing.
```

```{r}
# Find the optimal values of p and m for each model
opt_pred_nl = opt_m(y = pc_df[,1], x = pc_df[,-1], orig = 757, end = 1007, tau = 0.01, low_m =1, high_m  = 5)
opt_pm_m1 = opt_mp(y = pc_df[,1], x = pc_df[,-1], orig = 757, end = 1007, tau = 0.01, low_m =1, high_m  = 5, low_p = 1, high_p = 10, ar_tf = 1, mod_di = 1)
opt_pm_m2 = opt_mp(y = pc_df[,1], x = pc_df[,-1], orig = 757, end = 1007, tau = 0.01, low_m =1, high_m  = 5, low_p = 1, high_p = 10, ar_tf = 2, mod_di = 1)
opt_pm_m3 = opt_mp(y = pc_df[,1], x = pc_df[,-1], orig = 757, end = 1007, tau = 0.01, low_m =1, high_m  = 5, low_p = 1, high_p = 10, ar_tf = 3, mod_di = 1)

# Problem solving on 4/22/2020 - Appending these datasets to each other
pm_pretty_df = rbind(opt_pred_nl[[3]], opt_pm_m1[[4]], opt_pm_m2[[4]], opt_pm_m3[[4]])
pm_pretty_df
```

```{r}
#' A function that combines optimal values of m and p into a final table
#'
#' @param m1 - the data frame from the "MV CAViaR" run
#' @param m2 - the data frame from the "MV CAViaR + AR" run
#' @param m3 - the data frame from the "MV CAViaR + SAV" run
#' @param m4 - the data frame from the "MV CAViaR + AS" run
#'
#' @return - a nicely formatted table
#' @export
#'
#' @examples - pretty_pm(opt_pred_nl[[3]], opt_pm_m1[[4]], opt_pm_m2[[4]], opt_pm_m3[[4]])
pretty_pm = function(m1, m2, m3, m4){
  # Merge the individual data frames
  pm_pretty_df = rbind(m1, m2, m3, m4)
  # Format nicely
  pm_pretty_df %>% kable(caption = "Optimal Number of Diffusion Indices (m) and Lags (p) for Different Models", digits = 0) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = "The MV CAViaR model doesn't have an optimal value for p because there are no AR lags in the model"
  )
  
}

pretty_pm(opt_pred_nl[[3]], opt_pm_m1[[4]], opt_pm_m2[[4]], opt_pm_m3[[4]])
```


## MV Caviar - Running the univariate model multiple times

```{r}
#' Here is a function that runs the univariate CAViaR model 4 times
#'
#' @param df - the percent change data frame to consider
#' @param nfcst - number of forecasts to run
#' @param tau - the VaR level to consider
#' @param no_run - specifies if any models should not be run
#'
#' @return - a list of the 4 univariate model forecasts
#' @export
#'
#' @examples - aceg = gen_uv_test(pc_df, 1, 0.05, no_run = c(1,1,0,1))
gen_uv_test = function(df, nfcst, tau, no_run = c(0,0,0,0)){
  # model type (1 - SAV, 2 - AS, 3 - GARCH, 4 - ADAPTIVE) 
  # Initialize a list  
  out_list = list()
  # Run the four models - model 1; SAV
  if (no_run[1] == 0){
    uvcav_1 = rolling_predictions(df[,1], range_data = (1:length(df[,1])), nfcst = nfcst, model = 1, G = 10, col = 1, level = tau)
  }
  # Add a filler if there's no entry
  else {
    uvcav_1 = 0
  }
  # Model 2 - AS
  if (no_run[2] == 0){
    uvcav_2 = rolling_predictions(df[,1], range_data = (1:length(df[,1])), nfcst = nfcst, model = 2, G = 10, col = 1, level = tau)
  }
  else {
    uvcav_2 = 0
  }
  # Model 3 - GARCH
  if (no_run[3] == 0){
    uvcav_3 = rolling_predictions(df[,1], range_data = (1:length(df[,1])), nfcst = nfcst, model = 3, G = 10, col = 1, level = tau)
  }
  else {
    uvcav_3 = 0
  }
  # Model 4 - Adaptive
  if (no_run[4] == 0){
    uvcav_4 = rolling_predictions(df[,1], range_data = (1:length(df[,1])), nfcst = nfcst, model = 4, G = 10, col = 1, level = tau)
  }
  else {
    uvcav_4 = 0
  }
  # Export the data as a list
  return(list(uvcav_1, uvcav_2, uvcav_3, uvcav_4))
}

# blah = list()
```

```{r}
# pc_df
# tail(pc_df)
out = gen_uv_test(df = pc_df, nfcst = 3, tau = 0.01, no_run = c(0,0,0,0))
tail(out[[1]])
```

```{r}
# Problem solving on 4.17.2020
# caviarOptim(input_data[range_data,2])
# sum(is.na(pc_df[,1])) - No NA values

m1 = rolling_predictions(pc_df[,1], range_data = (1:nrow(pc_df)), nfcst = 3, model = 2, G = 10, col = 1)
# nrow(pc_df)
tail(m1)

# No such luck. Let's dive deeper.
rng = (1:(nrow(pc_df)-1))
m1_1 = caviarOptim(pc_df[rng,1])
m1_1

# Let's see if it works for even a different column
m1_dc = rolling_predictions(pc_df[,2], range_data = (1:nrow(pc_df)), nfcst = 3, model = 2, G = 10, col = 1)
# nrow(pc_df)
tail(m1_dc)

# rolling_predictions = function(input_data, range_data = (2:dim(input_data)[1]), nfcst = 250, model =1, level = 0.01, G = 5, col = 2){
#   # Run the varpredict function
#   varpredict <- rollapplyr(input_data[range_data,col], length(range_data) - nfcst, caviarOptim, model, level, predict = 1, k = G) %>% lag
#   # Eliminate NAs
#   # pred_no_na = na.omit(varpredict)
#   # Return the data
#   # return(pred_no_na)
#   return(varpredict)
# }

# It looks like there was an issue with the working directory link

```



## MV Caviar - Plotting function

```{r}
#' Function to plot the data which we generate in previous functions
#'
#' @param plot_matrix - matrix with the data to plot
#' @param norm_value - what to subtact from the data to make it on a percentage change basis. Default is 100.
#'
#' @return 
#' @export - a plot of the data by diffusion index number
#'
#' @examples = plt_data(plot_mtx[[1]]), abc = plt_data(plot_mat, tau = 0.01)
plt_data = function(plot_matrix, tau, resp_var, ntest){
  # Establish a maximum and minimum value
  max_val = max(plot_matrix[,1:ncol(plot_matrix)])
  min_val = min(plot_matrix[,1:ncol(plot_matrix)])
  # Calculate inital and ending time value
  start = index(plot_matrix)[1]
  end = index(plot_matrix)[nrow(plot_matrix)]
  ind_vals = index(plot_matrix) - start
  # Create an initial plot and add lines
    for (i in 1:ncol(plot_matrix)){
      if (i == 1){
        # 4/2/2020 - fixing the index
        plot.ts(ind_vals,plot_matrix[,i], type = "l", xlab = paste("Days Since", as.Date(start)), ylab = "Percent Change in PG", ylim = c(min_val,max_val), lwd = 1, main = paste("Predicting", resp_var, "Returns from", as.Date(start), "to", as.Date(end)), sub = paste("The VaR Level is ", 100*tau, "%", "; There are ", ntest, " Trading Days Plotted Above", sep = ""))
        # plot.ts(index(plot_matrix), plot_matrix[,i], type = "l", xlab = "Trading Days", ylab = "Percent Change in PG", ylim = c(min_val,max_val), lwd = 1, main = "Predicting PG Returns Over Last 250 Trading Days in 2008", sub = paste("The VaR Level is ", 100*tau, "%", sep = ""))
    } else if(i %in% seq(2,8,1)) {
        lines(ind_vals,plot_matrix[,i], col = i-1, lty = 2)
    } else {
        lines(ind_vals,plot_matrix[,i], col = i-1, lty = 2, lwd = 2)
    }
    }
  # Define a sequence for plotting
  plot_seq = seq(1, ncol(plot_matrix))
  legend("topleft", legend = c(colnames(plot_matrix)), col = plot_seq, lty = c(1, rep(2, 7), rep(3, ifelse(ncol(plot_matrix)-8 <= 0, 0, ncol(plot_matrix)-8))), lwd = c(1, rep(1, 7), rep(2, ifelse(ncol(plot_matrix)-8 <= 0, 0, ncol(plot_matrix)-8))))
  # Add a line for 0
  # abline(h = 0, col = "black", lty = 2)
}


```

## MV Caviar - Old Loss Test Function

```{r}
#' A function to calculate losses based on the test sample
#'
#' @param true_vec - the true vector of returns
#' @param pred_vec - the predicted vector from the model runs
#' @param tau - VaR level. Must match what the model used
#'
#' @return - total losses and the entire loss vector
#' @export
#'
#' @examples
loss_test = function(true_vec, pred_vec, tau){
  # Initialize a loss vector
  lvec = rep(0, length(true_vec))
  # Initialize a break vector to see when VaR is broken
  bvec = rep(0, length(true_vec))
  for (i in 1:length(true_vec)){
    # Calculate an indicator variable
    bvec[i] = ifelse(true_vec[i] < pred_vec[i], 1,0)
    # Use indicator in function below
    lvec[i] = (tau - bvec[i])*(true_vec[i] - pred_vec[i])
  }
  # Add up the losses
  # sumloss = sum(lvec)/length(lvec)
  sumloss = sum(lvec)
  # Add up the VaR breakage
  varbreak = sum(bvec)/length(bvec)
  return(list(sumloss,lvec, varbreak, bvec))
}
```



## MV Caviar - A generalized loss calculation function

```{r}
#' A function to calculate losses based on the plot matrix
#'
#' @param data_mat - a matrix of forecasted VaR values, with the true value in the first column
#' @param tau - VaR level. Must match what the model used
#'
#' @return - a list of four items. 
#' 1 = a vector of the losses of all models. 
#' 2 = a vector showing the percentage of VaR breaks by model
#' 3 = the loss matrix
#' 4 = the break matrix
#' @export
#'
#' @examples
gen_loss_test = function(data_mat, tau){
  # Initialize loss and break matrices
  lmat = bmat = matrix(0, nrow = nrow(data_mat), ncol = ncol(data_mat)-1)
  # bvec = rep(0, length(true_vec))
  # Populate the matrices
  for (i in 1:nrow(lmat)){
    for (j in 1:(ncol(lmat))){
      # Calculate an indicator variable
      bmat[i,j] = ifelse(data_mat[i,1] < data_mat[i,j+1], 1,0)
      # Use indicator in function below
      lmat[i,j] = (tau - bmat[i,j])*(data_mat[i,1] - data_mat[i,j+1])
    }    
  }
  # Add up the losses
  sumloss = colSums(lmat)
  # Add up the VaR breakage
  varbreak = colSums(bmat)/nrow(bmat)
  return(list(sumloss, varbreak, lmat, bmat))
}
```


## MV Caviar - Tables function

```{r}
#' A function to make a nice comparison of losses
#'
#' @param data_mat - input data matrix used in the calculation of losses
#' @param loss_list - a list of the losses calculated from the CAViaR function
#' @param tau - the risk level used
#' @param ntest - the number of test points
#'
#' @return
#' @export - returns a nicely formatted table
#'
#' @examples - pretty_tables(plot_mat, l_list, tau = 0.01)
pretty_tables = function(data_mat, loss_list, tau, ntest){
  # Combine into a data frame
  df = as.data.frame(rbind(loss_list[[1]], loss_list[[2]]))
  # Calculate inital and ending time value
  start = index(data_mat)[1]
  end = index(data_mat)[nrow(data_mat)]
  # Add row/column names
  colnames(df) <- colnames(data_mat[,-1])
  rownames(df) <- c("Losses", "VaR Breaks (%)")
  # Convert to a table
  df %>% kable(caption = paste("Comparison of VaR Methods for a ", tau*100, "% VaR", sep = ""), digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling() %>% footnote(general = paste("Calculated using", ntest, "trading days from", as.Date(start), "to", as.Date(end)))
}
```

```{r}
# Problem solving on 4/22 - Check that the plot matrix works
pretty_tables(var_1pc_2010_usetf[[1]], var_1pc_2010_usetf[[3]], 0.01, 250)
```


## Export function

Problem solving on 4.23.2020 for exports.

```{r}
# index(var_1pc_2016_usetf[[1]])

# DONE - this works.
# write.zoo(var_1pc_2016_usetf[[1]],"/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/var_1pc_2016_usetf_TEST.csv", quote = FALSE, sep = ",")
```


```{r}
#' A dressed up version of the export function
#'
#' @param var_file - file to export
#' @param path - filepath
#' @param filename - name of the file, ending with .CSV
#'
#' @return
#' @export - exported CSV file
#'
#' @examples - exp_func(var_file = var_1pc_2016_usetf[[1]], path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "TEST.csv")
exp_func = function(var_file, path, filename){
  # Write a zoo
  write.zoo(var_file, paste0(path, filename), quote = FALSE, sep = ",")
}

# exp_func(var_file = var_1pc_2016_usetf[[1]], path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "TEST.csv")

```

```{r}
# test_df = head(var_5pc_2010_usetf[[1]][,6:9])
# test_df$SAV
# test_df$`Abs. Slope`
# test_df$`Ind. GARCH`
# test_df$Adaptive
```


# Big Simulation Function

```{r}
#' This is the "master" function where we'll evaluate the importance of the VaR model over several time periods
#'
#' @param symbol_list - a list of symbols to feed into the model 
#' @param resp_var - the response variable
#' @param compl_case - should the model require complete cases? Default value is 1.
#' @param adj_close - use adjusted close price for the predictors? Default value is 1.
#' @param resp_adj_close - use adjusted close price for the response? Default value is 1.
#' @param start_date - start date to pull data from
#' @param end_date - end date to pull data from
#' @param nval - number of validation points to use
#' @param ntest - number of test points to use
#' @param tau - VaR level to use
#' @param low_m - low number of predictors to test
#' @param high_m  - low number of predictors to test
#' @param uv_list - a list of a pre-run univariate model. If a data frame is not provided, the lengthy uv model will run 
#' @param no_run - things not to run in the model
#' @param low_p - low value for number of lags
#' @param high_p - high value for number of lags
#' @param na_interp - should the function interpolate NA's
#' @param print_mdl - print the model summaries?
#' @param print_mp - print the optimal values for p and m
#' @param lag_pred - do you want to lag the m predictors (default is 1; strongly recommended)
#' @param rowname - what to name the rows of the nice p and m matrix
#' @param export_csv - do you want to export a CSV? Default is 1.
#' @param path - path to export the CSV
#' @param filename - what to name the CSV
#'
#' @return - a list of the plot matrix, a plot, a list with losses, and a table
#' @export - a plot and a table
#'
#' @examples - cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 5, tau = 0.01, uv_list = uv_cav_list)
cav_simul = function(symbol_list, resp_var, compl_case = 1, adj_close = 1, resp_adj_close = 1, start_date = "1900-01-01", end_date = Sys.Date(), nval, ntest, tau, low_m = 1, high_m, low_p = 1, high_p, uv_list = NULL, no_run = c(0,0,0,0), na_interp = TRUE, print_mdl = 0, print_mp = 0, lag_pred = 1, rowname = NULL, export_csv = 1, path, filename){
  # Select data parameters, pull the data, and percent change the data
  df = diff_index_df(symbol_list = symbol_list, resp_var = resp_var, compl_case = compl_case, adj_close = adj_close, resp_adj_close = resp_adj_close, start_date = start_date, end_date = end_date, lag_pred = lag_pred)
  # Take the percent change of the data
  pc_df = pc_diff_index(df)
  # Extract the legnth of the data frame
  nr = test_end = nrow(pc_df)
  # Calculate the start of the val period, the end of the val period, and the beginning and end of test period
  test_orig = test_end - ntest
  val_end = test_orig
  val_orig = test_orig - nval
  # Test for the optimal number of parameters
  opt_pred_nl = opt_m(y = pc_df[,1], x = pc_df[,-1], orig = val_orig, end = val_end, tau = tau, low_m = low_m, high_m = high_m, rowname = rowname)
  opt_pred_art1 = opt_mp(y = pc_df[,1], x = pc_df[,-1], orig = val_orig, end = val_end, tau = tau, low_m = low_m, high_m = high_m, low_p = low_p, high_p = high_p,mod_di = 1, ar_tf = 1, print_mdl = print_mdl, print_mp = print_mp, rowname = rowname)
  opt_pred_art2 = opt_mp(y = pc_df[,1], x = pc_df[,-1], orig = val_orig, end = val_end, tau = tau, low_m = low_m, high_m = high_m, low_p = low_p, high_p = high_p,mod_di = 1, ar_tf = 2, print_mdl = print_mdl, print_mp = print_mp, rowname = rowname)
  opt_pred_art3 = opt_mp(y = pc_df[,1], x = pc_df[,-1], orig = val_orig, end = val_end, tau = tau, low_m = low_m, high_m = high_m, low_p = low_p, high_p = high_p, mod_di = 1, ar_tf = 3, print_mdl = print_mdl, print_mp = print_mp, rowname = rowname)
  # gen_uv_test(pc_df, 1, 0.05, no_run = c(1,1,0,1))
  # Use the above forecasts to input into the above
  mv_fcst = mod_di(pc_df[,1], pc_df[,-1], orig = test_orig, m = opt_pred_nl[[1]], tau = tau, print_mdl = print_mdl)
  mv_fcst_art1 = mod_di_wl(pc_df[,1], pc_df[,-1], orig = test_orig, m = opt_pred_art1[[1]], p = opt_pred_art1[[2]], tau = tau, ar_tf = 1, print_mdl = print_mdl)
  mv_fcst_art2 = mod_di_wl(pc_df[,1], pc_df[,-1], orig = test_orig, m = opt_pred_art2[[1]], p = opt_pred_art2[[2]], tau = tau, ar_tf = 2, print_mdl = print_mdl)
  mv_fcst_art3 = mod_di_wl(pc_df[,1], pc_df[,-1], orig = test_orig, m = opt_pred_art3[[1]], p = opt_pred_art3[[2]], tau = tau, ar_tf = 3, print_mdl = print_mdl)
  # Calculate the number of predictions
  if (is.null(uv_list) == TRUE){
    # Print a warning
    print("WARNING: Not supplying an input data frame will require this function to run for a significant amount of time (1hr+)")
    # Call the function
    # gen_uv_test = function(df, nfcst, tau, no_run = c(0,0,0,0)){
    # print(head(pc_df))
    uv_list = gen_uv_test(df = pc_df, nfcst = ntest, tau = tau, no_run = no_run)
    # Add to a data frame
    # Incorporate the rolling predictions function results here
  plot_mat = cbind(pc_df[(test_orig+1):nrow(pc_df),1], mv_fcst$yhat[1:ntest], mv_fcst_art1$yhat[1:ntest], mv_fcst_art2$yhat[1:ntest], mv_fcst_art3$yhat[1:ntest], uv_list[[1]][(test_orig+1):test_end]*(-1), uv_list[[2]][(test_orig+1):test_end]*(-1), uv_list[[3]][(test_orig+1):test_end]*(-1), uv_list[[4]][(test_orig+1):test_end]*(-1))
  } else {
    # Assign the columns of the data frame
    # head(var_5pc_2010_usetf[[1]][,6:9])
    # model type (1 - SAV, 2 - AS, 3 - GARCH, 4 - ADAPTIVE) 
    # test_df = head(var_5pc_2010_usetf[[1]][,6:9])
    # test_df$SAV
    # test_df$`Abs. Slope`
    # test_df$`Ind. GARCH`
    # test_df$Adaptive
    plot_mat = cbind(pc_df[(test_orig+1):nrow(pc_df),1], mv_fcst$yhat[1:ntest], mv_fcst_art1$yhat[1:ntest], mv_fcst_art2$yhat[1:ntest], mv_fcst_art3$yhat[1:ntest], uv_list$SAV, uv_list$`Abs. Slope`, uv_list$`Ind. GARCH`, uv_list$Adaptive)
  }
  # Count the NAs and print a warning
  print(paste("NOTE: There are ", sum(is.na(plot_mat)), " NA(s) in the dataset", sep = ""))
  # Linearly interpolate the NAs
  if (na_interp == TRUE){
    # Assign the plot matrix to a new value
    plot_mat_na <- plot_mat
    # Print a warning
    print("WARNING: There were missing values in the plot matrix.")
    # Interpolate the NA's
    for (i in 1:ncol(plot_mat_na)){
      # Interpolate the data
      plot_mat[,i] <- na.approx(plot_mat_na[,i])
    }
  }
  # model type (1 - SAV, 2 - AS, 3 - GARCH, 4 - ADAPTIVE)
  # Add descriptive titles onto the plot_mat
  colnames(plot_mat) <- c("PG", "MV CAViaR", "MV CAViaR + AR", "MV CAViaR + SAV", "MV CAViaR + AS", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive")
  # Plot everything
  plot = plt_data(plot_mat, tau = tau, resp_var = resp_var, ntest = ntest)
  # Calculate losses
  l_list = gen_loss_test(plot_mat, tau = tau)
  # Put into tables
  tables = pretty_tables(plot_mat, l_list, tau = tau, ntest = ntest)
  # Run the function for optimal p and m
  pm_table = pretty_pm(opt_pred_nl[[3]], opt_pred_art1[[4]], opt_pred_art2[[4]], opt_pred_art3[[4]])
  # Export the matrix
  if (export_csv == 1){
    exp_func(var_file = plot_mat, path, filename)
  }
  # Print the tables and the plot
  print(plot)
  print(tables)
  print(pm_table)
  return(list(plot_mat, plot, l_list, tables, plot_mat_na, pm_table))
}
```

```{r}
var_1pc_2008_sm = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 3, low_m = 1, high_m = 5, low_p = 1, high_p = 10, tau = 0.01, print_mp = 1, path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "small_TEST.csv")

var_1pc_2008_sm_TEST = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 3, low_m = 1, high_m = 5, low_p = 1, high_p = 10, tau = 0.01, print_mp = 1, path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "small_TEST.csv", uv_list = var_1pc_2008_sm[[1]])

# var_1pc_2008_sm[[1]]$`Ind. GARCH`
# var_1pc_2008_sm[[1]]$Adaptive

# var_1pc_2008_sm_TEST[[1]] - var_1pc_2008_sm[[1]]
# tail(var_1pc_2008_sm)
# plt_data(var_1pc_2008_sm[[1]], tau = 0.01, resp_var = "PG", ntest = 3)
# init = index(var_1pc_2008_sm[[1]])[1]C
# index(var_1pc_2008_sm[[1]]) - init +1
# var_1pc_2008_sm


```



# Test runs

Here we're going to test a 1, 5 and 10% VaR run.
Today, I'm just going to kick off the 1% run and analyze it.

```{r}
# var_0_05_pc_2008 = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 5, tau = 0.005)
var_1pc_2008_mp = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 5, low_p = 1, high_p = 10,tau = 0.01, print_mdl = 1, print_mp = 1)
var_5pc_2008_mp = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 5, low_p = 1, high_p = 10,tau = 0.05, print_mdl = 1, print_mp = 1)
# var_5pc_2008 = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 5, tau = 0.05)
# var_10pc_2008 = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 5, tau = 0.10)
# var_20pc_2008 = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 5, tau = 0.20)

# plt_data(var_1pc_2008_mp[[1]], tau = 0.01, resp_var = "SPY", ntest = 250)
# var_1pc_2008_mp[[4]]



```

```{r}

plt_data(var_5pc_2008_mp[[1]], 0.05, "PG", 250)
```

# Sector ETFs as Explanatory Variables

Here's what I'm thinking.

Lowest level: 1%, 5%, 10% VaR
Next, 2008, 2010, 2014, or 2016
Highest - ETF choices
3 x 4 x 4 = 48 runs

## 2008 Ending

### U.S. ETFs

```{r}
# 1%, 5%, 10% VaR - 2008 - 1st set of predictors
# var_1pc_2008_usetf = cav_simul(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 9, low_p = 1, high_p = 10, tau = 0.01, print_mdl = 1, print_mp = 1)
# var_5pc_2008_usetf = cav_simul(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 9, low_p = 1, high_p = 10,tau = 0.05, print_mdl = 1, print_mp = 1)
# var_10pc_2008_usetf = cav_simul(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 9, low_p = 1, high_p = 10,tau = 0.10, print_mdl = 1, print_mp = 1)
```

### Global ETFs

```{r}
# 1%, 5%, 10% VaR - 2008 - 1st set of predictors
var_1pc_2008_glob_etf = cav_simul(c("JXI", "KXI", "IXJ", "IXP", "IXN", "RXI", "EXI", "IXG", "MXI", "IXC"), resp_var = "SPY", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 10, low_p = 1, high_p = 10, tau = 0.01, print_mdl = 1, print_mp = 1, path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_Glob_ETF_runs/", filename = "var_1pc_2008_glob_etf.csv", uv_list = var_1pc_2008_usetf[[1]])

var_5pc_2008_glob_etf = cav_simul(c("JXI", "KXI", "IXJ", "IXP", "IXN", "RXI", "EXI", "IXG", "MXI", "IXC"), resp_var = "SPY", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 10, low_p = 1, high_p = 10,tau = 0.05, print_mdl = 1, print_mp = 1, path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_Glob_ETF_runs/", filename = "var_5pc_2008_glob_etf.csv", uv_list = var_5pc_2008_usetf[[1]])

var_10pc_2008_glob_etf = cav_simul(c("JXI", "KXI", "IXJ", "IXP", "IXN", "RXI", "EXI", "IXG", "MXI", "IXC"), resp_var = "SPY", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 10, low_p = 1, high_p = 10,tau = 0.10, print_mdl = 1, print_mp = 1, path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_Glob_ETF_runs/", filename = "var_10pc_2008_glob_etf.csv", uv_list = var_10pc_2008_usetf[[1]])
```


```{r}
# Export the data as CSVs
# write.csv(var_1pc_2008_usetf[[1]],"/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/var_1pc_2008_usetf.csv", row.names = TRUE)
# write.csv(var_5pc_2008_usetf[[1]],"/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/var_5pc_2008_usetf.csv", row.names = TRUE)
# write.csv(var_10pc_2008_usetf[[1]],"/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/var_10pc_2008_usetf.csv", row.names = TRUE)

# Call the new export function
# exp_func(var_file = var_1pc_2008_usetf[[1]], path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "var_1pc_2008_usetf.csv")
# exp_func(var_file = var_5pc_2008_usetf[[1]], path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "var_5pc_2008_usetf.csv")
# exp_func(var_file = var_10pc_2008_usetf[[1]], path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "var_10pc_2008_usetf.csv")
# 

```




## 2010 Ending Year

### U.S. ETFs

```{r}
# 2010 Ending year 
# 1%, 5%, 10% VaR - 2010 - 1st set of predictors
# var_1pc_2010_usetf = cav_simul(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2006-01-01", end_date = "2010-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 9, low_p = 1, high_p = 10,tau = 0.01, print_mdl = 1, print_mp = 1)
# var_5pc_2010_usetf = cav_simul(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2006-01-01", end_date = "2010-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 9, low_p = 1, high_p = 10,tau = 0.05, print_mdl = 1, print_mp = 1)
# var_10pc_2010_usetf = cav_simul(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2006-01-01", end_date = "2010-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 9, low_p = 1, high_p = 10,tau = 0.10, print_mdl = 1, print_mp = 1)
```

### Global ETFs

```{r}
# 1%, 5%, 10% VaR - 2008 - 1st set of predictors
var_1pc_2010_glob_etf = cav_simul(c("JXI", "KXI", "IXJ", "IXP", "IXN", "RXI", "EXI", "IXG", "MXI", "IXC"), resp_var = "SPY", start_date = "2006-01-01", end_date = "2010-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 10, low_p = 1, high_p = 10, tau = 0.01, print_mdl = 1, print_mp = 1, path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_Glob_ETF_runs/", filename = "var_1pc_2010_glob_etf.csv", uv_list = var_1pc_2010_usetf[[1]])

var_5pc_2010_glob_etf = cav_simul(c("JXI", "KXI", "IXJ", "IXP", "IXN", "RXI", "EXI", "IXG", "MXI", "IXC"), resp_var = "SPY", start_date = "2006-01-01", end_date = "2010-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 10, low_p = 1, high_p = 10,tau = 0.05, print_mdl = 1, print_mp = 1, path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_Glob_ETF_runs/", filename = "var_5pc_2010_glob_etf.csv", uv_list = var_5pc_2010_usetf[[1]])

var_10pc_2010_glob_etf = cav_simul(c("JXI", "KXI", "IXJ", "IXP", "IXN", "RXI", "EXI", "IXG", "MXI", "IXC"), resp_var = "SPY", start_date = "2006-01-01", end_date = "2010-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 10, low_p = 1, high_p = 10,tau = 0.10, print_mdl = 1, print_mp = 1, path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_Glob_ETF_runs/", filename = "var_10pc_2010_glob_etf.csv", uv_list = var_10pc_2010_usetf[[1]])
```

```{r}
# Export the data as CSVs
# write.csv(var_1pc_2010_usetf[[1]],"/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/var_1pc_2010_usetf.csv", row.names = TRUE)
# write.csv(var_5pc_2010_usetf[[1]],"/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/var_5pc_2010_usetf.csv", row.names = TRUE)
# write.csv(var_10pc_2010_usetf[[1]],"/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/var_10pc_2010_usetf.csv", row.names = TRUE)

# Call the new export function
# exp_func(var_file = var_1pc_2010_usetf[[1]], path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "var_1pc_2010_usetf.csv")
# exp_func(var_file = var_5pc_2010_usetf[[1]], path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "var_5pc_2010_usetf.csv")
# exp_func(var_file = var_10pc_2010_usetf[[1]], path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "var_10pc_2010_usetf.csv")

# Problem solving on 4.23.2020
test_df = head(var_5pc_2010_usetf[[1]][,6:9])
test_df$SAV

```

## 2014 Ending Year

```{r}


# 1%, 5%, 10% VaR - 2014 - 1st set of predictors
# var_1pc_2014_usetf = cav_simul(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2010-01-01", end_date = "2014-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 9, low_p = 1, high_p = 10,tau = 0.01, print_mdl = 1, print_mp = 1)
# var_5pc_2014_usetf = cav_simul(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2010-01-01", end_date = "2014-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 9, low_p = 1, high_p = 10,tau = 0.05, print_mdl = 1, print_mp = 1)
# var_10pc_2014_usetf = cav_simul(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2010-01-01", end_date = "2014-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 9, low_p = 1, high_p = 10,tau = 0.10, print_mdl = 1, print_mp = 1)
```

```{r}
# 1%, 5%, 10% VaR - 2008 - 1st set of predictors
var_1pc_2014_glob_etf = cav_simul(c("JXI", "KXI", "IXJ", "IXP", "IXN", "RXI", "EXI", "IXG", "MXI", "IXC"), resp_var = "SPY", start_date = "2010-01-01", end_date = "2014-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 10, low_p = 1, high_p = 10, tau = 0.01, print_mdl = 1, print_mp = 1, path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_Glob_ETF_runs/", filename = "var_1pc_2014_glob_etf.csv", uv_list = var_1pc_2014_usetf[[1]])

var_5pc_2014_glob_etf = cav_simul(c("JXI", "KXI", "IXJ", "IXP", "IXN", "RXI", "EXI", "IXG", "MXI", "IXC"), resp_var = "SPY", start_date = "2010-01-01", end_date = "2014-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 10, low_p = 1, high_p = 10,tau = 0.05, print_mdl = 1, print_mp = 1, path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_Glob_ETF_runs/", filename = "var_5pc_2014_glob_etf.csv", uv_list = var_5pc_2014_usetf[[1]])

var_10pc_2014_glob_etf = cav_simul(c("JXI", "KXI", "IXJ", "IXP", "IXN", "RXI", "EXI", "IXG", "MXI", "IXC"), resp_var = "SPY", start_date = "2010-01-01", end_date = "2014-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 10, low_p = 1, high_p = 10,tau = 0.10, print_mdl = 1, print_mp = 1, path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_Glob_ETF_runs/", filename = "var_10pc_2014_glob_etf.csv", uv_list = var_10pc_2014_usetf[[1]])
```

```{r}
# Export the data as CSVs
# write.csv(var_1pc_2014_usetf[[1]],"/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/var_1pc_2014_usetf.csv", row.names = TRUE)
# write.csv(var_5pc_2014_usetf[[1]],"/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/var_5pc_2014_usetf.csv", row.names = TRUE)
# write.csv(var_10pc_2014_usetf[[1]],"/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/var_10pc_2014_usetf.csv", row.names = TRUE)

# Call the new export function
# exp_func(var_file = var_1pc_2014_usetf[[1]], path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "var_1pc_2014_usetf.csv")
# exp_func(var_file = var_5pc_2014_usetf[[1]], path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "var_5pc_2014_usetf.csv")
# exp_func(var_file = var_10pc_2014_usetf[[1]], path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "var_10pc_2014_usetf.csv")


```

```{r}
## 2016 Ending Year

# 1%, 5%, 10% VaR - 2016 - 1st set of predictors
# var_1pc_2016_usetf = cav_simul(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2012-01-01", end_date = "2016-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 9, low_p = 1, high_p = 10,tau = 0.01, print_mdl = 1, print_mp = 1)
# var_5pc_2016_usetf = cav_simul(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2012-01-01", end_date = "2016-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 9, low_p = 1, high_p = 10,tau = 0.05, print_mdl = 1, print_mp = 1)
# var_10pc_2016_usetf = cav_simul(c("XLU", "XLP", "XLV", "XLK", "XLY", "XLI", "XLF", "XLB", "XLE"), resp_var = "SPY", start_date = "2012-01-01", end_date = "2016-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 9, low_p = 1, high_p = 10,tau = 0.10, print_mdl = 1, print_mp = 1)
```

```{r}
# Export the data as CSVs
# write.csv(var_1pc_2016_usetf[[1]],"/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/var_1pc_2016_usetf.csv", row.names = TRUE)
# write.csv(var_5pc_2016_usetf[[1]],"/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/var_5pc_2016_usetf.csv", row.names = TRUE)
# write.csv(var_10pc_2016_usetf[[1]],"/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/var_10pc_2016_usetf.csv", row.names = TRUE)

# Call the new export function
# exp_func(var_file = var_1pc_2016_usetf[[1]], path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "var_1pc_2016_usetf.csv")
# exp_func(var_file = var_5pc_2016_usetf[[1]], path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "var_5pc_2016_usetf.csv")
# exp_func(var_file = var_10pc_2016_usetf[[1]], path = "/Users/stevenmoen/Documents/GitHub/CAViaR_MS_thesis/Data_Export/SPY_US_ETF_runs/", filename = "var_10pc_2016_usetf.csv")
```

## Export the tables


```{r}
# Export the tables - 2008
var_1pc_2008_usetf[[4]]
var_5pc_2008_usetf[[4]]
var_10pc_2008_usetf[[4]]

# 2010
var_1pc_2010_usetf[[4]]
var_5pc_2010_usetf[[4]]
var_10pc_2010_usetf[[4]]

# 2014
var_1pc_2014_usetf[[4]]
var_5pc_2014_usetf[[4]]
var_10pc_2014_usetf[[4]]

# 2016
var_1pc_2016_usetf[[4]]
var_5pc_2016_usetf[[4]]
var_10pc_2016_usetf[[4]]
```




##### JUNK CODE ########

This function uses an absolute reference to the number of forecasts to perform. I'm changing this to make it compatible with the uv_cav1 model.

```{r}

#' This is the "master" function where we'll evaluate the importance of the VaR model over several time periods
#'
#' @param symbol_list - a list of symbols to feed into the model 
#' @param resp_var - the response variable
#' @param compl_case - should the model require complete cases? Default value is 1.
#' @param adj_close - use adjusted close price for the predictors? Default value is 1.
#' @param resp_adj_close - use adjusted close price for the response? Default value is 1.
#' @param start_date - start date to pull data from
#' @param end_date - end date to pull data from
#' @param val_orig - start of the validation period
#' @param val_end - end of the validation period
#' @param test_orig - start of the test period
#' @param tau - VaR level to use
#' @param low_m - low number of predictors to test
#' @param high_m  - low number of predictors to test
#' @param uv_list 
#'
#' @return
#' @export
#'
#' @examples
cav_simul_OLD = function(symbol_list, resp_var, compl_case = 1, adj_close = 1, resp_adj_close = 1, start_date = "1900-01-01", end_date = Sys.Date(), val_orig, val_end, test_orig, tau, low_m = 1, high_m, uv_list){
  # Select data parameters, pull the data, and percent change the data
  df = diff_index_df(symbol_list = symbol_list, resp_var = resp_var, compl_case = compl_case, adj_close = adj_close, resp_adj_close = resp_adj_close, start_date = start_date, end_date = end_date)
  # Take the percent change of the data
  pc_df = pc_diff_index(df)
  # Test for the optimal number of parameters
  opt_pred_nl = opt_m(y = pc_df[,1], x = pc_df[,-1], orig = val_orig, end = val_end, tau = tau, low_m = low_m, high_m = high_m)
  opt_pred_art1 = opt_m(y = pc_df[,1], x = pc_df[,-1], orig = val_orig, end = val_end, tau = tau, low_m = low_m, high_m = high_m, mod_di = 1, ar_tf = 1)
  opt_pred_art2 = opt_m(y = pc_df[,1], x = pc_df[,-1], orig = val_orig, end = val_end, tau = tau, low_m = low_m, high_m = high_m, mod_di = 1, ar_tf = 2)
  opt_pred_art3 = opt_m(y = pc_df[,1], x = pc_df[,-1], orig = val_orig, end = val_end, tau = tau, low_m = low_m, high_m = high_m, mod_di = 1, ar_tf = 3)
  # gen_uv_test(pc_df, 1, 0.05, no_run = c(1,1,0,1))
  # Use the above forecasts to input into the above
  mv_fcst = mod_di(pc_df[,1], pc_df[,-1], orig = test_orig, m = opt_pred_nl[[1]], tau = tau)
  mv_fcst_art1 = mod_di_wl(pc_df[,1], pc_df[,-1], orig = test_orig, m = opt_pred_art1[[1]], tau = tau, ar_tf = 1)
  mv_fcst_art2 = mod_di_wl(pc_df[,1], pc_df[,-1], orig = test_orig, m = opt_pred_art2[[1]], tau = tau, ar_tf = 2)
  mv_fcst_art3 = mod_di_wl(pc_df[,1], pc_df[,-1], 1007, m = opt_pred_art3[[1]], tau = tau, ar_tf = 3)
  # Calculate the number of predictions
  npred = (nrow(pc_df)-(test_orig))
  # Incorporate the rolling predictions function results here
  plot_mat = cbind(pc_df[(test_orig+1):nrow(pc_df),1], mv_fcst$yhat[1:npred], mv_fcst_art1$yhat[1:npred], mv_fcst_art2$yhat[1:npred], mv_fcst_art3$yhat[1:npred], uvcav_1[(test_orig+1):nrow(pc_df)]*(-1), uvcav_2[(test_orig+1):nrow(pc_df)]*(-1), uvcav_3[(test_orig+1):nrow(pc_df)]*(-1), uvcav_4[(test_orig+1):nrow(pc_df)]*(-1))
  # model type (1 - SAV, 2 - AS, 3 - GARCH, 4 - ADAPTIVE) 
  # Add descriptive titles onto the plot_mat
  colnames(plot_mat) <- c("PG", "MV CAViaR", "MV CAViaR + AR(1)", "MV CAViaR + SAV", "MV CAViaR + AS", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive")
  # Plot everything
  plot = plt_data(plot_mat)
  # Calculate losses
  l_list = gen_loss_test(plot_mat, tau = tau)
  # Put into tables
  tables = pretty_tables(plot_mat, l_list)
  # Print the tables and the plot
  print(plot)
  print(tables)
  return(list(plot_mat, plot, l_list, tables))
}
```


```{r}
# Testing to see how apply function works
test_mat = matrix(rep(seq(1,5),5), nrow=5, ncol = 5)
# ?as.matrix
# Test apply function. Row means should be 1 through 5, col means should all be 3
row_me = apply(test_mat, 1, mean)
col_me = apply(test_mat, 2, mean)
row_se = sqrt(apply(test_mat, 1, var))
col_se = sqrt(apply(test_mat, 2, var))

# print(c(row_me, col_me, row_se, col_se))

# Exactly as I expected.

# test_mat
```

```{r}
# Test the matrix multiplication
```

```{r}
# Test to see if using lag is possible in general LM model

# Set random seed
set.seed(100)
# ?rnorm
y <- rnorm(1000)
x <- matrix(0,1000,10)
for (i in 1:10){
  x[,i] <- rnorm(1000)
}

# First, see if it's possible to do it with a lag
l1_y = lag(as.ts(y), k = 1)
# l1_y


# Fit model
arima(y, c(1,0,0))
# arima
head(l1_y)
head(y)
y_t = y[-1]
l1_y
# as.vector(l1_y)

# y_t
# l1_y
length(y_t)
length(l1_y)

ar_lag <- lm(y_t ~ as.vector(l1_y))
# ?lag
summary(ar_lag)


```

```{r}
df[1:5,]

# Let's take the first variable, lag it, and then append it

# Poor man's lag
pg_lag = c(NA, pc_df[-length(pc_df),1])
# pml

# Append to the data frame
pc_df2 = cbind(pc_df,pg_lag)
pc_trim = pc_df2[-1,]

# Run the rq regression
rq_test = rq(pc_trim[,1] ~ pc_trim[,-1], tau = 0.01)
summary(rq_test)

```

```{r}
rns = rnorm(1000)
quantile(rns, 0.01)
```

```{r}
# index(pc_df[1008,])

# Create a plot
ts.plot(pc_df[1008:1257,1], ylim = c(-0.1,0.1), xlab = "Trading Days", ylab = "Percent Change in PG", main = "Predicting PG Returns Over Last 250 Trading Days in 2008", sub = paste("First Trading Day is ", index(pc_df[1008,]), ", Last Trading Day is ", index(pc_df[1257,]), sep=""))
legend("topleft", legend=c("Actual Return", "Multivariate CAViaR", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "blue", "red", "green", "orange", "purple"), lty = c(1,2,2,2,2,2))
lines(mv_fcst$yhat[1:250], col = "blue", lty = 2)
lines(as.ts(uvcav_1[1008:1257])*(-1), col = "red", lty = 2)
lines(as.ts(uvcav_2[1008:1257])*(-1), col = "green", lty = 2)
lines(as.ts(uvcav_3[1008:1257])*(-1), col = "orange", lty = 2)
lines(as.ts(uvcav_4[1008:1257])*(-1), col = "purple", lty = 2)
# pc_df[1008:1257,1]

# legend("topright", legend=c("Simulated Return", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "red", "green", "blue", "purple"), lty = c(1,2,2,2,2))
# lines(m1_mth[2:26]*(-1), col = "red", lty = 2)
# lines(m2_mth[2:26]*(-1), col = "green", lty = 2)
# lines(m3_mth[2:26]*(-1), col = "blue", lty = 2)
# lines(m4_mth[2:26]*(-1), col = "purple", lty = 2)

# length(uvcav_1)
```

Before renaming the data. Delete if code above works after 4/6/2020 (incorporated 3/31/2020)


```{r}
#' Below is the modified diffusion index code to include lagged variables.
#'
#' @param y - response variable
#' @param x - predictor variables
#' @param orig - forecast origin
#' @param m - number of diffusion indexes used
#' @param tau - VaR level to use; must be between 0 and 1
#' @param ar_tf - AR transformation type. (1 - no transformation,
#' 2 - absolute value, 3 - asymmetric slope)
#' @param p - number of AR lags to include. Default is one.
#' @param print - option to print the model summary to make sure everytning is ok. 0 is default.
#' @param model - model type (1 - SAV, 2 - AS, 3 - GARCH, 4 - ADAPTIVE) 
#'
#' @return - returns a list of variables for use in the diffusion index
#' @export
#'
#' @examples
mod_di_wl = function (y, x, orig, m, tau, ar_tf = 1, p = 1, print = 0, model = 1, end = NULL) 
{
  # Converts the response variables into a matrix
  if (!is.matrix(x)) 
      x = as.matrix(x)
  # nT is number of t time-steps
  nT = dim(x)[1]
  # Add a line to establish the number of data points used in the test.
  if (is.null(end) != TRUE){
    nT = end
  }
  # k is the number of diffusion indices used
  k = dim(x)[2]
  # Sanity checks to ensure that the origin isn't past the number of time points
  if (orig > nT) 
      orig = nT
  # Makes sure that there aren't more predictors than there variables in the dataset
  if (m > k) 
      m = k
  # Makes sure there are at least some variables
  if (m < 1) 
      m = 1
  # Subdivides the dataframe
  x1 = x[1:orig, ]
  # Calculates means of each row
  me = apply(x1, 2, mean)
  # Calculates standard deviations of each column
  se = sqrt(apply(x1, 2, var))
  # Creates a matrix x1, which normalizes all the columns. 
  # This may be an issue since it assumes that the distribution is sufficiently described by the first two moments
  x1 = x
  for (i in 1:k) {
      x1[, i] = (x1[, i] - me[i])/se[i]
  }
  V1 = cov(x1[1:orig, ])
  # Performs an eigen decomposition
  m1 = eigen(V1)
  # Selects eigenvalues
  sdev = m1$values
  # Selects eigenvectors
  M = m1$vectors
  # Makes a smaller matrix
  M1 = M[, 1:m]
  # This is the diffusion index model - [orig x p]*[p x m] = [orig x m]
  Dindex = x1 %*% M1
  # Cut down both the response and predictors to be a reasonable size
  y1 = y[1:orig]
  DF = Dindex[1:orig, ]
  # Copy the data frame
  DF_wl = Dindex
  # Lag the y-variable
  for (i in 1:p){
    # Create a lagged variable
    lag_var = lag(y, i)
    # Append the first lag to the data frame
    DF_wl = cbind(DF_wl,lag_var)
  }
  # Identify the right columns
  l_ar = ncol(DF_wl)
  f_ar = l_ar - p + 1
  # Keep the last columns kept to the side
  all_lag = DF_wl[,(f_ar:l_ar)]
  # Cut off the first row to avoid NA's
  DF_trim = DF_wl[1:orig,]
  # Rename the columns
  # Here's the new function with an untransformed AR(p) lag
  if (ar_tf == 1){
    mm = rq(y1[-(1:p)] ~ DF_trim[-(1:p),], tau = tau)
  }
  # Here's the new function with an SAV AR(p) lag
  if (ar_tf == 2){
    mm = rq(y1[-(1:p)] ~ DF_trim[-(1:p),-(f_ar:l_ar)] + abs(DF_trim[-(1:p),(f_ar:l_ar)]), tau = tau)
  }
  # Here's the new function with an asymmetric slope for the AR(1) lag
  # Indicator; 0 if percent change is negative, 1 if it's positive
  # indi = ifelse(DF_trim[,ar] < 0, 0, 1)
  if (ar_tf == 3){
    # Create a matrix of indicators
    indi_mat = matrix(0, nrow(DF_wl), p)
    # Generalize the above code
    for (i in 1:p){
      # print(f_ar + i - 1)
      # Populate the indicator
      indi_mat[,i] = ifelse(DF_wl[,f_ar + i - 1] < 0, 0, 1)
    }
  }
  # Fitting the regression
  if (ar_tf == 3){
    mm = rq(y1[-(1:p)] ~ DF_trim[-(1:p),-(f_ar:l_ar)] + DF_trim[-(1:p),(f_ar:l_ar)] + indi_mat[((p+1):orig),], tau = tau)
    # Add a different line to account for the indicator variable
    # intercept + m + 2*nlag to account for the number of indicator variables
    coef = matrix(mm$coefficients, (1 + m + 2*p), 1)
  }
  if (print == 1){
    print(summary(mm))
  }
  # Puts coefficients in a matrix - added the AR terms
  # coef = matrix(mm$coefficients, (m + 1), 1)
  if (ar_tf != 3){
    coef = matrix(mm$coefficients, (1 + m + p), 1)
  }
  # Initializes yhat variables and MSE
  yhat = NULL
  loss = NULL
  if (orig < nT) {
    # Creates a nfcst by (m+2) matrix
    # Add on the lagged variables
    newx = cbind(rep(1, (nT - orig)), Dindex[(orig + 1):nT, ], all_lag[(orig+1):nT,])
    # Incorporate lagged variables
    if (ar_tf == 3){
      newx = cbind(rep(1, (nT - orig)), Dindex[(orig + 1):nT, ], all_lag[(orig+1):nT,], indi_mat[(orig+1):nT,])
    }
    # [nfcstx(m+1)]*[(m+1)x1] = [nfcstx1]
    yhat = newx %*% coef
    # Calculates errors
    loss = abs(sum(ifelse(y[(orig + 1):nT] > yhat, tau, (-1)*(1-tau))))
    # Modifying this part to only print this if specified
    if (print == 1){
      cat("Losses of out-of-sample forecasts: ", loss, "\n")
    }
  }
  SWfore <- list(coef = coef, yhat = yhat, loss = loss, loadings = M1, 
      DFindex = Dindex)
}

```

```{r}
# Find the optimal number of predictors
opt_pred_nl = opt_m(y = pc_df[,1], x = pc_df[,-1], orig = 757, end = 1007, tau = 0.01, low_m =1, high_m  = 5)
opt_pred_art1 = opt_m(y = pc_df[,1], x = pc_df[,-1], orig = 757, end = 1007, tau = 0.01, low_m =1, high_m = 5, mod_di = 1, ar_tf = 1)
opt_pred_art2 = opt_m(y = pc_df[,1], x = pc_df[,-1], orig = 757, end = 1007, tau = 0.01, low_m =1, high_m = 5, mod_di = 1, ar_tf = 1)
opt_pred_art3 = opt_m(y = pc_df[,1], x = pc_df[,-1], orig = 757, end = 1007, tau = 0.01, low_m =1, high_m = 5, mod_di = 1, ar_tf = 1)

opt_pred_nl
opt_pred_art1
opt_pred_art2
opt_pred_art3

# test1 = loss_calc(pc_df[,1], pc_df[,-1], 757, 1027, 1, 0.01)[[1]]
# test2 = loss_calc(pc_df[,1], pc_df[,-1], 757, 1027, 2, 0.01)[[1]]
# test3 = loss_calc(pc_df[,1], pc_df[,-1], 757, 1027, 3, 0.01)[[1]]
# test4 = loss_calc(pc_df[,1], pc_df[,-1], 757, 1027, 4, 0.01)[[1]]
# test5 = loss_calc(pc_df[,1], pc_df[,-1], 757, 1027, 5, 0.01)[[1]]

# print(c(test1, test2, test3, test4, test5))

```

Old "master function"

```{r}
#' This is the "master" function where we'll evaluate the importance of the VaR model over several time periods
#'
#' @param symbol_list - a list of symbols to feed into the model 
#' @param resp_var - the response variable
#' @param compl_case - should the model require complete cases? Default value is 1.
#' @param adj_close - use adjusted close price for the predictors? Default value is 1.
#' @param resp_adj_close - use adjusted close price for the response? Default value is 1.
#' @param start_date - start date to pull data from
#' @param end_date - end date to pull data from
#' @param nval - number of validation points to use
#' @param ntest - number of test points to use
#' @param tau - VaR level to use
#' @param low_m - low number of predictors to test
#' @param high_m  - low number of predictors to test
#' @param uv_list - a list of a pre-run univariate model. If a data frame is not provided, the lengthy uv model will run 
#' @param no_run - things not to run in the model
#'
#' @return - a list of the plot matrix, a plot, a list with losses, and a table
#' @export - a plot and a table
#'
#' @examples - cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 5, tau = 0.01, uv_list = uv_cav_list)
cav_simul = function(symbol_list, resp_var, compl_case = 1, adj_close = 1, resp_adj_close = 1, start_date = "1900-01-01", end_date = Sys.Date(), nval, ntest, tau, low_m = 1, high_m, low_p = 1, high_p, uv_list = NULL, no_run = c(0,0,0,0), na_interp = TRUE, print = 0){
  # Select data parameters, pull the data, and percent change the data
  df = diff_index_df(symbol_list = symbol_list, resp_var = resp_var, compl_case = compl_case, adj_close = adj_close, resp_adj_close = resp_adj_close, start_date = start_date, end_date = end_date)
  # Take the percent change of the data
  pc_df = pc_diff_index(df)
  # print(pc_df)
  # Extract the legnth of the data frame
  nr = test_end = nrow(pc_df)
  # Calculate the start of the val period, the end of the val period, and the beginning and end of test period
  test_orig = test_end - ntest
  val_end = test_orig
  val_orig = test_orig - nval
  # print(c(val_orig, val_end, test_orig, test_end))
  # nrow = 100; ntest = 10; nval = 10
  # test_orig should be 90, val_orig should be 80.
  # test_org = 100 - 10 = 90, implying 91 to 100 should be tested
  # val_end = 90, val_start should be 80, implying 81 to 90 should be tested
  # Test for the optimal number of parameters
  opt_pred_nl = opt_m(y = pc_df[,1], x = pc_df[,-1], orig = val_orig, end = val_end, tau = tau, low_m = low_m, high_m = high_m)
  opt_pred_art1 = opt_m(y = pc_df[,1], x = pc_df[,-1], orig = val_orig, end = val_end, tau = tau, low_m = low_m, high_m = high_m, mod_di = 1, ar_tf = 1, print = print)
  opt_pred_art2 = opt_m(y = pc_df[,1], x = pc_df[,-1], orig = val_orig, end = val_end, tau = tau, low_m = low_m, high_m = high_m, mod_di = 1, ar_tf = 2, print = print)
  opt_pred_art3 = opt_m(y = pc_df[,1], x = pc_df[,-1], orig = val_orig, end = val_end, tau = tau, low_m = low_m, high_m = high_m, mod_di = 1, ar_tf = 3, print = print)
  # gen_uv_test(pc_df, 1, 0.05, no_run = c(1,1,0,1))
  # Use the above forecasts to input into the above
  mv_fcst = mod_di(pc_df[,1], pc_df[,-1], orig = test_orig, m = opt_pred_nl[[1]], tau = tau, print = print)
  mv_fcst_art1 = mod_di_wl(pc_df[,1], pc_df[,-1], orig = test_orig, m = opt_pred_art1[[1]], tau = tau, ar_tf = 1, print = print)
  mv_fcst_art2 = mod_di_wl(pc_df[,1], pc_df[,-1], orig = test_orig, m = opt_pred_art2[[1]], tau = tau, ar_tf = 2, print = print)
  mv_fcst_art3 = mod_di_wl(pc_df[,1], pc_df[,-1], orig = test_orig, m = opt_pred_art3[[1]], tau = tau, ar_tf = 3, print = print)
  # Calculate the number of predictions
  if (is.null(uv_list) == TRUE){
    # Print a warning
    print("WARNING: Not supplying an input data frame will require this function to run for a significant amount of time (1hr+)")
    # Call the function
    uv_list = gen_uv_test(pc_df, ntest, tau, no_run = no_run)
  }
  # npred = (nrow(pc_df)-(test_orig))
  # Incorporate the rolling predictions function results here
  plot_mat = cbind(pc_df[(test_orig+1):nrow(pc_df),1], mv_fcst$yhat[1:ntest], mv_fcst_art1$yhat[1:ntest], mv_fcst_art2$yhat[1:ntest], mv_fcst_art3$yhat[1:ntest], uv_list[[1]][(test_orig+1):test_end]*(-1), uv_list[[2]][(test_orig+1):test_end]*(-1), uv_list[[3]][(test_orig+1):test_end]*(-1), uv_list[[4]][(test_orig+1):test_end]*(-1))
  # Count the NAs and print a warning
  print(paste("NOTE: There are ", sum(is.na(plot_mat)), " NA(s) in the dataset", sep = ""))
  # Linearly interpolate the NAs
  if (na_interp == TRUE){
    # Assign the plot matrix to a new value
    plot_mat_na <- plot_mat
    # Print a warning
    print("WARNING: There were missing values in the plot matrix.")
    # Interpolate the NA's
    for (i in 1:ncol(plot_mat_na)){
      # Interpolate the data
      plot_mat[,i] <- na.approx(plot_mat_na[,i])
    }
  }
  # model type (1 - SAV, 2 - AS, 3 - GARCH, 4 - ADAPTIVE)
  # Add descriptive titles onto the plot_mat
  colnames(plot_mat) <- c("PG", "MV CAViaR", "MV CAViaR + AR(1)", "MV CAViaR + SAV", "MV CAViaR + AS", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive")
  # Plot everything
  plot = plt_data(plot_mat, tau = tau)
  # Calculate losses
  l_list = gen_loss_test(plot_mat, tau = tau)
  # Put into tables
  tables = pretty_tables(plot_mat, l_list, tau = tau)
  # Print the tables and the plot
  print(plot)
  print(tables)
  return(list(plot_mat, plot, l_list, tables, plot_mat_na))
}
```


All of the optimal parameters to consider are 3.

```{r}
# Run the model
mv_fcst = mod_di(pc_df[,1], pc_df[,-1], 1007, 3, 0.01)
mv_fcst_art1 = mod_di_wl(pc_df[,1], pc_df[,-1], 1007, 3, 0.01, ar_tf = 1)
mv_fcst_art2 = mod_di_wl(pc_df[,1], pc_df[,-1], 1007, 3, 0.01, ar_tf = 2)
mv_fcst_art3 = mod_di_wl(pc_df[,1], pc_df[,-1], 1007, 3, 0.01, ar_tf = 3)

# Run the model for the other three scenarios

# mod_di_wl = function (y, x, orig, m, tau, ar_tf = 1, p = 1, print = 0, model = 1)

```

```{r}
# Warning!! These can take a while to run (~1 hour). Proceed with caution.

# Run the univariate CAViaR models - these take a while. Be careful.
# uvcav_1 = rolling_predictions(pc_df[,1], range_data = (1:length(pc_df[,1])), nfcst = 250, model = 1, G = 10, col = 1, level = 0.01)
# uvcav_2 = rolling_predictions(pc_df[,1], range_data = (1:length(pc_df[,1])), nfcst = 250, model = 2, G = 10, col = 1, level = 0.01)
# uvcav_3 = rolling_predictions(pc_df[,1], range_data = (1:length(pc_df[,1])), nfcst = 250, model = 3, G = 10, col = 1, level = 0.01)
# uvcav_4 = rolling_predictions(pc_df[,1], range_data = (1:length(pc_df[,1])), nfcst = 250, model = 4, G = 10, col = 1, level = 0.01)
```


```{r}
# Testing the above model
aceg = gen_uv_test(pc_df, 1, 0.05, no_run = c(1,1,0,1))
tail(aceg[[1]])
tail(aceg[[2]])
tail(aceg[[3]])
tail(aceg[[4]])
```


```{r}
# Combine everything into a plot matrix
# uvcav_1
plot_mat = cbind(pc_df[1008:1257,1], mv_fcst$yhat[1:250], mv_fcst_art1$yhat[1:250], mv_fcst_art2$yhat[1:250], mv_fcst_art3$yhat[1:250], uvcav_1[1008:1257]*(-1), uvcav_2[1008:1257]*(-1), uvcav_3[1008:1257]*(-1), uvcav_4[1008:1257]*(-1))

# model type (1 - SAV, 2 - AS, 3 - GARCH, 4 - ADAPTIVE) 

# Add descriptive titles onto the plot_mat
colnames(plot_mat) <- c("PG", "MV CAViaR", "MV CAViaR + AR(1)", "MV CAViaR + SAV", "MV CAViaR + AS", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive")

head(plot_mat)
ncol(plot_mat)
```

```{r}
# plot_mat[,2]
# plot.ts(index(plot_mat),plot_mat[,1], type = "l")
# lines(index(plot_mat), plot_mat[,2])
# as.Date(plot_mat[,1])


# abc
```


```{r}
# Calculate the losses
loss_mv = loss_test(pc_df[1008:1257,1], mv_fcst$yhat[1:250], tau = 0.01)[[1]]
loss_uv1 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_1[1008:1257])*(-1), tau = 0.01)[[1]]
loss_uv2 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_2[1008:1257])*(-1), tau = 0.01)[[1]]
loss_uv3 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_3[1008:1257])*(-1), tau = 0.01)[[1]]
loss_uv4 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_4[1008:1257])*(-1), tau = 0.01)[[1]]

# Combine into a vector
loss_vec = c(loss_mv, loss_uv1, loss_uv2, loss_uv3, loss_uv4)

# Calculate the VaR breaks
break_mv = loss_test(pc_df[1008:1257,1], mv_fcst$yhat[1:250], tau = 0.01)[[3]]
break_uv1 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_1[1008:1257])*(-1), tau = 0.01)[[3]]
break_uv2 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_2[1008:1257])*(-1), tau = 0.01)[[3]]
break_uv3 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_3[1008:1257])*(-1), tau = 0.01)[[3]]
break_uv4 = loss_test(pc_df[1008:1257,1], as.ts(uvcav_4[1008:1257])*(-1), tau = 0.01)[[3]]

# Combine into a vector
break_vec = c(break_mv, break_uv1, break_uv2, break_uv3, break_uv4)

# Combine into a data frame
df = as.data.frame(rbind(loss_vec, break_vec))
# Add row/column names
colnames(df) <- c("Multivariate CAViaR", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive")
rownames(df) <- c("Losses", "VaR Breaks (%)")
# Convert to a table
  df %>% kable(caption = "Comparison of VaR Methods", digits = 3) %>% kable_styling("striped", full_width = F) %>% kable_styling()


```


```{r}
# Calculate an indicator variable
ind = ifelse(y[orig+i] < yhat[i], 1,0)
# Use indicator in function below
lvec[i] = (tau - ind)*(y[orig+i] - yhat[i])


uvcav_1

# lines(var_1pc$yhat[1:300], col = "red", lty = 2)

# mod_di = function (y, x, orig, m, tau) 

ts.plot(mv_fcst$yhat)
lines(pc_df[1028:1257,1], col = "red")
```


```{r}
var_1pc_k1 = mod_di(pc_df[,1], pc_df[,-1], 757, 1, 0.01)
var_1pc_k3 = mod_di(pc_df[,1], pc_df[,-1], 757, 1, 0.01)
var_1pc_k5 = mod_di(pc_df[,1], pc_df[,-1], 757, 1, 0.01)

# var_1pc_k3


```

```{r}
l_list = gen_loss_test(plot_mat, 0.01)
l_list[[1]]
```


```{r}
# Replace NA's with 0s
plt_005 = var_0_05_pc_2008[[1]]
plt_005[is.na(plt_005)] <- 0

plt_data(plt_005)
```


Rerunning the 1% model again.

```{r}
# 2/25/20 - I expect this to converge.
var_1pc_2008_r2 = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 5, tau = 0.01)

var_1pc
```


```{r}
# Diagnose why there are missing values in the indirect GARCH call
var_1pc_2008[[3]][[3]][,7]

# Missing value at 152. This wasn't here earlier.
ts.plot(var_1pc_2008[[3]][[3]][,7])

# Let's inspect the underlying data - seems fine. Why would we get different values from before?
var_1pc_2008[[1]][,1][150:155]

# Let's try running the underlying function
ind_garch_1pc = gen_uv_test(pc_df, 250, 0.01, no_run = c(1,1,0,1))
ind_garch_1pc
# pc_df


# Look at the last 250 obs
tail(ind_garch_1pc[[3]], 250)
tail(ind_garch_1pc[[3]])

# Do a side-by-side comparison
plot.ts(var_1pc_2008[[3]][[3]][,7]*(-1))
tail(ind_garch_1pc[[3]], 5)*(-1)
var_1pc_2008[[3]][[3]][,7]*(-1)

ind_garch_var = tail(ind_garch_1pc[[3]], 250)*(-1)
(ind_garch_var) - var_1pc_2008[[3]][[3]][,7]
head(var_1pc_2008[[3]][[3]][,7])
head(ind_garch_var)

# Let's see what the previous univariate run produced
old_run_ig = tail(uvcav_3, 250)
big_model_ig = var_1pc_2008[[1]][,7]
tail(ind_garch_var)
tail(old_run_ig)


# So there's 3 different series
# The first is for the run called in the first "rolling predictions" function - old_run_ig
# The second is for the one from the gen_uv_test function - ind_garch_var
# The third is for the one from the cav_simul function - big_model_ig

# comp_mat_3 = cbind(old_run_ig*(-1), ind_garch_var, big_model_ig*(-1))
# comp_mat_3

# In theory, all three of these should be identical, or at least very very close because of the optimization involved. By and large, 1 and 2 are indeed close. 3 isn't close at all.

# Let's do quick sanity checks for the other runs
comp_mat_1 = cbind(tail(uvcav_1, 250)*(-1), var_1pc_2008[[1]][,6])
comp_mat_2 = cbind(tail(uvcav_2, 250)*(-1), var_1pc_2008[[1]][,7])
comp_mat_3 = cbind(tail(uvcav_3, 250)*(-1), var_1pc_2008[[1]][,8])
comp_mat_3a = cbind(tail(uvcav_3, 250)*(-1), var_1pc_2008[[1]][,8], tail(ind_garch_1pc[[3]], 250)*(-1))
comp_mat_4 = cbind(tail(uvcav_4, 250)*(-1), var_1pc_2008[[1]][,9])

# Plot the data
plot(comp_mat_1)
plot(comp_mat_2)
plot(comp_mat_3)
plot(comp_mat_3a)
comp_mat_3a
plot(comp_mat_4)
comp_mat_2
comp_mat_3
# (1 - SAV, 2 - AS, 3 - GARCH, 4 - ADAPTIVE) (defaults to 1)
comp_mat_4

# Are we even calling the right things? I was calling the losses!

```

Fixing the NAs in the data to send to Wei Biao. Will also serve as a test. This is on 3/3/20.

```{r}
tm_123 = var_0_05_pc_2008[[1]]
for (i in 1:ncol(tm_123)){
  # Interpolate the data
  tm_123[,i] <- na.approx(tm_123[,i])
}
# sum(is.na(tm_123))


```


To test this function, we're going to combine all the univariate CAViaR parts into one list - STM 2/22/20.

```{r}
uv_cav_list = list(uvcav_1, uvcav_2, uvcav_3, uvcav_4)
```

```{r}
# Let's fit the plot matrix - 4/2/20
plot_matrix = var_1pc_2008_sm[[1]]
tau = 0.01

plot.ts(plot_matrix[,1])

# Establish a maximum and minimum value
max_val = max(plot_matrix[,1:ncol(plot_matrix)])
min_val = min(plot_matrix[,1:ncol(plot_matrix)])
# Create an initial plot and add lines
  for (i in 1:ncol(plot_matrix)){
    if (i == 1){
      plot.ts(plot_matrix[,i], type = "l", xlab = "Trading Days", ylab = "Percent Change in PG", ylim = c(min_val,max_val), lwd = 1, main = "Predicting PG Returns Over Last 250 Trading Days in 2008", sub = paste("The VaR Level is ", 100*tau, "%", sep = ""))
  } else if(i %in% seq(2,8,1)) {
      lines(index(plot_matrix),plot_matrix[,i], col = i-1, lty = 2)
  } else {
      lines(index(plot_matrix),plot_matrix[,i], col = i-1, lty = 2, lwd = 2)
  }
  }
# Define a sequence for plotting
plot_seq = seq(1, ncol(plot_matrix))
legend("topleft", legend = c(colnames(plot_matrix)), col = plot_seq, lty = c(1, rep(2, 7), rep(3, ifelse(ncol(plot_matrix)-8 <= 0, 0, ncol(plot_matrix)-8))), lwd = c(1, rep(1, 7), rep(2, ifelse(ncol(plot_matrix)-8 <= 0, 0, ncol(plot_matrix)-8))))
# Add a line for 0
# abline(h = 0, col = "black", lty = 2)
```

```{r}
# function(symbol_list, resp_var, compl_case = 1, adj_close = 1, resp_adj_close = 1, start_date = "1900-01-01", end_date = Sys.Date(), val_orig, val_end, test_orig, tau, low_m = 1, high_m, uv_list){
# Test the big function
big_test_mod = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 5, tau = 0.01, uv_list = uv_cav_list)

head(big_test[[1]])
nrow(big_test[[1]])
nrow(big_test_mod[[1]])
head(big_test_mod[[1]])
big_test_mod[[4]]
big_test[[4]]
# diff_index_df(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31")
```


## Simulation Studies

```{r}
# Low to high volatility series
lth_series = stock_simul(500, 25, 0.001, -0.005, 0.001, 0.01)
mth_series = stock_simul(500, 25, 0.001, -0.005, 0.005, 0.01)
hth_series = stock_simul(500, 25,  0.001, -0.005, 0.01, 0.01)
```


```{r}
# Plot the data
plot(lth_series, type = "l", ylim = c(0.8, 1.8), xlab = "Days", ylab = "Growth (Indexed to 1)", main = "Growth Comparison of 3 Simulated Stocks")
lines(mth_series, type = "l", col = "red")
lines(hth_series, type = "l", col = "blue")
legend("bottomright", legend=c("Low Vol. Stock", "Mid Vol. Stock", "High Vol. Stock"),col=c("black", "red", "blue"), lty = c(1,1,1))

# Transform the data to be differenced log data
dl_lth <- diff(log(lth_series))
dl_mth <- diff(log(mth_series))
dl_hth <- diff(log(hth_series))



# Plot the differenced data
plot(dl_lth, type = "l", xlab = "Days", ylab = "Differenced Log Growth", ylim = c(-0.04, 0.04), main = "Log  Growth Rate Comparisons of 3 Simulated Stocks")
lines(dl_mth, type = "l", col = "red")
lines(dl_hth, type = "l", col = "blue")
legend("bottomright", legend=c("Low Vol. Stock", "Mid Vol. Stock", "High Vol. Stock"),col=c("black", "red", "blue"), lty = c(1,1,1))
```


```{r}
# Let's test the series
m1_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 1, G = 10, col = 1)
m2_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 2, G = 10, col = 1)
m3_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 3, G = 10, col = 1)
m4_lth = rolling_predictions(dl_lth, range_data = (1:length(dl_lth)), nfcst = 25, model = 4, G = 10, col = 1)

# Let's test the series
m1_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 1, G = 10, col = 1)
m2_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 2, G = 10, col = 1)
m3_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 3, G = 10, col = 1)
m4_mth = rolling_predictions(dl_mth, range_data = (1:length(dl_mth)), nfcst = 25, model = 4, G = 10, col = 1)

# Let's test the series
m1_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 1, G = 10, col = 1)
m2_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 2, G = 10, col = 1)
m3_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 3, G = 10, col = 1)
m4_hth = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 25, model = 4, G = 10, col = 1)
```

```{r}

```

```{r}
# First plot - LTH VaR
plot(dl_lth[500:524], type = "l", ylim = c(-0.04, 0.04), xlab = "Forecast Day", ylab = "Log Return", main = "VaR Forecast in High-Volatility Environment for a Low-Vol. Stock")
legend("topright", legend=c("Simulated Return", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "red", "green", "blue", "purple"), lty = c(1,2,2,2,2))
lines(m1_lth[2:26]*(-1), col = "red", lty = 2)
lines(m2_lth[2:26]*(-1), col = "green", lty = 2)
lines(m3_lth[2:26]*(-1), col = "blue", lty = 2)
lines(m4_lth[2:26]*(-1), col = "purple", lty = 2)

# Second plot - MTH VaR
plot(dl_mth[500:524], type = "l", ylim = c(-0.04, 0.04), xlab = "Forecast Day", ylab = "Log Return", main = "VaR Forecast in High-Volatility Environment for a Mid-Vol. Stock")
legend("topright", legend=c("Simulated Return", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "red", "green", "blue", "purple"), lty = c(1,2,2,2,2))
lines(m1_mth[2:26]*(-1), col = "red", lty = 2)
lines(m2_mth[2:26]*(-1), col = "green", lty = 2)
lines(m3_mth[2:26]*(-1), col = "blue", lty = 2)
lines(m4_mth[2:26]*(-1), col = "purple", lty = 2)

# Second plot - HTH VaR
plot(dl_hth[500:524], type = "l", ylim = c(-0.04, 0.04), xlab = "Forecast Day", ylab = "Log Return", main = "VaR Forecast in High-Volatility Environment for a Low-Vol. Stock")
legend("topright", legend=c("Simulated Return", "SAV", "Abs. Slope", "Ind. GARCH", "Adaptive"),col=c("black", "red", "green", "blue", "purple"), lty = c(1,2,2,2,2))
lines(m1_hth[2:26]*(-1), col = "red", lty = 2)
lines(m2_hth[2:26]*(-1), col = "green", lty = 2)
lines(m3_hth[2:26]*(-1), col = "blue", lty = 2)
lines(m4_hth[2:26]*(-1), col = "purple", lty = 2)

```

That doesn't look promising. What if we just look at the last 10?

```{r}
# Let's test the series
m1_hth_sm = rolling_predictions(dl_hth, range_data = (1:length(dl_hth)), nfcst = 10, model = 1, G = 10, col = 1)
```

```{r}
plot(dl_hth[515:524], type = "l", ylim = c(-0.04, 0.04))
lines(m1_hth_sm[1:10], col = "red")
```

Not good. Let's look at an in-sample test.

```{r}
# Let's test the series in sample
m1_hth_sm_is_05 = rolling_predictions(dl_hth, range_data = (1:400), nfcst = 10, model = 1, level = 0.05, G = 10, col = 1)
m1_hth_sm_is_01 = rolling_predictions(dl_hth, range_data = (1:400), nfcst = 10, model = 1, level = 0.01, G = 10, col = 1)


```

```{r}
plot(dl_hth[390:399], type = "l", ylim = c(-0.04, 0.04))
lines(m1_hth_sm_is_05[1:10], col = "red")
lines(m1_hth_sm_is_01[1:10], col = "green")
```

```{r}
# We may need to expand this to other areas. Ask Wei Biao for simulation advice.
```

## Multivariate Simulation Study

Real data is messy, but simulation could give us some idea of whether combining all the stocks into a pool could give a lift.

```{r}
#' Stock simulation for use in CAViaR simulation study
#'
#' @param n1 - number of data points in first series
#' @param n2 - number of data points in second series
#' @param s1_sd - standard deviation of first series
#' @param s2_sd - standard deviation of second series 
#' @param s1_mean - mean of first series 
#' @param s2_mean - mean of second series 
#' @param rseed - random seed for reproducibility
#'
#' @return - output time series
#' @export
#'
#' @examples hth_series = stock_simul(100, 25, 10, 10)
stock_simul = function(n1, n2,s1_gr, s2_gr, s1_sd, s2_sd, rseed = 1234){
  # Set random seed
  set.seed(rseed)
  # Initialize vector and put first value in the series
  s1 <- rep(1, n1)
  # Simulate first series
  for (i in 2:n1){
    s1[i] <- s1[i-1]*(1+s1_gr) + rnorm(1, mean = 0, sd = s1_sd)
  }
  # Initialize vector
  s2 <- rep(1, n2)
  # Put first value in
  s2[1] <- s1[n1]*(1+s2_gr) + rnorm(1, mean = 0, sd = s2_sd)
  # Simulate second series
  for (i in 2:n2){
    s2[i] <- s2[i-1]*(1+s2_gr) + rnorm(1, mean = 0, sd = s2_sd)
  }
  # Combine the two series
  long <- as.matrix(c(s1, s2))
  # Export the data
  return(long)
}

# test <- lth_series[500,1]*(1)

```






```{r}
#' Stock data generator for use in CAViaR simulation study
#' For maximum flexibility, this will only generate one series with one variance and growth rate.
#' Note that for simplicity, this is modeling the percentage change
#' 
#' @param t - number of time data points
#' @param mu - the mean return in a given day
#' @param sigma - the standard deviation
#' @param rseed - random seed for reproducibility. Default is NULL
#'
#' @return - output percentage changes of a stock
#' @export
#'
#' @examples dat_gen(1000, 0.001, 0.1)
dat_gen = function(t, mu, sigma, rseed = NULL){
  # Have an option for with and without random seed
  if (is.null(rseed) == TRUE){
    dat = rnorm(t, mu, sigma)
  }
  else {
    set.seed(rseed)
    dat = rnorm(t, mu, sigma)
  }
  return(dat)
}

# mean(dat_gen(1000, 0.001, 0.1))

```

```{r}
# Generalize the above vector to look at many different periods


```

# Combining a Diffusion Index Model with the Quantile Regression Model

```{r}
# Below is code that pulls real data for analysis
```


Response is IBM, pick 5 stocks for predictors.

```{r}
# Pull IBM
ibm = data_pull("IBM", start_date = "1986-04-06", end_date = "1999-04-08")

# Pull 10 stocks at random that existed in the DJIA over the timeframe
# Exxon
xom = data_pull("XOM", start_date = "1986-04-06", end_date = "1999-04-08")

# PG
pg = data_pull("PG", start_date = "1986-04-06", end_date = "1999-04-08")

# General Electric
ge = data_pull("GE", start_date = "1986-04-06", end_date = "1999-04-08")

# 3M
mmm = data_pull("MMM", start_date = "1986-04-06", end_date = "1999-04-08")

# Disney
dis = data_pull("DIS", start_date = "1986-04-06", end_date = "1999-04-08")

```

There is probably some work to be done with choosing these stocks. I only picked ones with data.

```{r}
# Plot the data
# plot.xts(ibm[,2])

```

Let's just see if there is any merit to just regressing one set of stocks on another.

```{r}
# Predictors
# ?SWfore
SWfore
```

```{r}
# Write a function to fix the NAs and output a plot and the table
fix_na <- function(input_df, tau){
  # Copy the data frame
  int_df <- input_df
  # Count the NAs and print a warning
  # paste("Window Size is",  floor(win_size), sep=" "), 
  print(paste("NOTE: There are ", sum(is.na(int_df)), " NA(s) in the dataset", sep = ""))
  # Fix the NAs
  for (i in 1:ncol(int_df)){
    # Interpolate the data
    int_df[,i] <- na.approx(int_df[,i])
  }
  # Plot everything
  plot = plt_data(int_df)
  # Calculate losses
  l_list = gen_loss_test(int_df, tau = tau)
  # Put into tables
  tables = pretty_tables(int_df, l_list)
  # Print the tables and the plot
  print(plot)
  print(tables)
  return(list(int_df, plot, l_list, tables))
}

```

```{r}
# var_0_05_pc_2008 = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 5, tau = 0.005)
# var_1pc_2008 = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 5, tau = 0.01)
# var_5pc_2008 = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 5, tau = 0.05)
# var_10pc_2008 = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 5, tau = 0.10)
# var_20pc_2008 = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 250, low_m = 1, high_m = 5, tau = 0.20)

# Call the function multiple times
fix_0005 = fix_na(var_0_05_pc_2008[[1]], tau = 0.005)
fix_001 = fix_na(var_1pc_2008[[1]], tau = 0.01)
fix_005 = fix_na(var_5pc_2008[[1]], tau = 0.05)
fix_010 = fix_na(var_10pc_2008[[1]], tau = 0.10)
fix_020 = fix_na(var_20pc_2008[[1]], tau = 0.20)
```

# Test the quiet portion

```{r}
var_1pc_2008_sm = cav_simul(c("DIS", "GE", "IBM", "MMM", "XOM"), resp_var = "PG", start_date = "2004-01-01", end_date = "2008-12-31", nval = 250, ntest = 2, low_m = 1, high_m = 5, tau = 0.01)
```

# Fixing the plot function

```{r}
# The problem description is that we're seeing odd things on the plot. I want to see if 
# extracting the date object and formatting it properly might help.

# date(var_20pc_2008[[1]])

# I will wait until I have an internet connection
```


# Stock simulation